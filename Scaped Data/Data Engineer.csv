JobId,Description,Title,MinimumSalary,MaximumSalary,staticUrl,keySkills,locations,Job,createdDate
091222502697,"<div> <ul> <li> Design and build a modern data warehouse in the cloud. Develops technical solutions to complex problems which require the regular use of ingenuity and creativity. Develop proof-of-concept (POC) solutions to help business units better visualize their business needs and to clarify requirements for development. </li> </ul> <ul> <li> Work with Business Technology Partners to understand business problems  </li> </ul> <ul> <li> Provide recommendations to business via analytics consulting services  </li> </ul> <ul> <li> Ingest data from all enterprise systems into a design that is fit for use </li> </ul> <ul> <li> Utilize cloud computing experience, knowledge and skills </li> </ul> <ul> <li> Utilize expertise in Microsoft Azure </li> </ul> <ul> <li> Utilize deep knowledge and skill in extract, transform, load (ETL) design, development, and performance tuning on Microsoft SSIS in SQL Server 2012 and above in a multi-dimensional Data Warehousing environment </li> </ul> <ul> <li> Data ingestion and modeling for digital customer platform </li> </ul> <ul> <li> Ingest data across safety assessment business related to customer platform </li> </ul> <ul> <li> Utilize methodologies in aggregating customer data from different applications, cleaning it, and analyzing it to get a reasonable picture of customer information and preferences </li> </ul> <ul> <li> Utilize experience with ADF, ADLS, Data Bricks and SQL </li> </ul> <ul> <li> Provide insights on data sets to business </li> </ul> <ul> <li> Perform ad-hoc analysis and present results in a clear and user-friendly manner. Perform testing, resolve issues and automate unit tests. Process, cleanse, and verify the integrity of data used for analysis. </li> </ul> <ul> <li> Conduct data profiling and cleansing as needed for each data source </li> </ul> <ul> <li> Build code using skills in advanced SQL Programming: PL/SQL, T-SQL, U-SQL, that automates data validation </li> </ul> <ul> <li> Review reports with results of data profiling on any new data source for business consumption </li> </ul> <b> <b> Job Qualifications </b> </b> <br /> <ul> <li> Bachelors degree in Computer Engineering, Computer Science or related discipline, Master s Degree preferred </li> </ul> <ul> <li> 7+ years of ETL design, development, and performance tuning on Microsoft SSIS in SQL Server 2012 and above (2016 preferable) in a multi-dimensional Data Warehousing environment  </li> </ul> <ul> <li> 4+ years of SSAS design, development, maintenance and performance tuning on Microsoft SQL Server 2012 and above (2016 preferable), with expert MDX and DAX skills </li> </ul> <ul> <li> 7+ years of advanced SQL Programming: PL/SQL, T-SQL, U-SQL </li> </ul> <ul> <li> 4+ years of Enterprise Data & Analytics solution architecture  </li> </ul> <ul> <li> 2+ years of Power BI experience including mobile solutions </li> </ul> <ul> <li> 2+ years of strong and extensive hands on experience in Azure, preferably data heavy / analytics applications leveraging relational and NoSQL databases, Data Warehouse and Big Data </li> </ul> <ul> <li> Experience with Azure Data Lake, Azure SQL Data Warehouse, Data Catalog, Azure Analysis Services, Data Bricks, Storage Account Gen2, Azure SQL Database, Azure DNS, Virtual Network, DocumentDB, Azure App Service, Data Factory </li> </ul> <ul> <li> Experience with Big Data Technologies such as: Hadoop, Sqoop, Hive, Kafka, Spark, Pyspark, Python, Scala or Pig </li> </ul> <ul> <li> Experience designing and building cloud native applications as well as Azure Cloud migration experience </li> </ul> <ul> <li> Experience with Azure IaaS and PaaS services </li> </ul> <ul> <li> Experience with Big Data Management (BDM) for relational and non-relational data (formats like json, xml, avro, parquet, copybook, etc.) </li> </ul> <ul> <li> PowerShell, Azure RunBook, and Azure DevOps experience </li> </ul> <ul> <li> Experience with setting up and operating data pipelines using Python or SQL  </li> </ul> <ul> <li> Strong analytical abilities and a strong intellectual curiosity </li> </ul> <ul> <li> Self-starter with the ability to work independently or as part of a project team  </li> </ul> <ul> <li> Capability to quickly conduct performance analysis, troubleshooting and remediation particularly in complex ETL mappings for SSAS </li> </ul> <ul> <li> Strong knowledge of emerging technologies and tools </li> </ul> <ul> <li> Strong organizational and time management skills </li> </ul> <ul> <li> Effective collaboration of tasks across the business and technical teams </li> </ul> <ul> <li> Proven and recent experience of implementing solutions from requirements gathering and process design to functional production deployment </li> </ul> <ul> <li> Demonstrated ability to produce high quality deliverables </li> </ul> <ul> <li> An equivalent combination of education and experience may be accepted as a satisfactory substitute for the specific education and experience listed above </li> </ul> <p> <strong> Preferred Skills: </strong> </p> <ul> <li> Hands on experience with data virtualization technologies: CIS (Tibco), Denodo, or SQL 2019 Virtualization </li> </ul> <ul> <li> Experience working with other public cloud technologies AWS, GCP  </li> </ul> <ul> <li> Excellent ability to communicate complex quantitative analysis in a clear, precise, and actionable manner with both technical and business stakeholders </li> </ul> <ul> <li> Experience building test automation to ensure data quality and accuracy </li> </ul> <ul> <li> Experience with any of the following scripting languages, R, SAS, JavaScript, Regex, PowerShell, shell scripting </li> </ul> <ul> <li> Experience working in data and analytics in the Life Sciences industry </li> </ul> </div>",Senior Data Engineer 1,600000,1100000,https://www.naukri.com/job-listings-senior-data-engineer-1-charles-river-laboratories-india-private-limited-mumbai-hyderabad-secunderabad-ahmedabad-bangalore-bengaluru-7-to-12-years-091222502697,"['Performance tuning', 'SAS', 'XML', 'Shell scripting', 'Javascript', 'TIBCO', 'DNS', 'SSIS', 'Troubleshooting', 'Python']","['Mumbai', 'Ahmedabad', 'Bengaluru', 'Hyderabad']",Data Engineer,2022-12-09 21:01:08
101222500656,"<div> <p> <br /> Manages one or more data engineering projects of moderate complexity. Leads or acts as key team member in defining and builds the data pipelines that will enable faster, better, data-informed decision-making within the business. Jointly designs and implements a new big data solution which supports high amounts and velocity of data and supports future growth. Identifies latest development, testing and deployment techniques to quickly deploy new releases to e.g. deploy new data pipelines and add data sources. Takes direct reports, when applicable, through Agile Framework and its respective tools. </p> <p> <br /> Influences stakeholders in adopting analyses / outcomes. Acts proactively to support the business to further professionalize their MI / Analytic reporting platform, using the ESSA approach. Opinions valued by business interface. Plays a prominent role in project related meetings etc. <br /> Utilises business partnering in order to gather, analyse and model data and key performance indicators of the highest complexity. Working as Data Engineering Lead to develop and deploy innovative big data platforms for advanced analytics and data processing. <br /> Works independently under broad managerial supervision. <br /> <br /> This job profile contains generic information and does not describe individual positions or required job competencies. Grading decisions will also depend on other factors. </p> </div>",Senior Data Engineer,1300000,1800000,https://www.naukri.com/job-listings-senior-data-engineer-shell-pvt-ltd-bangalore-bengaluru-4-to-8-years-101222500656,"['advanced analytics', 'HP data protector', 'development testing', 'Agile', 'Engineering projects', 'Data processing', 'Business partnering', 'Information technology', 'Engineering Lead', 'Recruitment']",['Bengaluru'],Data Engineer,2022-12-10 23:47:24
091222502484,"<p> </p><ul> <li> Applies techniques such as business intelligence (BI), reporting and data modelling, analytical techniques and big data processing to provide the business with relevant information to increase revenues, improve operational efficiency, optimize customer programs, respond quickly to emerging market trends and gain a competitive edge in the market </li> <li> Provides operational and functional support to the business on creating, storing, managing and maintaining information in all its forms, including the ability to incorporate policies and procedures for centrally managing and sharing information among different individuals, organisations and information systems throughout the information life cycle </li> <li> We are looking for a candidate with 8+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:  </li> <li> <b> Experience with Azure  </b> : ADF, ADLS, Databricks, PySpark, Spark SQL, Stream Analytics, SQL DW, COSMOS DB, Analysis Services, Azure Functions, Serverless Architecture, ARM Templates.  </li> <li> Experience with relational SQL/NoSQL databases, file handlings and API integrations  </li> <li> Experience with object-oriented/object function scripting languages: Python, SQL, Scala, Spark-SQL etc.  </li> <li> Nice to have experience with any of these toolset like Kafka, Stream sets, Alteryx, HANA, SLT and BODSRoles and Responsibilities:  </li> <li> Create and maintain optimal data pipeline architecture.  </li> <li> Assemble large, complex data sets that meet functional / non-functional business requirements.  </li> <li> Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.  </li> <li> Build the infrastructure required for optimal ETL/ELT of data from a wide variety of data sources using SQL and Azure, AWS big data technologies.  </li> <li> Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other KPI metrics.  </li> <li> Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.  </li> <li> Keep our data separated and secure across national boundaries through multiple data centres and Azure, AWS regions.  </li> <li> Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.  </li> <li> Work with data and analytics experts to strive for greater functionality in our data systems.  </li> </ul> <p> </p> <p> <strong> Qualifications: </strong> </p> <ul> <li> Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.  </li> <li> Experience building and optimizing data pipelines using ADF  </li> <li> Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement  </li> <li> Strong analytic skills related to working with structured and unstructured datasets  </li> <li> Build processes supporting data transformation, data structures, metadata, dependency and workload management.  </li> <li> A successful history of transforming, processing and extracting value from large disconnected datasets  </li> <li> Strong team player with organizational and communication skills  </li> <li> Experience supporting and working with cross-functional teams in a dynamic environment </li> </ul>",Senior Data Engineer,1300000,1800000,https://www.naukri.com/job-listings-senior-data-engineer-shell-pvt-ltd-bangalore-bengaluru-8-to-13-years-091222502484,"['Customer acquisition', 'Analytical', 'Data processing', 'Business intelligence', 'Operations', 'Information technology', 'Analytics', 'Recruitment', 'SQL']",['Bengaluru'],Data Engineer,2022-12-09 21:00:51
091222502483,"<div> <ul> <li> Communicate with both technical developers and business managers and gain respect and trust of leaders and staff.  </li> <li> Actively deliver the roll-out and embedding of Data Foundation initiatives in support of the key business programs advising on the technology and using leading market standard tools.  </li> <li> Coordinate the change management process, incident management and problem management process.  </li> <li> Ensure traceability of requirements from Data through testing and scope changes, to training and transition.  </li> <li> Drive implementation efficiency and effectiveness across the pilots and future projects to minimize cost, increase speed of implementation and maximize value delivery.  </li> </ul> <p> </p> <p> <b> <span> Education  </span> </b> </p> <ul> <li> University degree in any IT discipline  </li> <li> Minimum 7+ years of experience in the IT industry  </li> </ul> <p> </p> <p> <b> <span> Expected Skills  </span> </b> </p> <ul> <li> Proven technology champion in working with relational, Data warehouses databases, query authoring (SQL) as well as working familiarity with a variety of databases. Experience/Knowledge in working with NoSQL databases and can create E2E pipelines.  </li> <li> Highly Experienced in building and optimizing complex queries. Good with manipulating, processing and extracting value from large, disconnected datasets.  </li> <li> Proven Experience in working  <span> any one of the  </span> data engineering technologies and ADLS, ADF, Azure Databricks, Azure SQL, Synapse, SAP HANA. Your experience in handling big data sets and big data technologies will be an asset.  </li> <li> Proven champion with in-depth knowledge  <span> of any one of the  </span> scripting languages: Python, SQL, Spark-SQL/ PySpark.  </li> <li> Very Good understanding in Data Foundation initiatives, like Modelling, Data Quality Management, Data Governance, Data Maturity Assessments and Data Strategy in support of the key business stakeholders.  </li> </ul> <p> </p> <p> <b> <span> Leadership Skills  </span> </b> </p> <ul> <li> Be self-motivated, innovative, and very good in taking up new initiatives and driving them to closure.  </li> <li> Communicate skilfully with both technical developers, architects, and business stake holders.  </li> <li> Understand well- data foundation initiatives like data modelling, data quality, data governance, data maturity assessments and data strategy in support of the key business stake holders.  </li> <li> Work with source control technologies (such as GITHUB, Azure DevOps)  </li> <li> Have experience in working with AGILE, KANBAN methodologies.  </li> </ul> <p> </p> <p> <b> <span> Nice to Have Skills  </span> </b> </p> <ul> <li> Previously created or enhanced CI/CD build and releases pipelines.  </li> <li> Experience in working with scripting languages such as YAML, PowerShell, Terraform etc.  </li> <li> Experience with big data tools: Kafka, Hadoop, Spark and similar technologies. Experience with stream-processing systems.  </li> <li> Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow and more.  </li> <li> Core Python skills like: Numpy, Panda, Django  </li> <li> Certification in Azure </li> </ul> </div>",Senior Data Engineer,900000,1400000,https://www.naukri.com/job-listings-senior-data-engineer-shell-pvt-ltd-bangalore-bengaluru-7-to-12-years-091222502483,"['Change management', 'HP data protector', 'Problem management', 'Agile', 'Incident management', 'Data quality', 'Information technology', 'Recruitment', 'SQL', 'Python']",['Bengaluru'],Data Engineer,2022-12-09 21:00:51
091222502482,"<div> <ul> <li> Communicate with both technical developers and business managers and gain respect and trust of leaders and staff.  </li> <li> Actively deliver the roll-out and embedding of Data Foundation initiatives in support of the key business programs advising on the technology and using leading market standard tools.  </li> <li> Coordinate the change management process, incident management and problem management process.  </li> <li> Ensure traceability of requirements from Data through testing and scope changes, to training and transition.  </li> <li> Drive implementation efficiency and effectiveness across the pilots and future projects to minimize cost, increase speed of implementation and maximize value delivery.  </li> <li> <span> You will be an asset in our team bringing deep technical skills and capabilities to become a key part of projects defining the data journey in Shell, keen to engage, network and innovate in collaboration with companywide teams.  </span> </li> </ul> <p> </p> <p> <b> What we need from you  </b> </p> <ul> <li> University degree in any IT discipline with minimum 9 years of experience in the IT industry (out of which 5 years of cloud experience)  </li> <li> Proven experience of working in and delivering enterprise solutions built on at least one of the major cloud providers like Microsoft Azure or Amazon Web Services .  </li> <li> Proven technology champion in working with relational, Data warehouses databases, query authoring (SQL) as well as working familiarity with a variety of databases . Experience/Knowledge in working with NoSQL databases and can create E2E pipelines.  </li> <li> Highly Experienced in building and optimizing complex queries . Good with manipulating, processing and extracting value from large, disconnected datasets.  </li> <li> Proven Experience in working the data engineering technologies and ADLS, ADF, Azure Databricks & Delta Live Table, Azure SQL, Key Vault, Azure Synapse, Power BI . Your experience in handling big data sets and big data technologies will be an asset.  </li> <li> Proven champion with in-depth knowledge  <span> of the  </span> scripting languages: Python, SQL, Spark-SQL/ PySpark.  </li> <li> Practical knowledge of Kafka to build real-time streaming data pipelines and applications that adapt to the data streams.  </li> <li> Very Good understanding in Data Foundation initiatives, like Modelling, Data Quality Management, Data Governance, Data Maturity Assessments and Data Strategy in support of the key business stakeholders.  </li> <li> Can communicate skilfully with both technical developers, architects, and business stakeholders.  </li> <li> Understand well- data foundation initiatives like data modelling, data quality, data governance, data maturity assessments and data strategy in support of the key business stake holders.  </li> <li> Work with source control technologies (such as GITHUB, Azure DevOps )  </li> <li> Have experience in working with AGILE, KANBAN methodologies.  </li> <li> Relevant certifications in Azure like DP-203, AZ-204 or AZ-300 series.  </li> </ul> <p> </p> <p> <b> <span> Nice to Have Skills  </span> </b> </p> <ul> <li> Previously created or enhanced CI/CD build and releases pipelines.  </li> <li> Experience in working with scripting languages such as YAML, PowerShell, Terraform etc.  </li> <li> Experience with big data tools: Hadoop, Spark and similar technologies. Experience with stream-processing systems. </li> </ul> </div>",Senior Data Engineer,1300000,1800000,https://www.naukri.com/job-listings-senior-data-engineer-shell-pvt-ltd-bangalore-bengaluru-9-to-14-years-091222502482,"['Change management', 'NoSQL', 'Powershell', 'Agile', 'Problem management', 'Incident management', 'Data quality', 'Information technology', 'SQL', 'Python']",['Bengaluru'],Data Engineer,2022-12-09 21:00:51
091222501919,"<div> <p> <span> This position is responsible for designing complex data pipeline for data extraction, transformation and loading to and from different components and system. A Sr. Data Engineer has the vision to work with data across multiple systems. This position works independently and is seen as a senior data engineer. The position is responsible for driving the design and development efforts related to architecture, scalability, availability, and performance of data steaming and processing ETL pipeline in alignment with the product/application roadmap. </span> </p> <p> </p> <p> <b> As a Sr Data Engineer, you will be responsible for: </b> </p> <ul> <li> Performing source data mapping and modelling aligned with the product strategy. </li> <li> Performing integration of multiple data source-formats using various ETL tools, scripts, and stored procedures. </li> <li> Implementing different data integration methods like integration via API endpoints. </li> <li> Performing a variety of data transformations to support analytics models. </li> <li> Writing unit tests and integration tests to validate quality of data ETL process. </li> <li> Improving and automating existing ETL process for performance and scalability. </li> </ul> <p> <b> <span> <span> </span> </span> </b> </p> <p> <b> <span> Fuel your passion </span> </b> </p> <p> <b> <span> </span> To be successful in this role you will: </b> </p> <ul> <li> Be a Graduate in Computer Engineering. A minimum 5 yrs of professional experience </li> <li> Have experience in with Python and data ETL process with hands on experience of data source identification, mapping, ingestion, modelling and transformation for analytical consumption purposes. </li> <li> Have good understanding of logical and physical data models and data lineage. </li> <li> Exposure to working in an Agile environment. </li> <li> Experience of working with cloud deployment and open-source languages like JavaScript/Java would be an added advantage. </li> <li> Experience of working with databases like PostgreSQL or MongoDB would be an added advantage. </li> <li> Strong oral and written communication skills. </li> <li> Strong interpersonal and leadership skills. </li> </ul> </div>",Senior Data Engineer,800000,1200000,https://www.naukri.com/job-listings-senior-data-engineer-baker-hughes-the-network-mumbai-bangalore-bengaluru-5-to-10-years-091222501919,"['Postgresql', 'Analytical', 'Javascript', 'Agile', 'MongoDB', 'Stored procedures', 'Open source', 'Analytics', 'Python', 'Data extraction']","['Mumbai', 'Bengaluru']",Data Engineer,2022-12-09 20:37:14
071222502710,"<div> <p> <b> </b> </p> <ul> <li> As a Data Engineer you will be working with one of the largest and complex warehouse environments  </li> <li> Coordinate with key stakeholders and ensure effective design, development, validation in line with the stakeholder needs and architectural requirements  </li> <li> End to end development of scalable Big Data pipelines and Data streams  </li> <li> Manage various data related objectives such as system configuration and integration on multiple platforms  </li> <li> Develop key metrics for tests on data and ensure integrity of same on data architecture  </li> <li> Adhere to proper documentation based on each of the development activities  </li> <li> Work with huge data sets and exhibit problem solving, analytical and communication abilities  </li> <li> Will be working in a self-directed environment, take up responsibilities and drive them to completion  <br /> </li> </ul> <p> </p> <p> <b> <u> Required Skills  </u> </b> </p> <ul> <li> B.S. degree in a technical discipline.  </li> <li> Problem solving, analytical and good communication abilities  </li> <li> 7+ years of  <span> development experience with Azure Cloud and Big Data Analytic solutions  </span> </li> <li> Hands on experience in  <span> Azure - Data Factory, Data Lake store/Blob storage, SQL DB  </span> </li> <li> Experience in creating Big data Pipelines with Azure components  </li> <li> Good  <span> programming skills in Java, Scala or Python  </span> </li> <li> <span> SQL experience writing stored procedures, tuning indexes and troubleshooting performance bottlenecks  </span> </li> <li> Advanced experience with  <span> Data Warehousing concepts and standards  </span> </li> <li> Experience with  <span> ETL processing or ETL toolset  </span> </li> <li> Data Management concepts and standards (SQL Server, Oracle, Cloud, Hadoop, etc.)  </li> <li> Experience with  <span> Big Data Solutions. Hive, Spark or other frameworks  </span> </li> <li> Experience across cloud implementations and Solutions specially Microsoft Azure  </li> <li> Exposure to large databases, BI applications, data quality and performance tuning.  </li> <li> Practical knowledge of Linux or Unix, a plus  </li> <li> Experience in Snowflake (cloud data warehouse), a plus  </li> <li> Experience with MS Dynamics data integration and development, a plus  </li> <li> Experience with Oracle - JDE ERP, a plus  <strong> <em> .  </em> </strong> </li> </ul> </div>",Senior Data Engineer,600000,1000000,https://www.naukri.com/job-listings-senior-data-engineer-otis-hyderabad-secunderabad-ahmedabad-bangalore-bengaluru-7-to-9-years-071222502710,"['Unix', 'ERP', 'Linux', 'Data management', 'Analytical', 'Data quality', 'Stored procedures', 'Troubleshooting', 'Oracle', 'SQL']","['Ahmedabad', 'Bengaluru', 'Hyderabad']",Data Engineer,2022-12-07 21:03:37
091222900124,"<div><p><strong>Role and Responsibilities:</strong></p></div><div><div><div><div><div><div><ul><li>Creating complex data processing pipelines, as part of diverse, high energy teams</li><li>Designing scalable implementations of the models developed by our Data Scientists</li><li>Being able to deploy models in real-time applications either as part of a microservice(HTTP or RPC) with bounded context or as realtime pipelines producing events in response to user actions on ground</li><li>Hands-on programming based on TDD, usually in a pair programming environment</li><li>Deploying data pipelines in production based on Continuous Delivery practices.</li><li>Able to build and operate Data Pipelines, Build and operate Data Storage, Is familiar with Infrastructure definition and automation in this context. Is aware of adjacent technologies to the ones they have worked on. Good understanding of Data Modelling.</li><li>Involve in building and deploying large scale data processing pipelines in a production environment.</li><li>Experience building data pipelines and data centric applications using distributed storage solutions(including and not limited to HDFS like storage, Elasticsearch, Mongo, Kafka, Postgres/Mysql etc)</li></ul></div></div><div>Job Requirement</div><div><div><div><strong>Technical Competencies:</strong></div><ul><li>Experience in HDFS, S3, NoSql Databases and distributed platforms like Hadoop, Spark, Flink, Hive, Kafka, Oozie, Airflow, Elasticsearch etc.</li><li>Experience in any of MapR, Cloudera, and HortonWorks and/ or cloud based Hadoop Distributions(GCP preferred).</li><li>Experience creating and building data centric application involving ML models</li></ul><div><strong>Functional / Behavioural Competencies:</strong></div><ul><li>Actively seeking to learn newer tech and curiously experimenting is a trait that would be preferred.</li><li>Excellent understanding of technology landscape</li><li>Learning ability: Applies theoretical knowledge to practice</li><li>Focus on excellence</li><li>Mentoring team mates</li></ul><div><strong>Education & Experiences:</strong></div><ul><li>B. Tech, M. Tech (in Computer Sciences preferred)</li><li>Around 3+ years of experience. For transitioned data engineers over all experience of 5+ years in preferred.</li></ul><div><strong>Interview Process:</strong></div><ul><li>Round 1 – Assignment</li><li>Round 2 – Technical Discussion 1</li><li>Round 3 – Technical Discussion 2/ Managerial Round</li><li>Round 4 – HR Round</li></ul><div><strong>\</strong></div></div></div></div></div></div></div>",Senior Data Engineer Data,500000,700000,https://www.naukri.com/job-listings-senior-data-engineer-data-roppen-transportation-services-private-limited-bangalore-bengaluru-3-to-5-years-091222900124,"['Hive', 'Elasticsearch', 'Mongo', 'Mysql', 'Kafka', 'Postgres', 'Spark', 'Flink']",['Bengaluru'],Data Engineer,2022-12-09 02:07:19
101222500646,"<div> <p> <br /> Manages key elements of one or more data engineering projects. Team member in defining and builds the data pipelines that will enable faster, better, data-informed decision-making within the business. Supports the implementation of a new big data solution which supports high amounts and velocity of data and supports future growth.  </p> <p> Works with latest developments, testing and deployment techniques to support the deployment of new releases to e.g. deploy new data pipelines and add data sources. Delivers on responsibilities for Sprint deliverables, when required. </p> <p> <br /> Guides stakeholders in understanding analyses / outcomes and using them. Helps the business to find the right answer to solve their business problems, to determine feasible solutions contributing to develop new business models. Influential at team level through oral presentations, business cases and investment proposals. </p> <p> <br /> Supports stakeholders by gathering, analysing and modelling data and key performance indicators of moderate to high complexity. Working as a key member of a Team to develop and deploy innovative big data platforms for advanced analytics and data processing. </p> <p> <br /> Works under broad guidance and supervision by Manager / Team Lead for complex activities. <br /> <br /> This job profile contains generic information and does not describe individual positions or required job competencies. Grading decisions will also depend on other factors. </p> <p> </p> </div>",Data Engineer,1200000,1600000,https://www.naukri.com/job-listings-data-engineer-shell-pvt-ltd-bangalore-bengaluru-3-to-6-years-101222500646,"['advanced analytics', 'HP data protector', 'Engineering projects', 'Data processing', 'Business modeling', 'Representative', 'big data', 'Information technology', 'Supervision', 'Recruitment']",['Bengaluru'],Data Engineer,2022-12-10 23:47:23
101222500643,"<div> <p> <br /> Contributes to multiple data engineering projects. Team member in defining and builds the data pipelines that will enable faster, better, data-informed decision-making within the business. Supports the implementation of a new big data solution which supports high amounts and velocity of data and supports future growth.  </p> <p> Works with latest developments, testing and deployment techniques to support the deployment of new releases to e.g. deploy new data pipelines and add data sources. Acts as part of a Scrum, or as a Scrum Stakeholder (e.g. Agile White belt accredited) when required. <br /> Supports stakeholders by gathering, analysing and modelling data key performance indicators to develop quantitative and qualitative business insights in an agile fashion. Identifies and proposes opportunities for improved data analysis. Opinions valued within team and by project manager. <br /> Supports stakeholders by gathering, analysing and modelling data and key performance indicators of moderate complexity. Working as part of a Team to develop and deploy innovative big data platforms for advanced analytics and data processing. <br /> Works under direct supervision of Manager / Team Lead for moderately complex activities; broad guidance on operational activities. <br /> <br /> This job profile contains generic information and does not describe individual positions or required job competencies. Grading decisions will also depend on other factors. </p> <p> </p> </div>",Associate Data Engineer,1000000,1400000,https://www.naukri.com/job-listings-associate-data-engineer-shell-pvt-ltd-bangalore-bengaluru-2-to-4-years-101222500643,"['Data analysis', 'HP data protector', 'Agile', 'Engineering projects', 'Data processing', 'Scrum', 'Operations', 'Information technology', 'Recruitment', 'Python']",['Bengaluru'],Data Engineer,2022-12-10 23:47:23
091222501117,"<div> <ul> <li> <span> <span> Technology champion who constantly pursues knowledge enhancement and has inherent curiosity to understand work from multiple dimensions.  </span> </span> <span> </span> </li> </ul> <ul> <li> <p> <span> <span> Interest and passion in Big Data technologies and appreciates the value that can be brought in with an effective data management solution.  </span> </span> <span> </span> </p> </li> <li> <p> <span> <span> Has worked on real data challenges and handled  <span> </span> </span> <span> high  </span> <span> <span> </span> volume, velocity and variety of data.  </span> </span> <span> </span> </p> </li> <li> <p> <span> <span> Deep data focus with expertise in one of more technologies in the data domain (Azure Databricks,  <span> </span> </span> <span> Alteryx,  <span> </span> </span> <span> PySpark  </span> <span> ,  <span> </span> </span> <span> ADF ,  </span> <span> <span> </span> Kafka, SAP Hana  </span> <span> <span> </span> </span> <span> etc  </span> <span> )  </span> <span> .  <span> </span> </span> <span> Proficient with data warehousing and traditional ETL technologies.  </span> </span> <span> </span> </p> </li> <li> <p> <span> <span> Understands Infrastructure concepts and has worked with multiple cloud tenants.  </span> </span> <span> </span> </p> </li> <li> <p> <span> <span> Knowledge on latest database technologies like MongoDB, Cosmos, Cassandra etc  </span> </span> <span> </span> </p> </li> <li> <p> <span> <span> Knowledge on data analytics and data Visualization tools like Power  <span> </span> </span> <span> BI ,  </span> <span> <span> </span> Spotfire etc  </span> </span> <span> </span> </p> </li> <li> <p> <span> <span> Excellent analytical problem-solving skills, willingness to take ownership and resolve technical challenges.  </span> </span> <span> </span> </p> </li> <li> <p> <span> <span> Excellent communication and stakeholder management skills  </span> <span> .  </span> </span> <span> </span> </p> </li> </ul> <div> <p> <span> </span> </p> </div> <div> <p> <span> <span> <strong> Other  <span> </span> </strong> </span> <span> <strong> Dimensions : </strong> </span> </span> <span> </span> </p> </div> <div> <ul> <li> <p> <span> <span> Minimum  <span> </span> </span> <span> 2 -4  </span> <span> <span> </span> year  </span> <span> s of experience in the IT industry  </span> </span> <span> </span> </p> </li> <li> <p> <span> <span> University degree in any IT discipline  </span> </span> </p> </li> </ul> </div> </div>",Data Engineer,1200000,1600000,https://www.naukri.com/job-listings-data-engineer-shell-pvt-ltd-bangalore-bengaluru-2-to-4-years-091222501117,"['data domain', 'Data management', 'HP data protector', 'Analytical', 'Data analytics', 'data visualization', 'Cosmos', 'Stakeholder management', 'Information technology', 'Recruitment']",['Bengaluru'],Data Engineer,2022-12-09 19:15:23
091222501116,"<div> <ul> <li> Communicate with both technical developers and business managers and gain respect and trust of leaders and staff.  </li> <li> Actively deliver the roll-out and embedding of Data Foundation initiatives in support of the key business programs advising on the technology and using leading market standard tools.  </li> <li> Coordinate the change management process, incident management and problem management process.  </li> <li> Ensure traceability of requirements from Data through testing and scope changes, to training and transition.  </li> <li> Drive implementation efficiency and effectiveness across the pilots and future projects to minimize cost, increase speed of implementation and maximize value delivery.  </li> </ul> <p> </p> <p> </p> <p> </p> <p> <b> <span> Education  </span> </b> </p> <ul> <li> University degree in any IT discipline  </li> <li> Minimum 5+ years of experience in the IT industry  </li> </ul> <p> </p> <p> <b> <span> Expected Skills  </span> </b> </p> <ul> <li> Proven technology champion in working with relational, Data warehouses databases, query authoring (SQL) as well as working familiarity with a variety of databases. Experience/Knowledge in working with NoSQL databases and can create E2E pipelines.  </li> <li> Experienced in building and optimizing complex queries. Good with manipulating, processing and extracting value from large, disconnected datasets.  </li> <li> Proven Experience in working with  <b> <span> any one of the  </span> </b> data engineering technologies like ADLS, ADF, Azure Databricks, Azure SQL, Synapse, SAP HANA, AWS. Your experience in handling big data sets and big data technologies will be an asset.  </li> <li> Proven champion with in-depth knowledge of  <b> <span> any one of the  </span> </b> scripting languages: Python, SQL, Spark-SQL/ PySpark.  </li> <li> Very Good understanding in Data Foundation initiatives, like Modelling, Data Quality Management, Data Governance, Data Maturity Assessments and Data Strategy in support of the key business stakeholders.  </li> <li> Experience in working with AGILE, KANBAN methodologies. Able to run a sprint.  </li> <li> Communication Skills to engage both technical developers, Architects, and stakeholders.  </li> </ul> <p> </p> <p> <b> <span> Nice to Have Skills  </span> </b> </p> <ul> <li> Previously created or enhanced CI/CD build and releases pipelines.  </li> <li> Experience in working with scripting languages such as YAML, PowerShell, Terraform etc  </li> <li> Experience with big data tools: Kafka, Hadoop, Spark and similar technologies. Experience with stream-processing systems  </li> <li> Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow and more.  </li> <li> Core Python skills like: Numpy, Panda, Django  </li> <li> Certification in Azure  </li> </ul> </div>",Data Engineer,1200000,1600000,https://www.naukri.com/job-listings-data-engineer-shell-pvt-ltd-bangalore-bengaluru-5-to-7-years-091222501116,"['Change management', 'HP data protector', 'Problem management', 'Agile', 'Incident management', 'Data quality', 'Information technology', 'Recruitment', 'SQL', 'Python']",['Bengaluru'],Data Engineer,2022-12-09 19:15:23
091222501115,"<div> <ul> <li> Manages key elements of one or more data engineering projects. Team member in defining and builds the data pipelines that will enable faster, better, data-informed decision-making within the business. Supports the implementation of a new big data solution which supports high amounts and velocity of data and supports future growth. Works with latest developments, testing and deployment techniques to support the deployment of new releases to eg deploy new data pipelines and add data sources. Delivers on responsibilities for Sprint deliverables, when required.  </li> <li> Guides stakeholders in understanding analyses / outcomes and using them. Helps the business to find the right answer to solve their business problems, to determine feasible solutions contributing to develop new business models. Influential at team level through oral presentations, business cases and investment proposals.  </li> <li> Supports stakeholders by gathering, analysing and modelling data and key performance indicators of moderate to high complexity. Working as a key member of a Team to develop and deploy innovative big data platforms for advanced analytics and data processing.  </li> <li> Works under broad guidance and supervision by Manager / Team Lead for complex activities.  </li> <li> This job profile contains generic information and does not describe individual positions or required job competencies. Grading decisions will also depend on other factors.  </li> </ul> <ul> <li> We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:  </li> <li> Experience with Azure: ADF, ADLS, Databricks ,PySpark, SQL , SQL DW, COSMOS DB, Analysis Services, Azure Functions, ARM Templates  <br /> Experience with relational SQL and NoSQL databases,  <br /> Experience with object-oriented/object function scripting languages: Python, SQL, Scala, Spark-SQL etc  </li> </ul> </div>",Data Engineer,1200000,1600000,https://www.naukri.com/job-listings-data-engineer-shell-pvt-ltd-bangalore-bengaluru-3-to-6-years-091222501115,"['NoSQL', 'HP data protector', 'SCALA', 'Data processing', 'Information technology', 'Analysis services', 'SQL', 'Python', 'Recruitment']",['Bengaluru'],Data Engineer,2022-12-09 19:15:23
111222000017,"<p> </p><ul><li>Should have experience in Cloud, Data Management with specialization in Snowflake</li><li>Extensive experience architecting, designing and migrating workloads to Cloud      environment or Platform upgradations.</li><li>Hands on experience on Snowflake platform</li><li>Experience in Data ingestion to Snowflake from CSV, JSON, Parquet, Avro etc format</li><li>Experience in Data Lineage Analysis, Data Profiling, Mapping, Integration, ETL, Validation,      Cleansing, Masking, Sub setting, Profiling, Archiving, Purging, Virtualization & Metadata Management.</li><li>Experience writing ETL routines for Snowflake in Python or Scala or with ETL tool; Experience      in writing SnowSQL</li><li>AWS or Azure Platform deep knowledge.</li><li>Experience in Designing logical and physical data models</li><li>Experience in Estimation, sizing, database and schema designs.</li><li>Experience in Appliances like Teradata, Oracle Exadata, Netezza</li><li>Experience in Implementation and leading end to end projects - </li><li>Should have Strong analytical and problem-solving skills and should Provide guidance to      teams.</li></ul>",Data Engineer- Snowflake,1500000,3000000,https://www.naukri.com/job-listings-data-engineer-snowflake-fractal-analytics-bangalore-bengaluru-7-to-12-years-111222000017,"['ETL', 'Data warehousing']",['Bengaluru'],Data Engineer,2022-12-11 00:22:44
091222500867,"<div> <ul> <li> Ability to process and rationalize structured data and ability to integrate multiple large data sources and databases into one system  </li> <li> Proficient understanding of distributed computing principles and of the fundamental design principles behind a scalable application  </li> <li> Strong knowledge of the Big Data ecosystem  </li> <li> Practical experience in using HDFS or big data cloud storage  </li> <li> Practical expertise in developing applications and using querying tools on top of Hive, Spark (PySpark or ScalaSpark)  </li> <li> Strong Scala skills  </li> <li> Good communication skills (written and spoken), ability to engage with different stakeholders and to synthesise different opinions and priorities.  </li> <li> Experience in Python  </li> <li> Hands on experience in data migration project experience using  <strong> Snowflake  </strong> preferred  </li> <li> Hands on experience in  <strong> Modern data Platform  </strong> preferred  </li> </ul> <p> <u> <strong> Nice to Haves (not requirements):  </strong> </u> </p> <ul> <li> Knowledge of a Python web framework (preferably: Flask, Tornado, or twisted)  </li> <li> Basic understanding of front-end technologies, such as JavaScript, HTML/CSS  </li> <li> Knowledge of Elastic Search Stack (ELK)  </li> <li> Knowledge of and experience using data models and data dictionaries in a regulatory context. eg, Banking and Financial Markets  </li> </ul> </div>",Data Engineer,700000,1100000,https://www.naukri.com/job-listings-data-engineer-capco-technologies-pvt-ltd-pune-bangalore-bengaluru-9-to-11-years-091222500867,"['Data migration', 'Front end', 'Financial markets', 'Diversity and Inclusion', 'Javascript', 'cloud storage', 'HTML', 'big data', 'Financial services', 'Python']","['Pune', 'Bengaluru']",Data Engineer,2022-12-09 19:15:02
071222907267,"<span> <span> <span> <span> Be involved in data engineering activities like Creating pipelines / workflows for Source to Target etc. </span> </span> </span> </span> <p> </p> <p> <span> <span> <span> <span> Responsibilities: </span> </span> </span> </span> </p> <ul> <li> <span> <span> <span> <span> You will be involved in the Azure Data platform implementation. Willing to work in shift timings 2PM to 11PM </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Responsible for assessing feasibility of migrating customer solutions and/or integrating with 3rd party systems both Microsoft and non-Microsoft platforms </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Designing and developing Data Pipelines for Data Ingestion or Transformation using Python (PySpark)/Spark SQL </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Work with version control GitHub and CI/CD pipelines using Azure DevOps </span> </span> </span> </span> </li> </ul> <p> <span> <span> <span> <span> If you thrive in a dynamic, collaborative workplace, IBM provides an environment where you will be challenged and inspired every single day. And if you relish the freedom to bring creative, thoughtful solutions to the table, there s no limit to what you can accomplish here. </span> </span> </span> </span> <br /> <span> </span> <br /> <span> Required Technical and Professional Expertise </span> <span> </span> </p> <ul> <li> <span> <span> <span> <span> Total Years of Experience 5 years in IT industry </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> 2 years current and deep experience with Azure Data platform implementation. Proven experience managing projects through the entire project lifecycle. </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Relevant years of experience 2 years in Azure Data Factory, Azure Data Lake, Azure DevOps, Azure DataBricks, AzureSQL </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Proficient in Azure Data Integration, Azure Data Architecture </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Proven experience using the Microsoft Azure Data Stack (ADFv2, Azure SQL DB, Azure SQL Datawarehouse, Azure Data Lake, Azure Databricks, Analysis Services, Cosmos DB) </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Experience in Azure data migration patterns </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Experience in Python, C# is mandatory </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Agile methodology experience essential </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> 3 years experience assessing feasibility of migrating customer solutions and/or integrating with 3rd party systems both Microsoft and non-Microsoft platforms </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Hands on experience Designing and developing Data Pipelines for Data Ingestion or Transformation using Python (PySpark)/Spark SQL </span> </span> </span> </span> </li> </ul> <p> <span> Preferred Technical and Professional Expertise </span> <span> </span> </p> <ul> <li> <span> <span> <span> <span> AZ 900 - Azure Fundamentals </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> DP 200, DP 201, DP 203, AZ 204 - Data Engineering </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> AZ 400 - Devops Certification </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> You love collaborative environments that use agile methodologies to encourage creative design thinking and find innovative ways to develop with cutting edge technologies </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Ambitious individual who can work under their own direction towards agreed targets/goals and with creative approach to work </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Intuitive individual with an ability to manage change and proven time management </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Proven interpersonal skills while contributing to team effort by accomplishing related results as needed </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Up-to-date technical knowledge by attending educational workshops, reviewing publications </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Experience working with version control GitHub and CI/CD pipelines using Azure DevOps </span> </span> </span> </span> </li> </ul>",Data Engineer: Big Data,800000,1200000,https://www.naukri.com/job-listings-data-engineer-big-data-ibm-india-pvt-limited-bangalore-bengaluru-5-to-10-years-071222907267,"['Data migration', 'github', 'Version control', 'devops', 'Cosmos']",['Bengaluru'],Data Engineer,2022-12-07 13:07:43
051222901223,"<span> <span> <span> <span> Be involved in data engineering activities like Creating pipelines / workflows for Source to Target etc. </span> </span> </span> </span> <p> </p> <p> <span> <span> <span> <span> Responsibilities: </span> </span> </span> </span> </p> <ul> <li> <span> <span> <span> <span> You will be involved in the Azure Data platform implementation. Willing to work in shift timings 2PM to 11PM </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Responsible for assessing feasibility of migrating customer solutions and/or integrating with 3rd party systems both Microsoft and non-Microsoft platforms </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Designing and developing Data Pipelines for Data Ingestion or Transformation using Python (PySpark)/Spark SQL </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Work with version control GitHub and CI/CD pipelines using Azure DevOps </span> </span> </span> </span> </li> </ul> <p> <span> <span> <span> <span> If you thrive in a dynamic, collaborative workplace, IBM provides an environment where you will be challenged and inspired every single day. And if you relish the freedom to bring creative, thoughtful solutions to the table, there s no limit to what you can accomplish here. </span> </span> </span> </span> <br /> <span> </span> <br /> <span> Required Technical and Professional Expertise </span> <span> </span> </p> <ul> <li> <span> <span> <span> <span> Total Years of Experience 5 years in IT industry </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> 2 years current and deep experience with Azure Data platform implementation. Proven experience managing projects through the entire project lifecycle. </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Relevant years of experience 2 years in Azure Data Factory, Azure Data Lake, Azure DevOps, Azure DataBricks, AzureSQL </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Proficient in Azure Data Integration, Azure Data Architecture </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Proven experience using the Microsoft Azure Data Stack (ADFv2, Azure SQL DB, Azure SQL Datawarehouse, Azure Data Lake, Azure Databricks, Analysis Services, Cosmos DB) </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Experience in Azure data migration patterns </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Experience in Python, C# is mandatory </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Agile methodology experience essential </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> 3 years experience assessing feasibility of migrating customer solutions and/or integrating with 3rd party systems both Microsoft and non-Microsoft platforms </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Hands on experience Designing and developing Data Pipelines for Data Ingestion or Transformation using Python (PySpark)/Spark SQL </span> </span> </span> </span> </li> </ul> <p> <span> Preferred Technical and Professional Expertise </span> <span> </span> </p> <ul> <li> <span> <span> <span> <span> AZ 900 - Azure Fundamentals </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> DP 200, DP 201, DP 203, AZ 204 - Data Engineering </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> AZ 400 - Devops Certification </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> You love collaborative environments that use agile methodologies to encourage creative design thinking and find innovative ways to develop with cutting edge technologies </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Ambitious individual who can work under their own direction towards agreed targets/goals and with creative approach to work </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Intuitive individual with an ability to manage change and proven time management </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Proven interpersonal skills while contributing to team effort by accomplishing related results as needed </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Up-to-date technical knowledge by attending educational workshops, reviewing publications </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Experience working with version control GitHub and CI/CD pipelines using Azure DevOps </span> </span> </span> </span> </li> </ul>",Data Engineer: Big Data,800000,1200000,https://www.naukri.com/job-listings-data-engineer-big-data-ibm-india-pvt-limited-bangalore-bengaluru-2-to-5-years-051222901223,"['Data migration', 'github', 'Version control', 'devops', 'Cosmos']",['Bengaluru'],Data Engineer,2022-12-05 11:13:18
071222907608,"<span> </span> <p> </p> <ul> <li> <span> <span> <span> <span> Mentor or coach for scrum teams </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Expert into Agile Scrum principals, Task meeting/Retrospective </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Proven in Relative estimation, Story-based development </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Proficient in leading Iteration/sprint planning meeting, Conflict Resolution </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Strong into Business Analysis planning and monitoring, Enterprise Analysis, Requirement management and communication </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Provide objective guidance without personal or political considerations </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Experienced in implementing agile techniques in different cultures and environments </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Focus on people and Improvement by providing team a platform for improving not only during the retro but all the time. Create a safe environment for healthy conflict and meaningful collaboration. </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Experience to provide training to the team on the agile methodologies </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Implement the winning strategy according as per the ground conditions. </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Agile processes in each sprint at user story level as per the Definition of Done (DoD). </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Successfully run agile projects of varying size and complexity </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Identify project risks and raise them dedicatedly </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Agile process during the project execution; (on the ground to answer all the questions immediately). </span> </span> </span> </span> </li> </ul> <p> <span> Required Technical and Professional Expertise </span> <span> </span> </p> <ul> <li> Hadoop, Hive, Spark / PySpark, SQL, Oozie  </li> <li> Data Modelling in Hive  </li> <li> Programming Languages: Java / Python / Scala  </li> <li> Should have done micro / macro designing  </li> <li> Familiar with Unix Commands and basic work experience in Unix Shell Scripting  </li> <li> Data Modeling SQL Scripting (Teradata), BDW Data modelling  </li> <li> Excellent communication skills  </li> <li> Excellent client facing skills </li> </ul>",Data Engineer: Data Modeling,600000,1000000,https://www.naukri.com/job-listings-data-engineer-data-modeling-ibm-india-pvt-limited-ahmedabad-2-to-5-years-071222907608,"['Conflict resolution', 'Programming', 'SQL scripting', 'Unix shell scripting', 'Python']",['Ahmedabad'],Data Engineer,2022-12-07 13:14:29
091222904425,"<div><div><div><b>Role and Responsibilities:</b></div><ul><li>Creating complex data processing pipelines, as part of diverse, high energy teams</li><li>Designing scalable implementations of the models developed by our Data Scientists</li><li>Hands-on programming based on TDD</li><li>Deploying data pipelines in production based on Continuous Delivery practices</li></ul></div></div><div>Job Requirement</div><div><div><div><b>Technical Competencies:</b></div><ul><li>Functional understanding of Java/Python and any Hadoop stack familiarity is a bonus.</li><li>Learning ability : Is self-reflective, Has a hunger to improve, Has a keen interest to drive their own learning. Applies theoretical knowledge to practice</li><li>Like solving problems, designing data structure and Algo.</li><li>Nice to have: Attends and participates in conferences/meetups/user groups, Contributes to open source projects  </li></ul><div><b>Functional / Behavioural Competencies:</b></div><ul><li>Actively seeking to learn newer tech and curiously experimenting is a trait that would be preferred.</li><li>Excellent understanding of technology landscape</li><li>Learning ability: Applies theoretical knowledge to practice</li><li>Focus on excellence</li><li>Mentoring team mates</li></ul><div><b>Education & Experiences:</b></div><ul><li>B. Tech, M. Tech(in Computer Science preferred)</li><li>Around 2+ years of experience. For transitioned data engineers over all experience of 3+ years in preferred.</li></ul></div></div>",Data Engineer Data,500000,700000,https://www.naukri.com/job-listings-data-engineer-data-roppen-transportation-services-private-limited-bangalore-bengaluru-3-to-5-years-091222904425,"['Java', 'TDD', 'Hadoop', 'Python']",['Bengaluru'],Data Engineer,2022-12-09 10:40:04
071222907320,"<span> </span> <p> </p> <p> <span> <span> <span> <span> <span> <span> As Big Data Engineer, you will develop, maintain, evaluate and test big data solutions. You will be involved in data engineering activities like Creating pipelines / workflows for Source to Target etc. </span> </span> </span> </span> </span></span></p> <p> <span> <span> <b> <span> <span> Responsibilities </span> </span> </b> <span> <span> : </span> </span> </span> </span>   </p> <ul> <li> <span> <span> <span> <span> You will be involved in the design of data solutions using Hadoop based technologies along with Hadoop, Azure, HDInsights for Cloudera based Data Late using Scala Programming. </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Responsible to Ingest data from files, streams and databases. Process the data with Hadoop, Scala, SQL Database, Spark, ML, IoT </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Develop programs in Scala and Python as part of data cleaning and processing </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Responsible to design and develop distributed, high volume, high velocity multi-threaded event processing systems </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Develop efficient software code for multiple use cases leveraging Python and Big Data technologies for various use cases built on the platform </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Provide high operational excellence guaranteeing high availability and platform stability </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Implement scalable solutions to meet the ever-increasing data volumes, using big data/cloud technologies Pyspark, Kafka, any Cloud computing etc. </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> If you thrive in a dynamic, collaborative workplace, IBM provides an environment where you will be challenged and inspired every single day. And if you relish the freedom to bring creative, thoughtful solutions to the table, there s no limit to what you can accomplish here. </span> </span> </span> </span> </span> </span> </li> </ul> <p> <span> Required Technical and Professional Expertise </span> <span> </span> </p> <ul> <li> <span> <span> <span> <span> Minimum 7 years of experience in Big Data technologies </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Proficient in any of the programming languages - Python, Scala or Java </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Mandatory experience in Mid to Expert Level programming capabilities in a large-scale enterprise </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> In-depth experience in modern data platform components such as the Hadoop, Hive, Pig, Spark, Python, Scala, etc </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Experience with Distributed Versioning Control environments such as GIT </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Familiarity with development tools - experience on either IntelliJ / Eclipse / VSCode IDE, Build Tool Maven </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Demonstrated experience in modern API platform design including how modern UI s are built consuming services / APIs. </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Experience on Azure cloud including Data Factory, Databricks, Data Lake Storage is highly preferred </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Solid experience in all phases of Software Development Lifecycle - plan, design, develop, test, release, maintain and support, decommission </span> </span> </span> </span> </li> </ul> <p> <span> Preferred Technical and Professional Expertise </span> <span> </span> </p> <ul> <li> <span> <span> <span> <span> AZ 900 - Azure Fundamentals </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> DP 200, DP 201, DP 203, AZ 204 - Data Engineering </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> AZ 400 - Devops Certification </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> You love collaborative environments that use agile methodologies to encourage creative design thinking and find innovative ways to develop with cutting edge technologies </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Ambitious individual who can work under their own direction towards agreed targets/goals and with creative approach to work </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Intuitive individual with an ability to manage change and proven time management </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Proven interpersonal skills while contributing to team effort by accomplishing related results as needed </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Up-to-date technical knowledge by attending educational workshops, reviewing publications </span> </span> </span> </span> </li> </ul>",Data Engineer: Big Data,800000,1200000,https://www.naukri.com/job-listings-data-engineer-big-data-ibm-india-pvt-limited-hyderabad-secunderabad-7-to-12-years-071222907320,"['Interpersonal skills', 'GIT', 'Time management', 'Hadoop', 'Software development life cycle']",['Hyderabad'],Data Engineer,2022-12-07 13:08:41
061222904840,"<span> <span> <span> <span> Be involved in data engineering activities like Creating pipelines / workflows for Source to Target etc. </span> </span> </span> </span> <p> </p> <p> <span> <span> <span> <span> Responsibilities: </span> </span> </span> </span> </p> <ul> <li> <span> <span> <span> <span> You will be involved in the Azure Data platform implementation. Willing to work in shift timings 2PM to 11PM </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Responsible for assessing feasibility of migrating customer solutions and/or integrating with 3rd party systems both Microsoft and non-Microsoft platforms </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Designing and developing Data Pipelines for Data Ingestion or Transformation using Python (PySpark)/Spark SQL </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Work with version control GitHub and CI/CD pipelines using Azure DevOps </span> </span> </span> </span> </li> </ul> <p> <span> <span> <span> <span> If you thrive in a dynamic, collaborative workplace, IBM provides an environment where you will be challenged and inspired every single day. And if you relish the freedom to bring creative, thoughtful solutions to the table, there s no limit to what you can accomplish here. </span> </span> </span> </span> <br /> <span> </span> <br /> <span> Required Technical and Professional Expertise </span> <span> </span> </p> <ul> <li> <span> <span> <span> <span> Total Years of Experience 5 years in IT industry </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> 2 years current and deep experience with Azure Data platform implementation. Proven experience managing projects through the entire project lifecycle. </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Relevant years of experience 2 years in Azure Data Factory, Azure Data Lake, Azure DevOps, Azure DataBricks, AzureSQL </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Proficient in Azure Data Integration, Azure Data Architecture </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Proven experience using the Microsoft Azure Data Stack (ADFv2, Azure SQL DB, Azure SQL Datawarehouse, Azure Data Lake, Azure Databricks, Analysis Services, Cosmos DB) </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Experience in Azure data migration patterns </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Experience in Python, C# is mandatory </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Agile methodology experience essential </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> 3 years experience assessing feasibility of migrating customer solutions and/or integrating with 3rd party systems both Microsoft and non-Microsoft platforms </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Hands on experience Designing and developing Data Pipelines for Data Ingestion or Transformation using Python (PySpark)/Spark SQL </span> </span> </span> </span> </li> </ul> <p> <span> Preferred Technical and Professional Expertise </span> <span> </span> </p> <ul> <li> <span> <span> <span> <span> AZ 900 - Azure Fundamentals </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> DP 200, DP 201, DP 203, AZ 204 - Data Engineering </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> AZ 400 - Devops Certification </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> You love collaborative environments that use agile methodologies to encourage creative design thinking and find innovative ways to develop with cutting edge technologies </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Ambitious individual who can work under their own direction towards agreed targets/goals and with creative approach to work </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Intuitive individual with an ability to manage change and proven time management </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Proven interpersonal skills while contributing to team effort by accomplishing related results as needed </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Up-to-date technical knowledge by attending educational workshops, reviewing publications </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Experience working with version control GitHub and CI/CD pipelines using Azure DevOps </span> </span> </span> </span> </li> </ul>",Data Engineer: Big Data,800000,1200000,https://www.naukri.com/job-listings-data-engineer-big-data-ibm-india-pvt-limited-hyderabad-secunderabad-5-to-10-years-061222904840,"['Data migration', 'github', 'Version control', 'devops', 'Cosmos']",['Hyderabad'],Data Engineer,2022-12-06 11:24:07
091222502680,"<div> <ul> <li> <p> Design, develop, optimize, and maintain data architecture and pipelines that adhere to ETL principles and business goals </p> </li> <li> <p> Solve complex data problems to deliver insights that helps the organizations business to achieve their goals </p> </li> <li> <p> Create data products for analytics and data scientist team members to improve their productivity </p> </li> <li> <p> Advise, consult, mentor and coach other data and analytic professionals on data standards and practices </p> </li> <li> <p> Foster a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions </p> </li> <li> <p> Lead the evaluation, implementation and deployment of emerging tools and process for analytic data engineering to improve the organizations productivity as a team </p> </li> <li> <p> Develop and deliver communication and education plans on analytic data engineering capabilities, standards, and processes </p> </li> <li> <p> Partner with business analysts and solutions architects to develop technical architectures for strategic enterprise projects and initiatives. </p> </li> <li> <p> Learn about machine learning, data science, computer vision, artificial intelligence, statistics, and/or applied mathematics </p> </li> </ul> <p> <strong> QUALIFICATIONS & EXPERIENCE </strong> </p> <ul> <li> <p> 5 years of experience working in data engineering or architecture role, 7 preferred (3 years with 5 preferred for junior role) </p> </li> <li> <p> Expertise in SQL and data analysis and experience with at least one programming language (Python or Scala preferred) </p> </li> <li> <p> Experience developing and maintaining data warehouses in big data solutions </p> </li> <li> <p> Experience with developing solutions on cloud computing services and infrastructure in the data and analytics space (preferred) </p> </li> <li> <p> Database development experience using Hadoop or BigQuery and experience with a variety of relational, NoSQL, and cloud database technologies </p> </li> <li> <p> Worked with BI tools such as Tableau, Power BI, Looker, Shiny </p> </li> <li> <p> Experience with advanced technology tools like Python with Pandas/Numpy/Scikit </p> </li> <li> <p> Conceptual knowledge of data and analytics, such as dimensional modeling, ETL, reporting tools, data governance, data warehousing, structured and unstructured data. </p> </li> <li> <p> Big Data Development experience using Hive, Impala, Spark and familiarity with Kafka (Preferred) </p> </li> <li> <p> Familiarity with the Linux operating system (Preferred) </p> </li> <li> <p> Exposure to machine learning, data science, computer vision, artificial intelligence, statistics, and/or applied mathematics </p> </li> <li> <p> Passionate about Agile software processes, data-driven development, reliability, and experimentation </p> </li> <li> <p> Experience working on a collaborative Agile product team </p> </li> <li> <p> Self-motivated with strong problem-solving and learning skills </p> </li> <li> <p> Flexibility to changes in work direction as the project develops </p> </li> <li> <p> Excellent communication, listening, and influencing skills </p> </li> <li> <p> Demonstrated strong number sense, intellectually curious and willing to adjust position based on additional information </p> </li> <li> <p> Strong work ethic; ability to work at an abstract level and gain consensus </p> </li> <li> <p> Familiarity with SAP,MS Azure, Denodo and power BI is a plus </p> </li> </ul> <p> <strong> Education </strong> </p> <ul> <li> Bachelors degree required; Computer Science, MIS, or Engineering preferred </li> </ul> </div>",Senior Data Engineer,500000,800000,https://www.naukri.com/job-listings-senior-data-engineer-bd-bangalore-bengaluru-5-to-10-years-091222502680,"['Cloud computing', 'Data analysis', 'SAP', 'Linux', 'MIS', 'Machine learning', 'Agile', 'SQL', 'Python']",['Bengaluru'],Data Engineer,2022-12-09 21:01:07
101222500830,"<div> <p> <strong> </strong>   </p> <ul> <li> The job duties and requirements are defined for the role of Informatica IICS data engineer. </li> </ul> <ul> <li> The senior role provides technical leadership and mentorship to junior team members. </li> </ul> <ul> <li> The candidate should have relevant experience working in at least 3 to 4 end to end project involving <br /> IICS. </li> </ul> <ul> <li> This position ensures the performance of all duties in accordance with the company s policies and <br /> procedures, all U.S. state and federal laws and regulations, wherein the company operates. </li> </ul> <p> <strong> Job Specification / Skills and Competencies </strong> </p> <ul> <li> Design, develop, and support Data Integration applications and infrastructure utilizing various <br /> technologies, primarily Informatica Cloud services, to process large volumes of data. </li> </ul> <ul> <li> Should have hands on experience in using Informatica Cloud services like Data Integration, Application <br /> Integration, B2B Exchange, Data Quality, API Management etc. </li> </ul> <ul> <li> Create and test simple/complex mapping tasks and task flows, debug issues and implement performance <br /> optimization techniques. </li> </ul> <ul> <li> Deep knowledge on installation of secure agents, configuring agent server, encryption, and <br /> authentication protocols in different Informatica cloud services, recommending optimal values for task <br /> level parameters for better memory utilization and performance improvement of data flows. </li> </ul> <ul> <li> Perform administrator activities like user account management, setting up permissions, creating <br /> connections, metering, license upgrades, product upgrades </li> </ul> <ul> <li> To adhere to the Information Security Management policies and procedures. </li> </ul> </div>",Senior IICS Data Engineer,600000,900000,https://www.naukri.com/job-listings-senior-iics-data-engineer-experionglobal-kochi-cochin-bangalore-bengaluru-trivandrum-thiruvananthapuram-3-to-6-years-101222500830,"['Application integration', 'PDF', 'Cloud Services', 'Information security management', 'Performance optimization', 'Data quality', 'Account management', 'Informatica', 'Performance improvement', 'Metering']","['Trivandrum', 'Kochi', 'Bengaluru']",Data Engineer,2022-12-10 23:47:38
160922005904,"<p><strong>Required Skill: </strong></p><br /><p> Minimum 3-5 years of relevant and proven experience as an Data engineer.</p><p>Good knowledge of Configuring and working on Multi-node clusters and distributed data processing framework Spark. Python and PySpark, Spark SQL.</p><p>Experience in process orchestration tools Apache Airflow, Apache NiFi etc. --(Good to have)</p><p>Experience in crafting scalable data pipelines, sophisticated event processing, analytics components using big data technology (Spark, Python, PySpark)</p><p>Knowledge on DevOps concepts and good to have exposure on CICD tools like Jenkins, CircleCI ,Airflow etc.</p><p>Worked on tools like Docker, Kubernetes. ---(Good to have)</p><br /><p><strong>Key Competencies & Personal Skills</strong></p><ul><li>Excellent organization skills, attention to detail, and ability to multi-task.</li><li>Demonstrated sense of responsibility and capability to deliver quickly.</li><li>Excellent communication skills.</li><li>Proactive problem-solver. Relationship builder and team player.</li><li>Flexibility to work in multiple complex projects and changing priorities.</li><li>Excellent interpersonal skill.</li><li>Flexible to learn.</li><li>Adaptability to continuously changing team requirements.</li><li>Strong analytical thinking and problem-solving skills</li></ul><br /><p><strong>Key Skills:</strong></p><ul><li>Work on robust, secure and architectural environment.</li><li>Opportunity to work in a technical satisfying environment.</li><li>Flawless blend of volume & niche to enrich your entire experience & add starts to your career.</li><li>Young fast-growing team.</li></ul><br /><br />",Data Engineer,1000000,2000000,https://www.naukri.com/job-listings-data-engineer-xoriant-kochi-cochin-pune-ahmedabad-gurgaon-gurugram-chennai-bangalore-bengaluru-mumbai-all-areas-7-to-12-years-160922005904,[],"['Chennai', 'Pune', 'Ahmedabad', 'Bengaluru', 'Kochi', 'Gurgaon', 'Mumbai (All Areas)']",Data Engineer,2022-12-02 20:45:18
281122001510,<p><strong>Roles and Responsibilities</strong> </p><br /><br /><p><strong>Desired Candidate Profile</strong> </p><br /><br /><p><strong>Perks and Benefits</strong> </p><br /><br />,Data Engineer,1000000,2000000,https://www.naukri.com/job-listings-data-engineer-v-soft-consulting-kolkata-hyderabad-secunderabad-pune-ahmedabad-chennai-bangalore-bengaluru-delhi-ncr-mumbai-all-areas-5-to-7-years-281122001510,[],"['Chennai', 'Pune', 'Delhi NCR', 'Ahmedabad', 'Bengaluru', 'Hyderabad', 'Kolkata', 'Mumbai (All Areas)']",Data Engineer,2022-12-07 15:01:44
301122913375,"<span> <span> <span> <span> <span> <span> As Big Data Engineer, you will develop, maintain, evaluate and test big data solutions. You will be involved in data engineering activities like Creating pipelines / workflows for Source to Target etc. </span> </span> </span> </span> </span> </span> <p> </p> <p> <span> <span> <span> <span> <span> <span> Responsibilities: </span> </span> </span> </span> </span> </span> </p> <ul> <li> <span> <span> <span> <span> You will be involved in the design of data solutions using Hadoop based technologies along with Hadoop, Azure, HDInsights for Cloudera based Data Late using Scala Programming. </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Responsible to Ingest data from files, streams and databases. Process the data with Hadoop, Scala, SQL Database, Spark, ML, IoT </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Develop programs in Scala and Python as part of data cleaning and processing </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Responsible to design and develop distributed, high volume, high velocity multi-threaded event processing systems </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Develop efficient software code for multiple use cases leveraging Python and Big Data technologies for various use cases built on the platform </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Provide high operational excellence guaranteeing high availability and platform stability </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Implement scalable solutions to meet the ever-increasing data volumes, using big data/cloud technologies Pyspark, Kafka, any Cloud computing etc. </span> </span> </span> </span> </li> </ul> <p> <span> <span> <span> <span> <span> <span> If you thrive in a dynamic, collaborative workplace, IBM provides an environment where you will be challenged and inspired every single day. And if you relish the freedom to bring creative, thoughtful solutions to the table, there s no limit to what you can accomplish here. </span> </span> </span> </span> </span> </span> <br /> <span> </span> <br /> <span> Required Technical and Professional Expertise </span> <span> </span> </p> <ul> <li> <span> <span> Overall industry experience of 12 years </span> </span> </li> <li> <span> <span> <span> <span> Proficient in any of the programming languages - Python, Scala or Java </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Mandatory experience in Mid to Expert Level programming capabilities in a large-scale enterprise </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> In-depth experience in modern data platform components such as the Hadoop, Hive, Pig, Spark, Python, Scala, etc </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Experience with Distributed Versioning Control environments such as GIT </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Familiarity with development tools - experience on either IntelliJ / Eclipse / VSCode IDE, Build Tool Maven </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Demonstrated experience in modern API platform design including how modern UI s are built consuming services / APIs. </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Experience on Azure cloud including Data Factory, Databricks, Data Lake Storage is highly preferred </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Solid experience in all phases of Software Development Lifecycle - plan, design, develop, test, release, maintain and support, decommission </span> </span> </span> </span> </li> </ul> <p> <span> Preferred Technical and Professional Expertise </span> <span> </span> <br /> <span> <span> <span> <span> <span> <span> Good to have at least one of the Certifications listed here: </span> </span> </span> </span> </span> </span> </p> <ul> <li> <span> <span> <span> <span> AZ 900 - Azure Fundamentals </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> DP 200, DP 201, DP 203, AZ 204 - Data Engineering </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> AZ 400 - Devops Certification </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> You love collaborative environments that use agile methodologies to encourage creative design thinking and find innovative ways to develop with cutting edge technologies </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Ambitious individual who can work under their own direction towards agreed targets/goals and with creative approach to work </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Intuitive individual with an ability to manage change and proven time management </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Proven interpersonal skills while contributing to team effort by accomplishing related results as needed </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Up-to-date technical knowledge by attending educational workshops, reviewing publications </span> </span> </span> </span> </li> </ul>",Data Engineer: Big Data,800000,1200000,https://www.naukri.com/job-listings-data-engineer-big-data-ibm-india-pvt-limited-bangalore-bengaluru-12-to-16-years-301122913375,"['Interpersonal skills', 'GIT', 'Time management', 'Hadoop', 'Software development life cycle']",['Bengaluru'],Data Engineer,2022-11-30 18:18:39
071222905404,"<p>We are looking for a Sr. Data Engineer SQL+Python to join our team on immediate basis (Next 30 days, max) with below mentioned responsibilities :<br /><br />Note - You must be available to join within next 2-4 weeks<br /><br />Roles and Responsibilities : <br /><br />- Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using Python & SQL.<br /><br />- Work with Data Scientists/BI teams to assist with data-related technical issues and support their data processing needs.<br /><br />- Build analytical tools that utilize the data pipeline to provide actionable insights into key business performance metrics.<br /><br />Mandatory Skills : <br /><br />- Proficient in Python (min 4 Yrs) , and SQL.<br /><br />- Good understanding of Relational databases, Big data environments<br /><br />- Proficient in building end-to-end data pipelines.<br /><br />- Experience Handling end to end development of projects without any additional support<br /><br />- Excellent communication skills and proven ability to handle customers.<br /><br />Good to Have : <br /><br />- Any experience with SAS/R<br /><br />- Experience using Rest/Soap apis for data movement.<br /><br />- Any experience with Spark/Scala</p>",Senior Data Engineer - SQL & Python,800000,1400000,https://www.naukri.com/job-listings-senior-data-engineer-sql-python-zorba-consulting-india-pvt-ltd-kolkata-mumbai-hyderabad-secunderabad-pune-chennai-ahmedabad-bangalore-bengaluru-delhi-ncr-3-to-5-years-071222905404,"['spark', 'Analytical', 'SAS R', 'SCALA', 'Relational databases', 'Infrastructure', 'Data processing', 'big data', 'Catering', 'Python']","['Chennai', 'Pune', 'Delhi NCR', 'Mumbai', 'Ahmedabad', 'Bengaluru', 'Hyderabad', 'Kolkata']",Data Engineer,2022-12-07 11:18:57
051222905290,"Role Description: <p> Data engineers to build data analytical solutions that will address increasingly complex business questions. Data Engineer will be a hands-on person responsible for designing, prototyping and implementing data products that support a wide variety of data processing, data science and analytics needs. Data Engineers work closely with data scientists, product managers, data platform team to understand the functional data requirements and leverage the underlying tech stack to come up with scalable, robust data mapplications which can crunch terabytes of data in real-time. Data products and applications you build will enable data driven decision making across business, analytics and operations. </p> <p> You should have expertise in designing, implementing, and operating stable, scalable, solutions to flow data from production systems into analytical data platform (big data tech stack MPP) and into end-user facing applications for both real-time and batch use cases </p> <p> </p> <p> The ability to do deep problem solving and build elegant, maintainable solutions to complex problems. Designing platforms as consumable data services across the organization using Big Data tech stack. Do high level design independently; Functional modelling, break-down of a module </p> <p> </p> <p> Influence product requirements & operational plans. Instill best practices for development and champion their adoption, while working with product managers to estimate and plan projects in agile development framework. </p> <p> </p> <p> Architectural & Design Choices, Deep knowledge on one or more tech stacks, identify alternative tech choices and trade-offs. Build and execute data modeling projects across multiple tech stacks i.e. big data, MPP, OLAP using agile development techniques </p> <p> </p> <p> Strong problem Solving skills, Identify feasible alternatives and freeze on the optimal choice of design approach. Strong engineering mindset - build automated monitoring, alerting, self healing (restartability/graceful failures) features while building the consumption pipelines. </p> <p> </p> <p> Challenge status quo and propose innovative ways to process, model, consume data when it comes to tech stack choices or design principles. Translate business requirements into technical specification (fact / dimension / filters / derivations / aggregations) </p> <p> </p> <p> Lead by example, mentor and guide team members on everything from structured problem solving to development of best practices </p> <p> An ideal candidate will have excellent communication skills to be able to work with engineering, product and business owners to develop and define key business questions and to build data sets that answer those questions. you should bring your passion for working with huge data sets and bringing datasets together to answer business questions and drive change </p> Skills Required (What you ll need): <p> </p> <ul> <li> 5-8 years' experience with a Bachelor s Degree in Computer Science, Engineering, Technology or related field required. 3 to 4 years of relevant software development experience with sound skills in database modeling (relational, multi-dimensional) & optimization and data architecture - databases e.g. Vertica </li> <li> In depth understanding of streaming technologies like Kafka, Spark Streaming. Proficiency in one of the programming languages preferably Java, Scala or Python. Good knowledge of Agile, SDLC/CICD practices and tools with good understanding of distributed systems </li> <li> Experience with Enterprise Business Intelligence Platform/Data platform sizing, tuning, optimization and system landscape integration in large-scale, enterprise deployments. </li> <li> Must have proven experience with Hadoop, Mapreduce, Hive, Spark, Scala programming. Must have in-depth knowledge of performance tuning/optimizing data processing jobs, debugging time consuming jobs. </li> <li> Proven experience in development of conceptual, logical, and physical data models for Hadoop, relational, EDW (enterprise data warehouse) and OLAP database solutions. </li> <li> Experience working extensively in multi-petabyte DW environments. Experience in engineering large-scale systems in a product environment </li> <li> Previous work experience:  <b> 5  </b> <b> </b> <b> Years </b> <b> </b> <b> of </b> <b> </b> <b> experience. </b> <b> </b> </li> </ul> <p> <b> Education </b> <b> </b> <b> Qualifications: </b> <b> </b> BE/B.Tech </p>",Data Engineer III,800000,1200000,https://www.naukri.com/job-listings-data-engineer-iii-flipkart-internet-private-limited-bangalore-bengaluru-5-to-8-years-051222905290,"['Performance tuning', 'Debugging', 'Agile', 'OLAP', 'Recruitment']",['Bengaluru'],Data Engineer,2022-12-05 16:24:01
071222502163,"<div> <div> <ul> <li> Create and maintain optimal data pipeline architecture </li> <li> Assemble large, complex data sets that meet functional / non-functional business requirements. </li> <li> Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc </li> <li> Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using AWS big data technologies. </li> <li> Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency, and other key business performance metrics. </li> <li> Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. </li> <li> Keep our data separated and secure across national boundaries through multiple data centers and AWS regions. </li> <li> Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader. </li> <li> Work with data and analytics experts to strive for greater functionality in our data systems. </li> </ul> <p> </p> <p> <strong> Wedon t need superheroes, just super minds </strong> </p> <p> </p> <ul> <li> Exceptional programming skills with Python or PySpark. </li> <li> Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases and NoSQL database. </li> <li> Working knowledge of AWS services such as Glue, Lambda and CloudFormation etc  </li> <li> Experience building and optimizing big data data pipelines, architectures (preferably in Git) </li> <li> Strong analytic skills related to working with unstructured datasets. </li> <li> Build processes supporting data transformation, data structures, metadata, dependency and workload management. </li> <li> A successful history of manipulating, processing and extracting value from large, disconnected datasets. </li> <li> Strong Communication along with project management and organizational skills. </li> <li> Experience supporting and working with cross-functional teams in a dynamic environment. </li> </ul> </div> </div>",Data Engineer,600000,1000000,https://www.naukri.com/job-listings-data-engineer-siemens-limited-bangalore-bengaluru-1-to-4-years-071222502163,"['Supply chain', 'Business services', 'Customer acquisition', 'metadata', 'Project management', 'Data structures', 'Business solutions', 'Operations', 'Analytics', 'SQL']",['Bengaluru'],Data Engineer,2022-12-07 19:48:29
160922500011,"<div> We are looking for a  <strong> <span> Staff Data Engineer  </span> </strong> to join the Data Engineering team, who will be responsible for integrating data into our Data Activation Platform from customers clinical, claims, and other data sources, executing analytical jobs and powering up the Innovaccer suite of products. You will work closely with customers to build data and analytics solutions to support their business needs, and be the engine that powers the partnership that we build with them.  <br /> In this role, you will work closely with a team of Data Engineers, in collaboration with the Product & Engineering teams, to implement the entire end-to-end data and analytics workflows on completely cloud-based environments (AWS or Azure), that will support customer objectives.  </div> <div> <div> <p> You'll work with some of the brightest minds in the industry, work with one of the richest healthcare data sets in the world, use cutting-edge technology, and see your efforts affect products and people on a regular basis.  </p> <p> <strong> A Day in the Life  </strong> </p> <ul> <li> Guide and mentor the team in best practices for handling the data.  </li> <li> Review and provide feedback before the deliverables are shared with the customer.  </li> <li> Play with and transform data.  </li> <li> Work towards creating easy-to-digest analytical reports for US healthcare customers.  </li> <li> Design and build interfaces that facilitate workflows between Data Activation Platform and client third party systems as scoped while complying with respective standards and industry best practices.  </li> <li> Define and document best practices along with thorough message specifications.  </li> <li> Monitor and tune the configuration of interfaces for high availability once deployed in production environments.  </li> </ul> <strong> What You Need  </strong> <ul> <li> 7 years in an analytics role in data ETL and analytics/reporting  </li> <li> Strong knowledge of SQL  </li> <li> Data modeling ability - knowledge of different data modeling concepts  </li> <li> File handling and transformations in Python is a good to have  </li> <li> Dashboarding experience (PowerBI/Tableau/Qlikview) will be a strong advantage  </li> <li> Self-starter, curious, accountable, enjoys a healthy level of autonomy, strong work ethic, able to succeed in a fast-paced, high-intensity startup environment  </li> <li> Extensive experience relaying technical and non-technical information in a clear and concise manner  </li> <li> Demonstrated expert problem solving and analytical skills  </li> <li> Excellence in multitasking and managing multiple high-priority customer engagements at once  </li> <li> Ability to assess complex client environments and workflows and arrive at integration solutions that will satisfy seamless experience between our platform and theirs  </li> </ul> </div> <div> </div> </div>",Staff Data Engineer,1000000,1400000,https://www.naukri.com/job-listings-staff-data-engineer-innovaccer-noida-hyderabad-secunderabad-chennai-bangalore-bengaluru-1-to-3-years-160922500011,"['Analytical skills', 'Health insurance', 'Product engineering', 'Claims', 'Data modeling', 'QlikView', 'Subject Matter Expert', 'US healthcare', 'SQL', 'Python']","['Chennai', 'Bengaluru', 'Hyderabad', 'Noida']",Data Engineer,2022-12-07 11:25:17
071222500224,"<div> <p> </p> <ul> <li> Ability to process and rationalize structured data and ability to integrate multiple large data sources and databases into one system  </li> <li> Proficient understanding of distributed computing principles and of the fundamental design principles behind a scalable application  </li> <li> Strong knowledge of the Big Data ecosystem  </li> <li> Practical experience in using HDFS or big data cloud storage  </li> <li> Practical expertise in developing applications and using querying tools on top of Hive, Spark (PySpark or ScalaSpark)  </li> <li> Strong Scala skills  </li> <li> Good communication skills (written and spoken), ability to engage with different stakeholders and to synthesise different opinions and priorities.  </li> <li> Experience in Python  </li> <li> Hands on experience in data migration project experience using  <strong> Snowflake  </strong> preferred  </li> <li> Hands on experience in  <strong> Modern data Platform  </strong> preferred  </li> </ul> <p> <u> Nice to Haves (not requirements):  </u> </p> <ul> <li> Knowledge of a Python web framework (preferably: Flask, Tornado, or twisted)  </li> <li> Basic understanding of front-end technologies, such as JavaScript, HTML/CSS  </li> <li> Knowledge of Elastic Search Stack (ELK)  </li> <li> Knowledge of and experience using data models and data dictionaries in a regulatory context. E.g., Banking and Financial Markets  <strong> .  </strong> </li> </ul> </div>",Data Engineer,1200000,1600000,https://www.naukri.com/job-listings-data-engineer-capco-technologies-pvt-ltd-bangalore-bengaluru-9-to-12-years-071222500224,"['Data migration', 'Front end', 'Financial markets', 'Diversity and Inclusion', 'SCALA', 'Javascript', 'HTML', 'big data', 'Financial services', 'Python']",['Bengaluru'],Data Engineer,2022-12-07 17:37:21
071222907652,"As Data engineer, you will develop and move data from the operational and external environments to the business intelligence environment using Ab Initio software. Skills include designing and developing extract, transform and load (ETL) processes.  <p> </p> <p> Responsibilities:  </p> <ul> <li> Coordinate with multiple technical teams to ensure apt integration of functions to identify and define necessary system enhancements to deploy new products and process improvements  </li> <li> Provide expertise in area and advanced knowledge of applications programming and ensure application design adheres to the overall architecture blueprint  </li> <li> Utilize advanced knowledge of system flow and develop standards for coding, testing, debugging, and implementation  </li> <li> Develop comprehensive knowledge of how areas of business, such as architecture and infrastructure, integrate to accomplish business goals  </li> <li> Resolve variety of high impact problems/projects through in-depth evaluation of complex business processes, system processes, and industry standards  </li> <li> Provide in-depth analysis with interpretive thinking to define issues and develop innovative solutions  </li> </ul> <p> <span> Required Technical and Professional Expertise </span> <span> </span> </p> <ul> <li> Minimum 6 years of experience in Abinitio development  </li> <li> Expertise in advanced Abinitio components  </li> <li> Ability to demonstrate micro / macro designing and familiar with Unix Commands and basic work experience in Unix Shell Scripting  </li> <li> Demonstrated ability in solutioning covering data ingestion, data cleansing, ETL, data mart creation and exposing data for consumers  </li> </ul> <p> <span> Preferred Technical and Professional Expertise </span> <span> </span> </p> <ul> <li> You love collaborative environments that use agile methodologies to encourage creative design thinking and find innovative ways to develop with cutting edge technologies  </li> <li> Ambitious individual who can work under their own direction towards agreed targets/goals and with creative approach to work  </li> <li> Intuitive individual with an ability to manage change and proven time management  </li> <li> Proven interpersonal skills while contributing to team effort by accomplishing related results as needed  </li> <li> Up-to-date technical knowledge by attending educational workshops, reviewing publications </li> </ul>",Data Engineer: Data Integration,600000,1000000,https://www.naukri.com/job-listings-data-engineer-data-integration-ibm-india-pvt-limited-pune-6-to-10-years-071222907652,"['data cleansing', 'Interpersonal skills', 'Time management', 'Creative designing', 'Operations']",['Pune'],Data Engineer,2022-12-07 13:15:08
071222905370,"Designs and builds solutions to move data from operational and external environments to the business intelligence environment using Ab Initio software and DataStage (formerly Ascential) - IBM s WebSphere Data Integration Suite. Skills include designing and developing extract, transform and load (ETL) processes. Experience includes full lifecycle implementation of the technical components of a business intelligence solution. Skills include: ETL, Informatica <p> </p> <p> <span> Required Technical and Professional Expertise </span> <span> </span> </p> <ul> <li> Informatica BDM, SQL, UNIX, Informatica BDM, SQL, UNIX. </li> <li> Strong understanding of Informatica Power center ETL tool with 3 to 6 years of relevant experience. </li> </ul> <p> <span> Preferred Technical and Professional Expertise </span> <span> </span> </p> <ul> <li> You love collaborative environments that use agile methodologies to encourage creative design thinking and find innovative ways to develop with cutting edge technologies </li> <li> Ambitious individual who can work under their own direction towards agreed targets/goals and with creative approach to work </li> <li> Intuitive individual with an ability to manage change and proven time management </li> <li> Proven interpersonal skills while contributing to team effort by accomplishing related results as needed </li> <li> Up-to-date technical knowledge by attending educational workshops, reviewing publications </li> </ul>",Data Engineer: Data Integration,700000,1100000,https://www.naukri.com/job-listings-data-engineer-data-integration-ibm-india-pvt-limited-hyderabad-secunderabad-3-to-6-years-071222905370,"['Interpersonal skills', 'Time management', 'Creative designing', 'Agile', 'Operations']",['Hyderabad'],Data Engineer,2022-12-07 11:18:09
091222501392,"<div> <p> </p> <ul> <li> We don t expect to be perfect, but we are always proactively seeking out ways to help ourselves and our teams to minimize pain points within our infrastructure and code base.  </li> </ul> <ul> <li> We have a team that focuses a lot on developer experience with the best tools, coding practices, design systems, etc.  </li> </ul> <ul> <li> We give autonomy and freedom to our engineer to turn their ideas into reality.  </li> </ul> <ul> <li> We are not afraid of failing when we are experimenting with different technologies, development methodologies, and tooling.  </li> </ul> <ul> <li> We build strong relationships with team members.  </li> </ul> <p> <br /> <strong> What you will be doing:  </strong> </p> <ul> <li> Building scalable, highly performant infrastructure to integrate a variety of raw data sources in Wizikey using APIs or scrapping or a combination of both.  </li> </ul> <ul> <li> Creating and maintaining optimal data pipeline architecture.  </li> </ul> <ul> <li> Developing batch & real-time solutions to power real-time analytics and intelligent insights in the product  </li> </ul> <ul> <li> Building prototypes, and proofs of concept for solutions.  </li> </ul> <ul> <li> Creating frameworks and tools for analytics and data scientist team members that assist them in building and optimizing our product.  </li> </ul> <ul> <li> Implementing complex analytical projects with a focus on collecting, managing, analyzing, and visualizing data.  </li> </ul> <ul> <li> Be in constant communication with team members and other relevant parties and convey results efficiently and clearly.  </li> </ul> <br /> <b> Qualifications  </b> <br /> <p> <strong> Skills & Qualifications:  </strong> </p> <ul> <li> You are an enthusiastic Software Engineer with a keen interest in data engineering and love to play with data  </li> </ul> <ul> <li> You have 3+ years of data engineering experience that includes handling a lot of different types of data using relevant tools.  </li> </ul> <ul> <li> You have performed software development in a production environment using a mainstream object-oriented programming language (Java, Python, Scala, etc.).  </li> </ul> <ul> <li> You have experience with ETL/ELT pipelines using both batch and real-time solutions.  </li> </ul> <ul> <li> You emphasize a lot of good practices like code quality, testing, following best practices and modularization on large codebases.  </li> </ul> <ul> <li> You have experience working with AWS and its tools.  </li> </ul> <p> <strong> Would be a plus if you have the following:  </strong> </p> <ul> <li> Experience with big data tools like Hadoop, Spark, Kafka, or similar  </li> </ul> <ul> <li> Experience with relational SQL and NoSQL databases  </li> </ul> <ul> <li> Experience with data pipeline and workflow management tools such as Airflow  </li> </ul> <ul> <li> Experience with stream-processing systems such as Spark-Streaming or similar.  </li> </ul> </div><br><br><p> </p>",Senior Data Engineer,400000,800000,https://www.naukri.com/job-listings-senior-data-engineer-wizikey-gurgaon-gurugram-3-to-4-years-091222501392,"['NoSQL', 'Coding', 'spark', 'Analytical', 'SCALA', 'Infrastructure', 'Object oriented programming', 'Analytics', 'SQL', 'Python']",['Gurgaon'],Data Engineer,2022-12-09 19:45:05
091222007348,"<p>  </p><p><strong>Responsibilities </strong></p><ul><li>Collaborate with business/customer & understand requirements</li><li>Create & maintain technical specifications, data mapping, data dictionary and operations run book</li><li>Design, implement, test, deploy, and maintain stable & secure data engineering solutions</li><li>Develop and perform unit, system, performance, and regression testing </li><li>Build and maintain data platform and analytics environments in cloud & on-premises</li><li>Support operational activities, automate repetitive tasks & troubleshoot issues</li><li>Define and promote best practices and design principles for data solutions and architecture</li><li>Work in onsite-offshore delivery model </li></ul><br /><p><strong>Qualifications</strong> </p><ul><li>Bachelor's degree in Computer Science/Software Engineering or equivalent combination of education and experience </li><li>8 + years of Total IT experience</li><li>6+ years of design and implementation experience in data lake, data warehouse, analytics, data migration, data integration & ML using AWS/Azure or On-premise data engineering technologies </li><li>Experience in at least one leading data integration platform (SSIS/Talend/Informatica/MuleSoft)</li><li>Experience in multiple database/data warehouse platforms (Oracle, Redshift, SQL DW etc.)</li><li>Knowledge of relational data modeling & data governance</li><li>Working knowledge of Software development practices such as Agile methodologies and DevOps</li></ul><br /><p><strong>Preferred qualifications </strong></p><ul><li>Experience/Understanding of US Pharma for R&D/Commercial business functions</li><li>Hands-on experience in Tableau/Power BI </li><li>Microsoft/AWS certified for engineering data solutions using Cloud services</li></ul>",Sr Data Engineer,2000000,3500000,https://www.naukri.com/job-listings-sr-data-engineer-celito-tech-india-hyderabad-secunderabad-8-to-13-years-091222007348,"['oracle', 'Mulesoft Anypoint', 'power bi', 'Tableau', 'SSIS', 'AZURE', 'redshift', 'Agile', 'Talend', 'AWS', 'Informatica Powercenter', 'SQL DW']",['Hyderabad'],Data Engineer,2022-12-09 15:33:11
101222500716,"<div> <p> <span> <span> <span> <span> <span> <span> </span> </span> </span> </span> </span> </span>   </p> <ul> <li> <span> Lead architectures design, data source acquisition and analysis, data extraction, transformation etc. </span> </li> <li> <span> Convert the business questions into data engineering projects, prepare data for modeling, conduct feature engineering and pattern analysis etc.  </span> </li> <li> <span> Working in Azure DevOps/Agile Scrum based execution methodologies. </span> </li> <li> <span> Develop monitoring technique for data quality and model performance; Manage the life cycle of the project deliverables through updated release.  </span> </li> <li> <span> Design and develop data ingestion pipelines in cloud frameworks. </span> </li> <li> <span> Knowledge of Conceptual, Logical and Physical data models required for Data Warehousing. </span> </li> <li> <span> Creating and maintaining Data Projects and deployments of the same.  </span> </li> <li> <span> Support our web-based software solutions/applications as required to serve various business productivity improvements. </span> </li> <li> <span> Deliver data visualization and interactive dashboards to internal/external customers. </span> </li> </ul> <p> </p> <p> </p> <p> <b> Education Qualification:  </b> </p> <ul> <li> <span> Bachelor s/Master s/PhD Degree in Computer Science, Information Technology, Mathematics, Engineering, Computer Applications or related fields.  </span> </li> <li> </li> </ul> <p> <b> Technical Competence Skills: </b> </p> <ul> <li> <span> Working experiences in data infrastructures, warehouse, transform (ELT), schema models and dashboard. </span> </li> <li> <span> Working experience in any cloud platforms - Microsoft Azure / AWS/ Google Cloud. </span> </li> <li> <span> Strong programming/coding skills using Python, Java, MySQL, Spark etc. </span> </li> <li> Exposure to machine learning or machine learning pipelines is a plus </li> </ul> <p> <b> <span> Leadership Competence: </span> </b> </p> <ul> <li> <span> Strong communication skills- ability to lead the technical and strategic discussions, both verbal and written, with global counterparts and leadership teams. </span> </li> <li> <span> Self-motivated- proactive in identifying the risks and issues in projects and taking lead in solving the problems quickly. </span> </li> <li> <span> Collaboration skills- drives collaborations with global teams while working on projects and develops trusted long-term professional relationships. </span> </li> <li> <span> Learning Agility- willing to continuously learn new products and technology. </span> </li> </ul> <p> <b> Relevant Experience: </b> </p> <ul> <li> <span> 5-8 years of relevant work experience in designing and maintaining enterprise data pipelines using CI/CD </span> </li> </ul> <p> <b> Others (% of travel, language, etc.):  </b> </p> <ul> <li> 30% </li> <li> English Hindi </li> </ul> <p> </p> <p> </p> </div>",Senior Data Engineer,900000,1400000,https://www.naukri.com/job-listings-senior-data-engineer-donaldson-gurgaon-gurugram-5-to-8-years-101222500716,"['Coding', 'MySQL', 'Schema', 'Machine learning', 'Data quality', 'Information technology', 'Monitoring', 'Data extraction', 'Python']",['Gurgaon'],Data Engineer,2022-12-10 23:47:29
101222500226,"<div> <ul> <li> Bachelor or Masters Degree in Computer Science, or other relevant quantitative field  </li> <li> Proven experience developing complex software systems as a software engineer (at least 5 years)  </li> <li> Experience building data processing systems using Python, Pandas,PySpark, and SQL  </li> <li> Experience of building distributed systems would be a plus  </li> <li> Good work ethic and self-starter who can deliver results quickly.  </li> <li> Drive to simplify, automate and offer innovative approaches.  </li> <li> Enjoy solving challenging problems, both technical and functional .  </li> <li> Good communication skills and the ability to translate technical concepts.  </li> <li> Highly proficient in English (written and verbal), German knowledge is a plus.  </li> <li> Comfortable working within multidimensional Agile Team with direct involvement from business SMEs.  </li> <li> Care about security, quality, user experience, documentation, testing, team spirit and the overall big picture  <br /> </li> </ul> </div>",Senior Data Engineer,400000,800000,https://www.naukri.com/job-listings-senior-data-engineer-info-origin-inc-hyderabad-secunderabad-5-to-7-years-101222500226,"['Agile', 'Data processing', 'German', 'Distribution system', 'SQL', 'Python', 'Testing']",['Hyderabad'],Data Engineer,2022-12-10 18:26:31
101222500570,"<div> <p> <span> Collaboration <span> </span> <span> - <span> </span> </span> </span> <span> <span> Collaborate </span> </span> <span> <span> s </span> </span> <span> <span> <span> </span> with internal/external stakeholders to manage data <span> </span> </span> </span> <span> <span> logistics </span> </span> <span> <span> <span> </span> - including data specifications, transfers, structures, and rules </span> </span> <span> <span> . </span> </span> <span> <span> <span> </span> Collaborate </span> </span> <span> <span> s </span> </span> <span> <span> <span> </span> with business users, business analysts and technical architects in transforming business requirements into analytical workbenches, tools and dashboards reflecting usability best practices and current design trends. </span> </span> <span> <span> <span> </span> </span> <span> Demonstrates <span> </span> </span> </span> <span> <span> analytical, <span> </span> </span> </span> <span> <span> interpersonal </span> </span> <span> <span> <span> </span> and </span> </span> <span> <span> <span> </span> professional </span> </span> <span> <span> <span> </span> communication skills </span> </span> <span> <span> . Learns quickly and works effectively ind </span> </span> <span> <span> i </span> </span> <span> <span> vidually </span> </span> <span> <span> <span> </span> </span> </span> <span> <span> and as part of a team </span> </span> <span> <span> .  </span> </span> <span> <span> </span> </span> <span> </span> </p> <div> <p> <span> </span> </p> </div> <div> <p> <span> Process Improvement <span> </span> <span> - <span> </span> </span> </span> <span> <span> Access, </span> </span> <span> <span> <span> </span> </span> </span> <span> <span> extract, </span> </span> <span> <span> <span> </span> and <span> </span> </span> </span> <span> <span> transform Credit and Retail data from a variety of sources of all sizes (including client marketing databases, <span> </span> </span> </span> <span> <span> 2 </span> </span> <span> <span> nd </span> </span> <span> <span> and 3 </span> </span> <span> <span> rd </span> </span> <span> <span> party data) using Hadoop, Spark, SQL, Big data technologies etc. </span> </span> <span> <span> <span> </span> </span> </span> <span> <span> Provide </span> </span> <span> <span> s </span> </span> <span> <span> <span> </span> automation help to analytical teams around data centric needs using orchestration tools, <span> </span> </span> </span> <span> <span> SQL </span> </span> <span> <span> <span> </span> and <span> </span> </span> </span> <span> <span> possibly other </span> </span> <span> <span> <span> </span> big data/cloud solutions for efficiency improvement. </span> </span> <span> </span> </p> </div> <div> <p> <span> </span> </p> </div> <div> <p> <span> Project Support </span> <span> <span> - <span> </span> </span> </span> <span> <span> Support Sr. Specialist and Specialist in new analytical proof of concepts and tool exploration projects. </span> </span> <span> <span> <span> </span> </span> </span> <span> <span> Effectively manage time and resources <span> </span> </span> </span> <span> <span> in order to </span> </span> <span> <span> <span> </span> deliver on time/correctly on concurrent projects. </span> </span> <span> <span> <span> </span> </span> </span> <span> <span> Involved in creating POCs to ingest and process streaming data using Spark and HDFS. </span> </span> <span> </span> </p> </div> <div> <p> <span> </span> </p> </div> <div> <p> <span> Data and Analytics </span> <span> <span> <span> </span> - <span> </span> </span> </span> <span> <span> Answer and trouble shoot questions about data sets and analytical tools; Develop, <span> </span> </span> </span> <span> <span> maintain </span> </span> <span> <span> <span> </span> and enhance new and existing analytics tools to support internal customers. </span> </span> <span> <span> <span> </span> </span> </span> <span> <span> I </span> </span> <span> <span> ngest data from files, streams and databases </span> </span> <span> <span> <span> </span> </span> </span> <span> <span> then </span> </span> <span> <span> <span> </span> </span> </span> <span> <span> p </span> </span> <span> <span> rocess the data with Python and <span> </span> </span> </span> <span> <span> Pyspark </span> </span> <span> <span> <span> </span> </span> </span> <span> <span> in order to </span> </span> <span> <span> <span> </span> </span> </span> <span> <span> store data to Hive or NoSQL database. </span> </span> <span> <span> <span> </span> </span> </span> <span> <span> Manage data coming from <span> </span> </span> </span> <span> <span> different sources </span> </span> <span> <span> <span> </span> and involved in HDFS maintenance and loading of structured and unstructured data. </span> </span> <span> <span> <span> </span> </span> </span> <span> <span> Apply knowledge in Agile </span> </span> <span> <span> <span> </span> Scrum <span> </span> </span> </span> <span> <span> methodology </span> </span> <span> <span> <span> </span> that <span> </span> </span> </span> <span> <span> leverages </span> </span> <span> <span> <span> </span> the Client Bigdata platform and used version control tool Git. </span> </span> <span> <span> <span> </span> </span> </span> <span> <span> </span> </span> <span> <span> I </span> </span> <span> <span> mport </span> </span> <span> <span> </span> </span> <span> <span> and </span> </span> <span> <span> <span> </span> export </span> </span> <span> <span> <span> </span> data using Sqoop from HDFS to RDBMS and vice-versa. </span> </span> <span> <span> <span> </span> </span> </span> <span> <span> Demonstrate an <span> </span> </span> </span> <span> <span> understanding of Hadoop Architecture and underlying Hadoop framework including Storage Management. </span> </span> <span> <span> <span> </span> </span> </span> <span> <span> Create POCs to ingest and process streaming data using Spark and HDFS. Work on back-end using Scala, Python and Spark to perform several <span> </span> </span> </span> <span> <span> aggregation </span> </span> <span> <span> <span> </span> logics </span> </span> <span> </span> </p> </div> <div> <p> <span> <span> </span> </span> <span> </span> </p> </div> <div> <p> <span> Technical Skills </span> <span> <span> <span> </span> </span> </span> <span> <span> - <span> </span> </span> </span> <span> <span> Expert in writing complicated SQL Queries and database analysis for <span> </span> </span> </span> <span> <span> good performance </span> </span> <span> <span> . </span> </span> <span> <span> <span> </span> </span> </span> <span> <span> Experience in working on Microsoft Azure Services like ADLS/Blob Storage solutions, Azure <span> </span> </span> </span> <span> <span> DataFactory </span> </span> <span> <span> , Azure Functions and Databricks. </span> </span> <span> <span> <span> </span> Utilize basic knowledge of Rest <span> </span> </span> </span> <span> <span> API </span> <span> <span> </span> for designing networked applications </span> </span> <span> </span> </p> <p> </p> <p> </p> <div>   </div> </div> </div>","Data Engineer, 1",400000,700000,https://www.naukri.com/job-listings-data-engineer-1-alliancedata-bangalore-bengaluru-2-to-5-years-101222500570,"['Linux', 'project support', 'Analytical', 'Finance', 'Wellness', 'hdfs', 'big data', 'SQL', 'Python', 'Logistics']",['Bengaluru'],Data Engineer,2022-12-10 23:47:17
091222502611,"<div> <ul> <li> <p> Design, develop, optimize, and maintain data architecture and pipelines that adhere to ETL principles and business goals </p> </li> <li> <p> Solve complex data problems to deliver insights that helps the organizations business to achieve their goals </p> </li> <li> <p> Create data products for analytics and data scientist team members to improve their productivity </p> </li> <li> <p> Design the solutions for multiple large data warehouses in Azure cloud environment utilizing the parallel architecture and appropriate technology stack </p> </li> <li> <p> Perform various proof of concepts to test the ideas and technology capabilities and subsequently drives these concepts into real solutions </p> </li> <li> <p> Work with various business units to identify the requirements and build corresponding data models and data pipelines </p> </li> <li> <p> Advise, consult, mentor and coach other data and analytic professionals on data standards and practices </p> </li> <li> <p> Foster a culture of sharing, re-use, design for scale stability, and operational efficiency of data and analytical solutions </p> </li> <li> <p> Lead the evaluation, implementation and deployment of emerging tools and process for analytic data engineering to improve the organizations productivity as a team </p> </li> <li> <p> Partner with business analysts and solutions architects to develop technical architectures for strategic enterprise projects and initiatives. </p> </li> </ul> <p> <strong> QUALIFICATIONS & EXPERIENCE </strong> </p> <ul> <li> <p> 5 years of experience working in data engineering or architecture or consultant role, 7 preferred (3 years with 5 preferred for junior role) </p> </li> <li> <p> Expertise in SQL and data analysis and experience with at least one programming language (Python preferred) </p> </li> <li> <p> Experience developing and maintaining data warehouses in big data solutions and SAP HANA  </p> </li> <li> <p> Should be able to demonstrate work experience in Hive and Spark  </p> </li> <li> <p> Experience working on In-memory database (SAP HANA preferred) </p> </li> <li> <p> Must have work experience in Hadoop as data warehouse/data lake implementations </p> </li> <li> <p> Experience with developing solutions on cloud computing services and infrastructure (MS Azure preferred) in the data and analytics space (preferred) </p> </li> <li> <p> Database development experience using Hadoop ecosystem and experience with a variety of relational, NoSQL, and cloud database technologies </p> </li> <li> <p> Conceptual knowledge of data and analytics, such as dimensional modeling, ETL, reporting tools, data governance, data warehousing, structured and unstructured data. </p> </li> <li> <p> Big Data Development experience using Hive, Spark and familiarity with Kafka (Preferred) </p> </li> <li> <p> Familiarity with the Linux operating system (Preferred) </p> </li> <li> <p> Passionate about Agile software processes, data-driven development, reliability, and experimentation </p> </li> <li> <p> Experience working on a collaborative Agile product team </p> </li> <li> <p> Self-motivated with strong problem-solving and learning skills </p> </li> <li> <p> Flexibility to changes in work direction as the project develops </p> </li> <li> <p> Excellent communication, listening, and influencing skills </p> </li> <li> <p> Demonstrated strong number sense, intellectually curious and willing to adjust position based on additional information </p> </li> <li> <p> Strong work ethic; ability to work at an abstract level and gain consensus </p> </li> <li> <p> Familiarity with SAP, MS Azure, Denodo and power BI is a plus </p> </li> </ul> <p> <strong> Education </strong> </p> <ul> <li> Bachelors degree required; Computer Science, MIS, or Engineering preferred </li> </ul> </div>",Data Engineer,300000,600000,https://www.naukri.com/job-listings-data-engineer-bd-bangalore-bengaluru-5-to-10-years-091222502611,"['Cloud computing', 'Data analysis', 'SAP', 'Linux', 'MIS', 'Agile', 'SQL', 'Python', 'Data architecture']",['Bengaluru'],Data Engineer,2022-12-09 21:01:01
091222501486,"<div> <div> </div> <ul> <li> A bachelors degree in Computer Science with 10 years of experience in leading data engineering projects  </li> </ul> <ul> <li> Strong understanding of data warehousing concepts and prior experience of implementing large scale data architecture projects involving batch and real time data  </li> </ul> <ul> <li> Expertise in working on cloud data warehouses like Snowflake and delivering business insights in BI tools  </li> </ul> <ul> <li> Exposure to any of the prominent cloud service providers (AWS/Azure/GCP)  </li> </ul> <ul> <li> Good experience with python libraries (pandas, numpy etc.) and frameworks like Spark.  </li> </ul> <ul> <li> Experience in working on tools like Airflow and prior experience with Agile, DevOps/CICD and Data Testing  </li> </ul> <ul> <li> Experience in working on Data Governance areas like Data Quality, Data Lineage, Metadata Management etc.  </li> </ul> <ul> <li> Strong technology acumen, knowledge of software engineering process, design knowledge and architecture intelligence  </li> </ul> <ul> <li> Mentor a team of Data Engineers and take responsibility for Data Engineering deliverables  </li> </ul> <ul> <li> Ability to collaborate across teams and advocate/influence multiple stakeholders.  </li> </ul> </div>",Principal Engineer - Data Engineering,1500000,1900000,https://www.naukri.com/job-listings-principal-engineer-data-engineering-quince-hyderabad-secunderabad-bangalore-bengaluru-10-to-12-years-091222501486,"['Computer science', 'Process design', 'metadata', 'GCP', 'Agile', 'data governance', 'Manager Technology', 'Data quality', 'Python', 'Data architecture']","['Bengaluru', 'Hyderabad']",Data Engineer,2022-12-09 19:45:13
091222906935,"• Design, build & maintain data pipelines (ETL development) for various marketing use cases<br />• Ensure effective controls are in place for a secure & resilient data transfer process (both ingest/egest)<br />• Rapidly troubleshoot and fix data queries/issues<br />• Build quality & performance metrics to systematically track & monitor progress of data pipelines <br />Depth Type: Deep<br />• Excellent Python or Java programming skills & familiarity with various data storage technologies and architectures<br />• Good knowledge of GCP data stack (BigQuery, Cloud Storage, DataProc etc)<br />• Deep understanding of distributed data processing using Spark and stream processing using Apache Kafka<br />• Experience in writing highly optimized SQLs on large data sets<br />• Good understanding of Job scheduling tools like UC4, Airflow<br />• Comfortable working in jupyter notebook environment<br />• Agile mindset & is well versed with DevOps tools & processes<br />Breadth Type: Narrow<br />• Experience building cloud-based data pipelines<br />• Experience in real-time and batch data transfer techniques<br />• Good knowledge of architecture patterns for Data, Security and Applications.<br />• Digital Marketing domain.",Data Engineer,2250000,2500000,https://www.naukri.com/job-listings-data-engineer-cloudare-technologies-bangalore-bengaluru-5-to-10-years-091222906935,"['Cloud Storage', 'Java', 'DevOps', 'BigQuery', 'DataProc', 'Python', 'SQL']",['Bengaluru'],Data Engineer,2022-12-09 14:50:45
091222009897,"<div><b>Visionet Systems Pvt. Ltd.,</b></div><b>Experience: 5 to 9 years</b><div><b>Notice Period: Immediate to 30 Days</b></div><div><b>Job Type: Permanent Full time</b></div><div><b>Location: Bangalore</b></div><div><b>Job Description:</b></div><div><ul><li>Hands-On Experience in Apache Spark, preferably Pyspark (2+ Years)</li><li>Experience in software development and coding in Python (2+ Years)</li><li>Hands-On Experience in SQL (2+ Years)</li><li>Experience in Airflow or AWS Managed Airflow (2+ Years)</li><li>Experience in AWS Services like – S3, Athena, Redshift, Dynamo DB, Amazon Cloudwatch,EMR (EMR Studio), EC2, RDS (1+ Years)</li><li>Experience delivering features with CI/CD process – preferably through Jenkins</li><li>Experience in Git</li><li>Good Analytical and Problem Solving Skills</li></ul></div>",Hiring For Data Engineer Pyspark,50000,300000,https://www.naukri.com/job-listings-hiring-for-data-engineer-pyspark-visionet-systems-bangalore-bengaluru-5-to-9-years-091222009897,[],['Bengaluru'],Data Engineer,2022-12-09 18:27:37
091222007233,"<br /><ul><li>Hands-on-experience in developing Data Engineering solutions using Azure Services.</li><li>Highly experienced in Azure data factory, Azure Databricks, Azure Synapse Analytics.</li><li>Excellent optimization of SQL queries.</li><li>Proven experience in programming with Python/Pyspark.</li><li>Create reusable and scalable data pipelines.</li><li>Collaborating within a project team to solve complex problems</li><li>Excellent communication and interpersonal skills.</li><li>DP-203 and DP-900 certifications will be an added advantage.</li></ul>",Azure Data Engineer,800000,1500000,https://www.naukri.com/job-listings-azure-data-engineer-latentview-chennai-1-to-4-years-091222007233,['Data Engineering'],['Chennai'],Data Engineer,2022-12-09 15:23:53
291122914970,"<div><div><div><br /><b>Job Purpose and Impact</b><br /></div><div><p><span><span>The Data Engineer will treat data as an asset to design, build and execute high performance and data centric solutions by using the comprehensive big data capabilities for the company's data platform environment. In this role, you will build and optimize data products to bring data and analytics products and solutions to businesses. You will contribute to design of data structures and pipelines to collect data and design and implement data transformations, combinations, or aggregations. You will help build data streams and APIs to make data more consumable.</span></span><br /></p></div></div><div><div><br /><br /><b>Key Accountabilities</b><br /></div><div><p><span><span>Collaborate with businesses and other stakeholders to participate in product or solution designs.<br /><span></span> Develop robust, scalable, and sustainable data products or solutions utilizing cloud-based technologies. </span></span></p><p><span><span><span></span></span></span><span><span><span> Provide moderately complex technical support through all phases of product or solution life cycle. </span></span></span><br /><span><span><span></span></span></span><span><span><span> Perform data analysis, handle data modeling, and configure and develop data pipelines to move and optimize data assets.</span></span></span><br /><span><span><span></span></span></span><span><span><span> Build moderately complex prototypes to test new concepts and provide ideas on reusable frameworks, components and data products or solutions and help promote adoption of new technologies.</span></span></span><br /><span><span><span></span></span></span><span><span><span> Independently solve moderately complex issues with minimal supervision, while escalating more complex issues to appropriate staff.</span></span></span><br /><span><span><span></span></span></span><span><span><span> Other duties as assigned </span></span></span></p></div></div><div><div><br /><br /><b>Qualifications</b><br /></div><div><p><span><span><span> </span>Bachelor’s degree in a related field or equivalent experience</span></span></p><p><span><span><span></span> Minimum of two years of related work experience</span></span></p><p><span><span><span></span> Experience with data modeling, data warehousing & ETL processes and analytical tools</span></span></p><p><span><span><span></span> Experience with Hadoop-based analytics, preferably Cloudera</span></span></p><p><span><span><span></span> Experience building Kafka or Apache Pulsar streaming solutions</span></span></p><p><span><span><span></span> Experience with API frameworks, gateways, and management</span></span></p><p><span><span><span></span> Other minimum qualifications may apply</span></span><br /></p><p><span><span><span><b>PREFERRED QUALIFICATIONS </b></span></span></span></p><p><span><span><span></span> Proficiency in programming languages such as Java, Python, Scala, C#, Golang</span></span></p><p><span><span><span></span> Experience with SQL/database and NoSQL backends</span></span></p><p><span><span><span></span> Experience with Unix shell scripting, reporting and data analytics</span></span></p><p><span><span><span></span> Experience with API development and integration</span></span></p><p><span><span><span></span> Experience with data pipelining and performance optimization</span></span></p><p><span><span><span></span> Experience working with various data sources, cloud-based, cross-application integrations </span></span></p><p><span><span><span></span> Experience with Apache Spark, Big data analytics, Data Architecture, and Machine learning</span></span></p><p><span><span><span></span> Experience developing software using agile methodologies such as Scrum and/or Kanban</span></span></p></div></div></div><div></div>",Senior Data Engineer,300000,600000,https://www.naukri.com/job-listings-senior-data-engineer-cargill-india-pvt-ltd-bangalore-bengaluru-2-to-4-years-291122914970,"['C#', 'Golang', 'Java', 'API development', 'Scala', 'Cloudera', 'Big data analytics', 'Unix shell scripting', 'Apache Spark', 'Kanban', 'NoSQL', 'Data Architecture', 'Python']",['Bengaluru'],Data Engineer,2022-11-29 19:39:23
091222007967,"•	8+ years of experience in Data Engineering on SQL-Server technologies, <br>•	worked majorly in Analytics and Reporting based applications with solid experience of <br>•	Database query writing and management (SQL) <br>•	Programming experience in TSQL",Immediate openings For Sr Data Engineers in Bangalore,1000000,2000000,https://www.naukri.com/job-listings-immediate-openings-for-sr-data-engineers-in-bangalore-bangalore-strategic-solutions-bangalore-bengaluru-8-to-12-years-091222007967,"['VBA', 'Dotnet', 'TSQL', 'SQL']",['Bengaluru'],Data Engineer,2022-12-09 16:27:01
051222007909,"<p>Profile- Data Engineer</p><p>Exp- 3- 9 Years</p><p>Salary- upto 4.5 LPA</p><br /><p>Skills-</p><br /><p>We are looking for below mentioned skills</p><p><br /></p><p>Experience on Data Warehouse platforms (including data mapping, ETL, data load, transformation)</p><p>- Data Warehousing and Dimensional Data Modeling skills</p><p>- Working experience as Big Data Engineer, Azure Data Engineer on Azure Cloud platform</p><p>- Azure Synapse</p><p>- Azure Data Factory</p><p>- Python/Scala and Spark</p><p>- SQL</p><br /><br /><br /><br /><br /><br />","Data Engineer- Data Warehouse platforms, Azure Synapse,",50000,300000,https://www.naukri.com/job-listings-data-engineer-data-warehouse-platforms-azure-synapse-altimetrik-hyderabad-secunderabad-pune-chennai-jaipur-bangalore-bengaluru-3-to-8-years-051222007909,"['Big Data', 'Spark', 'ETL', 'Data Bricks', 'Python']","['Chennai', 'Pune', 'Bengaluru', 'Hyderabad', 'Jaipur']",Data Engineer,2022-12-05 19:15:20
091122006339,"<p><strong>Roles and Responsibilities</strong> </p><ul><li>Exp range desired - 8-10 years exp</li><li>We are in need of experienced Senior Data Engineers with below mandatory skills in the order of priority</li><li>- Azure Databricks</li><li>- Scala programming or pyspark at the minimum</li><li>- Datapipelines with realtime streaming experience</li><li>- Azure Data factory pipelines development</li></ul><br /><br /><br /><p><strong>Desired Candidate Profile</strong> </p><p> </p><ul><li>Exp range desired - 8-10 years exp</li><li>We are in need of experienced Senior Data Engineers with below mandatory skills in the order of priority</li><li>-Azure Databricks</li><li>-Scala programming or pyspark at the minimum</li><li>-Datapipelines with realtime streaming experience </li><li>-Azure Data factory pipelines development""</li></ul><br /><br /><p><strong>Perks and Benefits</strong> </p><p>Lot of perks and benefits</p><br /><br />",Data Engineer,1500000,2500000,https://www.naukri.com/job-listings-data-engineer-infogain-india-p-ltd-noida-mumbai-pune-gurgaon-gurugram-bangalore-bengaluru-8-to-10-years-091122006339,['azure data factory'],"['Pune', 'Mumbai', 'Bengaluru', 'Gurgaon', 'Noida']",Data Engineer,2022-12-06 18:18:24
131122000099,"<p>Advanced working SQL knowledge to create complex queries.</p><ul><li>Hands on Experience in working on Microsoft Azure Services like ADLS Blob Storage solutions, Event Hubs, Service Bus, scale sets, Load Balancers, Azure Functions, Analytics services (Azure SQL, Azure Data Factory, Azure Synapse, Azure Analysis Services, Azure Data Lake, Databricks)</li><li>Experience with schema design and dimensional data modeling</li><li>Knowledge of data warehouse best practices, development standards and methodologies</li><li>Design ETL/ELT processes from the source systems to Target systems</li><li>Experience in Python/PowerShell, Scala, and SQL</li><li>Execution experience with Agile/Scrum project methodology</li><li>Knowledge on continuous integration continuous deployment. Experience of data migration and deployment from On-Prem to Cloud environment and vice-versa.</li><li>Gather requirements and deliver data solutions to address those requirements.</li><li>Build out new API integrations to support continuing increases in data volume and complexity.</li><li>Ensure all developments are fully documented and meet design requirements.</li><li>Develop and maintain scalable data pipelines.</li></ul><br /><p>Desired Skills:</p><br /><ul><li>Excellent communication skills in written and verbal English.</li><li>Ability to work in and with cross-functional distributed teams, including working across multiple time zones and providing remote support.</li><li>Be an independent self-learner with the  let s get this done  approach.</li><li>Certification on any cloud platform.</li></ul>",Data Engineer || Azure,500000,1500000,https://www.naukri.com/job-listings-data-engineer-azure-infogain-india-p-ltd-noida-mumbai-pune-bangalore-bengaluru-6-to-10-years-131122000099,"['sql', 'azure']","['Pune', 'Mumbai', 'Bengaluru', 'Noida']",Data Engineer,2022-12-06 18:18:25
081222011553,"<p>Data Engineer: 5A - 6 to 7 Years of experince<br />• Excellent GCP software/data engineering skills and proficiency Python programming language<br />• Familiarity with system design, data structures, algorithms, storage systems, and GCP cloud infrastructure<br />• Understanding of data modelling and data architecture concepts<br />• Working knowledge of advanced analytics (i.e. Machine Learning, Statistics) is a plus<br />• Hands on development experience on GCP<br />• Good in Creating Technical Specification and Data Flow document<br />• GCP Services  Dataflow, BigQuery, Cloud Storage, Dataproc, Pub/Sub<br />• Excellent verbal and written communication skills<br />• Proficient in English<br />Mandatory Skillset for Data Engineer: Dataflow, BigQuery, Cloud Storage, Dataproc, Pub/Sub</p>",Gcp Data Engineer,2500000,3000000,https://www.naukri.com/job-listings-gcp-data-engineer-birlasoft-noida-hyderabad-secunderabad-pune-chennai-bangalore-bengaluru-mumbai-all-areas-5-to-8-years-081222011553,[],"['Chennai', 'Pune', 'Bengaluru', 'Hyderabad', 'Noida', 'Mumbai (All Areas)']",Data Engineer,2022-12-08 18:34:17
311022002171,"<p><strong>Roles and Responsibilities</strong> </p><p> </p><ul><li>6+ years experience in Azure Stack(ADF,ADLS, Synapse, Databricks, PowerBI) and strong Data Warehouse, SQL DB , Data Modelling Skills. Hands on exposure to coding and mentor team on technical abilities.""</li></ul><br /><br /><p><strong>Desired Candidate Profile</strong> </p><br /><p> </p><ul><li>6+ years experience in Azure Stack(ADF,ADLS,Synapse,Databricks,PowerBI) and strong Data Warehouse, SQL DB , Data Modelling Skills. Hands on exposure to coding and mentor team on technical abilities.""</li></ul><br /><br /><p><strong>Perks and Benefits</strong> </p><br /><p>lot of perks and benefits</p>",Azure Data Engineer,1300000,2300000,https://www.naukri.com/job-listings-azure-data-engineer-infogain-india-p-ltd-noida-pune-gurgaon-gurugram-bangalore-bengaluru-mumbai-all-areas-6-to-8-years-311022002171,"['Power Bi', 'sql']","['Pune', 'Bengaluru', 'Gurgaon', 'Noida', 'Mumbai (All Areas)']",Data Engineer,2022-12-06 18:18:24
111122005843,<p>We are in need of experienced Data Engineers with below mandatory skills in the order of priority</p><br /><p>-	Azure Databricks</p><p>-	Scala programming or pyspark at the minimum</p><p>-	Datapipelines with realtime streaming experience </p><p>-	Azure Data factory pipelines development</p><p>-       Azure data factory</p>,Azure Data Engineer || Infogain Hiring,700000,1700000,https://www.naukri.com/job-listings-azure-data-engineer-infogain-hiring-infogain-india-p-ltd-noida-mumbai-pune-bangalore-bengaluru-8-to-12-years-111122005843,['SCALA'],"['Pune', 'Mumbai', 'Bengaluru', 'Noida']",Data Engineer,2022-12-06 18:18:24
071222013263,"<p>we have the following opening:</p><br /><p>Position: Data Engineer-AWS</p><p>Years of Exp: 6 years to 9 years</p><p>Job Location: Mumbai/Kolkata/Noida/Ahmedabad/WFH</p><p>Notice period: immediate to 15 days </p><br /><p>INTERESTED CANDIDATE ,PLEASE SHARE THE RESUME TO nibedita.dwibedi@inadev.com</p><br /><p>JD:</p><p> </p><p>Skills : Athena, Glue, Pyspark, DynamoDB, Lambda and EMR . Postgres</p><p>Good to Have: Redshift</p><br /><br />",Data Engineer-AWS,2250000,2750000,https://www.naukri.com/job-listings-data-engineer-aws-inadev-noida-kolkata-mumbai-ahmedabad-6-to-10-years-071222013263,"['DATA Engineer', 'Postgresql', 'Dynamodb', 'AWS', 'postgres']","['Mumbai', 'Ahmedabad', 'Noida', 'Kolkata']",Data Engineer,2022-12-07 19:02:30
301122913058,"<span> <span> <span> <span> Be involved in data engineering activities like Creating pipelines / workflows for Source to Target etc. </span> </span> </span> </span> <p> </p> <p> <span> <span> <span> <span> Responsibilities: </span> </span> </span> </span> </p> <ul> <li> <span> <span> <span> <span> You will be involved in the Azure Data platform implementation. Willing to work in shift timings 2PM to 11PM </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Responsible for assessing feasibility of migrating customer solutions and/or integrating with 3rd party systems both Microsoft and non-Microsoft platforms </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Designing and developing Data Pipelines for Data Ingestion or Transformation using Python (PySpark)/Spark SQL </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Work with version control GitHub and CI/CD pipelines using Azure DevOps </span> </span> </span> </span> </li> </ul> <p> <span> <span> <span> <span> If you thrive in a dynamic, collaborative workplace, IBM provides an environment where you will be challenged and inspired every single day. And if you relish the freedom to bring creative, thoughtful solutions to the table, there s no limit to what you can accomplish here. </span> </span> </span> </span> <br /> <span> </span> <br /> <span> Required Technical and Professional Expertise </span> <span> </span> </p> <ul> <li> <span> <span> <span> <span> Total Years of Experience 5 years in IT industry </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> 2 years current and deep experience with Azure Data platform implementation. Proven experience managing projects through the entire project lifecycle. </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Relevant years of experience 2 years in Azure Data Factory, Azure Data Lake, Azure DevOps, Azure DataBricks, AzureSQL </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Proficient in Azure Data Integration, Azure Data Architecture </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Proven experience using the Microsoft Azure Data Stack (ADFv2, Azure SQL DB, Azure SQL Datawarehouse, Azure Data Lake, Azure Databricks, Analysis Services, Cosmos DB) </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Experience in Azure data migration patterns </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Experience in Python, C# is mandatory </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Agile methodology experience essential </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> 3 years experience assessing feasibility of migrating customer solutions and/or integrating with 3rd party systems both Microsoft and non-Microsoft platforms </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Hands on experience Designing and developing Data Pipelines for Data Ingestion or Transformation using Python (PySpark)/Spark SQL </span> </span> </span> </span> </li> </ul> <p> <span> Preferred Technical and Professional Expertise </span> <span> </span> </p> <ul> <li> <span> <span> <span> <span> AZ 900 - Azure Fundamentals </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> DP 200, DP 201, DP 203, AZ 204 - Data Engineering </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> AZ 400 - Devops Certification </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> You love collaborative environments that use agile methodologies to encourage creative design thinking and find innovative ways to develop with cutting edge technologies </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Ambitious individual who can work under their own direction towards agreed targets/goals and with creative approach to work </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Intuitive individual with an ability to manage change and proven time management </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Proven interpersonal skills while contributing to team effort by accomplishing related results as needed </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Up-to-date technical knowledge by attending educational workshops, reviewing publications </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Experience working with version control GitHub and CI/CD pipelines using Azure DevOps </span> </span> </span> </span> </li> </ul>",Data Engineer: Big Data,800000,1200000,https://www.naukri.com/job-listings-data-engineer-big-data-ibm-india-pvt-limited-pune-2-to-5-years-301122913058,"['Data migration', 'github', 'Version control', 'devops', 'Cosmos']",['Pune'],Data Engineer,2022-11-30 18:12:07
301122912839,"<span> <span> <span> <span> <span> <span> As Big Data Engineer, you will develop, maintain, evaluate and test big data solutions. You will be involved in data engineering activities like Creating pipelines / workflows for Source to Target etc. </span> </span> </span> </span> </span> </span> <p> </p> <p> <span> <span> <span> <span> <span> <span> Responsibilities: </span> </span> </span> </span> </span> </span> </p> <ul> <li> <span> <span> <span> <span> You will be involved in the design of data solutions using Hadoop based technologies along with Hadoop, Azure, HDInsights for Cloudera based Data Late using Scala Programming. </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Responsible to Ingest data from files, streams and databases. Process the data with Hadoop, Scala, SQL Database, Spark, ML, IoT </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Develop programs in Scala and Python as part of data cleaning and processing </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Responsible to design and develop distributed, high volume, high velocity multi-threaded event processing systems </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Develop efficient software code for multiple use cases leveraging Python and Big Data technologies for various use cases built on the platform </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Provide high operational excellence guaranteeing high availability and platform stability </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Implement scalable solutions to meet the ever-increasing data volumes, using big data/cloud technologies Pyspark, Kafka, any Cloud computing etc. </span> </span> </span> </span> </li> </ul> <p> <span> <span> <span> <span> <span> <span> If you thrive in a dynamic, collaborative workplace, IBM provides an environment where you will be challenged and inspired every single day. And if you relish the freedom to bring creative, thoughtful solutions to the table, there s no limit to what you can accomplish here. </span> </span> </span> </span> </span> </span> <br /> <span> </span> <br /> <span> Required Technical and Professional Expertise </span> <span> </span> </p> <ul> <li> <span> <span> Overall industry experience of 12 years </span> </span> </li> <li> <span> <span> <span> <span> Proficient in any of the programming languages - Python, Scala or Java </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Mandatory experience in Mid to Expert Level programming capabilities in a large-scale enterprise </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> In-depth experience in modern data platform components such as the Hadoop, Hive, Pig, Spark, Python, Scala, etc </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Experience with Distributed Versioning Control environments such as GIT </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Familiarity with development tools - experience on either IntelliJ / Eclipse / VSCode IDE, Build Tool Maven </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Demonstrated experience in modern API platform design including how modern UI s are built consuming services / APIs. </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Experience on Azure cloud including Data Factory, Databricks, Data Lake Storage is highly preferred </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Solid experience in all phases of Software Development Lifecycle - plan, design, develop, test, release, maintain and support, decommission </span> </span> </span> </span> </li> </ul> <p> <span> Preferred Technical and Professional Expertise </span> <span> </span> <br /> <span> <span> <span> <span> <span> <span> Good to have at least one of the Certifications listed here: </span> </span> </span> </span> </span> </span> </p> <ul> <li> <span> <span> <span> <span> AZ 900 - Azure Fundamentals </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> DP 200, DP 201, DP 203, AZ 204 - Data Engineering </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> AZ 400 - Devops Certification </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> You love collaborative environments that use agile methodologies to encourage creative design thinking and find innovative ways to develop with cutting edge technologies </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Ambitious individual who can work under their own direction towards agreed targets/goals and with creative approach to work </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Intuitive individual with an ability to manage change and proven time management </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Proven interpersonal skills while contributing to team effort by accomplishing related results as needed </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Up-to-date technical knowledge by attending educational workshops, reviewing publications </span> </span> </span> </span> </li> </ul>",Data Engineer: Big Data,800000,1200000,https://www.naukri.com/job-listings-data-engineer-big-data-ibm-india-pvt-limited-hyderabad-secunderabad-12-to-15-years-301122912839,"['Interpersonal skills', 'GIT', 'Time management', 'Hadoop', 'Software development life cycle']",['Hyderabad'],Data Engineer,2022-11-30 18:07:09
301122912540,"<span></span><br /><span><span><span><span><span><span>As Big Data Engineer, you will develop, maintain, evaluate and test big data solutions. You will be involved in data engineering activities like Creating pipelines / workflows for Source to Target etc.</span></span></span></span></span></span><p></p><p><span><span><span><span><span><span>Responsibilities:</span></span></span></span></span></span></p><ul><li>You will be involved in the design of data solutions using Hadoop based technologies along with Hadoop, Azure, HDInsights for Cloudera based Data Late using Scala Programming.</li><li>Responsible to Ingest data from files, streams and databases. Process the data with Hadoop, Scala, SQL Database, Spark, ML, IoT</li><li>Develop programs in Scala and Python as part of data cleaning and processing</li><li>Responsible to design and develop distributed, high volume, high velocity multi-threaded event processing systems</li><li>Develop efficient software code for multiple use cases leveraging Python and Big Data technologies for various use cases built on the platform</li><li>Provide high operational excellence guaranteeing high availability and platform stability</li><li>Implement scalable solutions to meet the ever-increasing data volumes, using big data/cloud technologies Pyspark, Kafka, any Cloud computing etc.</li></ul><p><span><span><span><span><span><span>If you thrive in a dynamic, collaborative workplace, IBM provides an environment where you will be challenged and inspired every single day. And if you relish the freedom to bring creative, thoughtful solutions to the table, there s no limit to what you can accomplish here.</span></span></span></span></span></span><br /><span></span><br /><span>Required Technical and Professional Expertise</span><span></span></p><ul><li>Minimum 7 years of experience in Big Data technologies</li><li>Proficient in any of the programming languages - Python, Scala or Java</li><li>Mandatory experience in Mid to Expert Level programming capabilities in a large-scale enterprise</li><li>In-depth experience in modern data platform components such as the Hadoop, Hive, Pig, Spark, Python, Scala, etc</li><li>Experience with Distributed Versioning Control environments such as GIT</li><li>Familiarity with development tools - experience on either IntelliJ / Eclipse / VSCode IDE, Build Tool Maven</li><li>Demonstrated experience in modern API platform design including how modern UI s are built consuming services / APIs.</li><li>Experience on Azure cloud including Data Factory, Databricks, Data Lake Storage is highly preferred</li><li>Solid experience in all phases of Software Development Lifecycle - plan, design, develop, test, release, maintain and support, decommission</li></ul><p><span></span><br /><span>Preferred Technical and Professional Expertise</span><span></span><br /><span><span><span><span><span><span>Good to have at least one of the Certifications listed here:</span></span></span></span></span></span></p><ul><li>AZ 900 - Azure Fundamentals</li><li>DP 200, DP 201, DP 203, AZ 204 - Data Engineering</li><li>AZ 400 - Devops Certification</li><li>You love collaborative environments that use agile methodologies to encourage creative design thinking and find innovative ways to develop with cutting edge technologies</li><li>Ambitious individual who can work under their own direction towards agreed targets/goals and with creative approach to work</li><li>Intuitive individual with an ability to manage change and proven time management</li><li>Proven interpersonal skills while contributing to team effort by accomplishing related results as needed</li><li>Up-to-date technical knowledge by attending educational workshops, reviewing publications</li></ul><div></div><div></div><div><div><div><div><span><i></i><i></i></span><span></span></div></div></div></div>",Data Engineer: Big Data,800000,1200000,https://www.naukri.com/job-listings-data-engineer-big-data-ibm-india-pvt-limited-hyderabad-secunderabad-2-to-5-years-301122912540,"['Interpersonal skills', 'GIT', 'Time management', 'Hadoop', 'Software development life cycle']",['Hyderabad'],Data Engineer,2022-11-30 18:02:04
291122910713,"<span> Your Role and Responsibilities </span> <span> </span> <br /> <span> <span> <span> <span> Be involved in data engineering activities like Creating pipelines / workflows for Source to Target etc. </span> </span> </span> </span> <p> </p> <p> <span> <span> <span> <span> Responsibilities: </span> </span> </span> </span> </p> <ul> <li> <span> <span> <span> <span> You will be involved in the Azure Data platform implementation. Willing to work in shift timings 2PM to 11PM </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Responsible for assessing feasibility of migrating customer solutions and/or integrating with 3rd party systems both Microsoft and non-Microsoft platforms </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Designing and developing Data Pipelines for Data Ingestion or Transformation using Python (PySpark)/Spark SQL </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Work with version control GitHub and CI/CD pipelines using Azure DevOps </span> </span> </span> </span> </li> </ul> <p> <span> <span> <span> <span> If you thrive in a dynamic, collaborative workplace, IBM provides an environment where you will be challenged and inspired every single day. And if you relish the freedom to bring creative, thoughtful solutions to the table, there s no limit to what you can accomplish here. </span> </span> </span> </span> <br /> <span> </span> <br /> <span> Required Technical and Professional Expertise </span> <span> </span> </p> <ul> <li> <span> <span> <span> <span> Total Years of Experience 5 years in IT industry </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> 2 years current and deep experience with Azure Data platform implementation. Proven experience managing projects through the entire project lifecycle. </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Relevant years of experience 2 years in Azure Data Factory, Azure Data Lake, Azure DevOps, Azure DataBricks, AzureSQL </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Proficient in Azure Data Integration, Azure Data Architecture </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Proven experience using the Microsoft Azure Data Stack (ADFv2, Azure SQL DB, Azure SQL Datawarehouse, Azure Data Lake, Azure Databricks, Analysis Services, Cosmos DB) </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Experience in Azure data migration patterns </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Experience in Python, C# is mandatory </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Agile methodology experience essential </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> 3 years experience assessing feasibility of migrating customer solutions and/or integrating with 3rd party systems both Microsoft and non-Microsoft platforms </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Hands on experience Designing and developing Data Pipelines for Data Ingestion or Transformation using Python (PySpark)/Spark SQL </span> </span> </span> </span> </li> </ul> <p> <span> Preferred Technical and Professional Expertise </span> <span> </span> </p> <ul> <li> <span> <span> <span> <span> AZ 900 - Azure Fundamentals </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> DP 200, DP 201, DP 203, AZ 204 - Data Engineering </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> AZ 400 - Devops Certification </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> You love collaborative environments that use agile methodologies to encourage creative design thinking and find innovative ways to develop with cutting edge technologies </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Ambitious individual who can work under their own direction towards agreed targets/goals and with creative approach to work </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Intuitive individual with an ability to manage change and proven time management </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Proven interpersonal skills while contributing to team effort by accomplishing related results as needed </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Up-to-date technical knowledge by attending educational workshops, reviewing publications </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Experience working with version control GitHub and CI/CD pipelines using Azure DevOps </span> </span> </span> </span> </li> </ul>",Data Engineer: Big Data,800000,1200000,https://www.naukri.com/job-listings-data-engineer-big-data-ibm-india-pvt-limited-hyderabad-secunderabad-2-to-5-years-291122910713,"['Data migration', 'github', 'Version control', 'devops', 'Cosmos']",['Hyderabad'],Data Engineer,2022-11-29 17:12:44
051222500923,"<div> <ul> <li> Hands-on experience in Data warehousing, Data integration and Data analysis technologies </li> <li> Possess strong understanding of data modelling tools and standards, ETL tools, and big data technologies. </li> <li> Experience in ETL tools like Talend, DataStage, Informatica, Ab-Initio, SSIS </li> <li> Experience in Snowflake/ Databricks/ BigQuery/ AWS Glue </li> <li> Expert knowledge in SQL </li> <li> FS/ Banking domain expertise </li> <li> Self-starter with strong self-management skills, Team player, Strong oral and written communications skills. </li> <li> Able to creatively apply analytical solutions to business problems and adapting to changes per requirement/standards. </li> <li> Ability to work under constant pressure to tight deadlines and deliver high quality output. </li> </ul> <p> </p> </div>",Data Engineer,700000,1100000,https://www.naukri.com/job-listings-data-engineer-capco-technologies-pvt-ltd-pune-4-to-7-years-051222500923,"['Data analysis', 'Data modeling', 'Analytical', 'Datastage', 'Banking', 'Ab Initio', 'Informatica', 'SSIS', 'big data', 'SQL']",['Pune'],Data Engineer,2022-12-05 18:27:10
051222007035,"<p> </p><p><strong>Job Title: Azure Data Engineer</strong></p><p><strong>Location: Gurgaon</strong></p><p><strong>Experience: 4 to 8 Years</strong></p><br /><p><strong>JOB DESCRIPTION</strong></p><p><strong>ROLES & RESPONSIBILITIES</strong></p><ul><li>Evaluating, developing, maintaining, and testing data engineering solutions for advanced analytics projects.</li><li>Implement processes and logic to extract, transform, and distribute data across one or more data stores from a wide variety of sources</li><li>Distill business requirements and translate into technical solutions for data systems      including data warehouses, cubes, marts, lakes, ETL integrations, BI tools or other components.</li><li>Strong knowledge of replication processes, change data capture processes, T-SQL, and Dynamic SQL skills</li><li>Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the Azure platform.</li><li>Optimize data integration platform to provide optimal performance under increasing data      volumes</li><li>Support the data architecture and data governance function to continually expand their capabilities</li><li>Support the automation of requirements of database deployments using Azure DevOps</li><li>Experience in development of Solution Architecture for Enterprise Data Lakes (applicable      for AM/Manager level candidates)</li><li>Should have exposure to client facing roles</li><li>Strong communication, inter-personal and team management skills</li></ul><br /><p><strong>THE INDIVIDUAL data brick data warehouse</strong></p><ul><li>Proficient in any object-oriented/ functional scripting languages: Java, Python, C# etc.</li><li>Design and develop new solutions on the Azure Cloud Platform specifically for Azure SQL Data Warehouse/DB, Azure Data Factory, Analysis Services, Azure Data Lake Store/ Blob</li><li>Hands-on experience working in complex data warehouse implementations using Azure SQL Data warehouse, Azure Data Factory and Azure SQL Database.</li><li>Strong t-SQL skills with experience in Azure SQL DW</li><li>Experience in creating data pipelines using Azure Data Factory, Polybase and U-SQL. </li><li>Experience in developing data pipelines to transform, aggregate and or process data using Azure Databricks platform</li><li>Hands-on experience in implementing Azure Cloud data warehouses, Azure and No-SQL databases and hybrid data ingestion scenarios</li><li>Experience in creating tabular models (DAX) in Visual Studio. </li></ul><br /><br /><br />",Azure Data Engineer,1000000,2000000,https://www.naukri.com/job-listings-azure-data-engineer-kpmg-india-gurgaon-gurugram-4-to-8-years-051222007035,"['Azure Synapse', 'Pyspark', 'DWH', 'Python', 'SQL']",['Gurgaon'],Data Engineer,2022-12-05 17:55:04
071222908006,"<p><b>Job Description</b><br /></p><div><div><div><div><div><p><span><span><span>Collaborate with business users, development teams, and operation engineering teams to tackle business requirements and deliver against high operational standards of system availability and reliability.<br />Able to create Collaborative, intellectually curious environment in team.<br />Identify process improvement opportunities to drive innovation.<br />Ability to uncover and develop data-oriented stories that resonate with business partners/executive audiences.<br />Should have 6+ years of experience in maintaining data warehouse systems and working on large scale data transformation using EMR, Hadoop, Hive or other Big Data Technologies.</span></span></span><br /><br /></p></div></div><div><div><br /><br /><b>Primary Skills</b><br /></div><div><p><span><span><span>3+ experience with Cloud technologies, especially AWS services (S3, Athena, Glue, DynamoDB, Step Functions, etc).<br />Strong understanding of Redshift, EMR, Athena, Aurora, Kinesis, Lambda, EC2 etc.<br />Must have Technical strength in SQL, Cloud Technologies, ETL development and data warehousing.<br />Able to mentor and manage other data engineers.<br />Ensure best data engineering practices followed.<br />Ability to communicate with business team as well as Technical team.<br />Experience in building Data Quality framework.<br />Excellent analytical ability to understand business issues associated with data management and governance, business processes in terms of data, and related standards, data security and privacy<br />Experience in building data pipelines using SQL, Python & Pyspark in AWS.</span></span></span><br /><br /></p></div></div><div><div><br /><br /><b>Secondary Skills</b><br /></div><div><p><span><span><span>Strong understanding of emerging technologies ,methodologies and products that may be of interest to our clients</span></span></span></p></div></div></div></div></div><div><div><div></div></div></div>",AWS Data Engineer,800000,1100000,https://www.naukri.com/job-listings-aws-data-engineer-capgemini-technology-services-india-limited-kolkata-hyderabad-secunderabad-pune-chennai-6-to-9-years-071222908006,"['Pyspark', 'Hadoop', 'EMR', 'SQL', 'Aurora', 'Hive', 'Cloud Technologies', 'Kinesis', 'Big data', 'ETL', 'AWS', 'Lambda']","['Chennai', 'Pune', 'Hyderabad', 'Kolkata']",Data Engineer,2022-12-07 13:22:56
071222907504,"<span> </span> <p> </p> <ul> <li> <span> <span> <span> <span> Mentor or coach for scrum teams </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Expert into Agile Scrum principals, Task meeting/Retrospective </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Proven in Relative estimation, Story-based development </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Proficient in leading Iteration/sprint planning meeting, Conflict Resolution </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Strong into Business Analysis planning and monitoring, Enterprise Analysis, Requirement management and communication </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Provide objective guidance without personal or political considerations </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Experienced in implementing agile techniques in different cultures and environments </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Focus on people and Improvement by providing team a platform for improving not only during the retro but all the time. Create a safe environment for healthy conflict and meaningful collaboration. </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Experience to provide training to the team on the agile methodologies </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Implement the winning strategy according as per the ground conditions. </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Agile processes in each sprint at user story level as per the Definition of Done (DoD). </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Successfully run agile projects of varying size and complexity </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Identify project risks and raise them dedicatedly </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Agile process during the project execution; (on the ground to answer all the questions immediately). </span> </span> </span> </span> </li> </ul> <p> <span> Required Technical and Professional Expertise </span> <span> </span> </p> <ul> <li> Hadoop, Hive, Spark / PySpark, SQL, Oozie  </li> <li> Data Modelling in Hive  </li> <li> Programming Languages: Java / Python / Scala  </li> <li> Should have done micro / macro designing  </li> <li> Familiar with Unix Commands and basic work experience in Unix Shell Scripting  </li> <li> Data Modeling SQL Scripting (Teradata), BDW Data modelling  </li> <li> Excellent communication skills  </li> <li> Excellent client facing skills </li> </ul>",Data Engineer: Data Modeling,600000,1000000,https://www.naukri.com/job-listings-data-engineer-data-modeling-ibm-india-pvt-limited-pune-2-to-5-years-071222907504,"['Conflict resolution', 'Programming', 'SQL scripting', 'Unix shell scripting', 'Python']",['Pune'],Data Engineer,2022-12-07 13:12:33
071222907345,"As Snowflake Developer, you are responsible to develop end to end application software including design, coding, testing, deploy and supporting of applications <p> </p> <p> Responsibilities: </p> <ul> <li> Responsible to develop triggers, functions, stored procedures to support this effort </li> <li> Assist with impact analysis of changing upstream processes to Data Warehouse and Reporting systems </li> <li> Assist with design, testing, support, and debugging of new and existing ETL and reporting processes. </li> <li> Perform data profiling and analysis using a variety of tools. Troubleshoot and support production processes. Create and maintain documentation. </li> </ul> <p> If you thrive in a dynamic, collaborative workplace, IBM provides an environment where you will be challenged and inspired every single day. And if you relish the freedom to bring creative, thoughtful solutions to the table, there s no limit to what you can accomplish here. <br /> <span> </span> <br /> <span> Required Technical and Professional Expertise </span> <span> </span> </p> <ul> <li> Minimum 5 years of experience in developing software applications including: analysis, design, coding, testing, deploy and supporting of applications </li> <li> Hands-on experience with Snowflake utilities, SnowSQL, SnowPipe, Big Data model techniques </li> <li> Extensive experience in developing complex stored procedures/SQL queries. </li> <li> Experience in Data Migration from RDBMS to Snowflake cloud data warehouse </li> <li> Deep understanding of relational as well as NoSQL data stores, methods and approaches (star and snowflake, dimensional modelling) </li> </ul> <p> <span> Preferred Technical and Professional Expertise </span> <span> </span> </p> <ul> <li> Hands-on with any scripting skills is preferred (Python/Spark) </li> <li> Experience in ETL development & deployment using IBM DataStage Knowledge on AWS Knowledge on Data Build Tool (dbt) </li> <li> You love collaborative environments that use agile methodologies to encourage creative design thinking and find innovative ways to develop with cutting edge technologies </li> <li> Ambitious individual who can work under their own direction towards agreed targets/goals and with creative approach to work </li> <li> Intuitive individual with an ability to manage change and proven time management </li> <li> Proven interpersonal skills while contributing to team effort by accomplishing related results as needed </li> <li> Up-to-date technical knowledge by attending educational workshops, reviewing publications </li> </ul>",Data Engineer: Data Warehouse,400000,800000,https://www.naukri.com/job-listings-data-engineer-data-warehouse-ibm-india-pvt-limited-mumbai-5-to-10-years-071222907345,"['Data migration', 'NoSQL', 'Debugging', 'Stored procedures', 'Data warehousing']",['Mumbai'],Data Engineer,2022-12-07 13:09:05
091222910101,"<li> <strong> Project Role : </strong> Data Platform Engineer </li> <li> <strong> Project Role Description : </strong> Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models. </li> <li> <strong> Management Level : </strong> 10 </li> <li> <strong> Work Experience : </strong> 4-6 years </li> <li> <strong> Work location : </strong> Bengaluru </li> <li> <strong> Must Have Skills : </strong> </li> <ul> <br /> <li> <strong> Key Responsibilities :  </strong> a Need to collaborate with system owners and technical personnel for data governance and resolves any data quality or technical issues related to datab Plan and lead MDM requirement analysis sessions with business users c Coordinate MDM program activities for the Customer and Product data domains d Conduct source system analysis, and data profiling for well lifecycle managemente Interprets business requirements and converts them into technical requirements, defines use cases, test scenario </li> <br /> <li> <strong> Technical Experience :  </strong> a Collaborates with source systems data stewards, system owners and technical personnel for data governance and resolves any data quality or technical issues related to data ingestionb Analyzing cleansing various source systems data and building high performance Inbound and Outbound data integrations using Informatica ETLc Hands-on experience with Informatica MDM Hub config - Data modeling Data Mappings Landing, staging and Base Objects, Data validation, Match, Merge, Trust and Survivorship </li> <br /> <li> <strong> Professional Attributes :  </strong> a Self-starter, ability to manage own time and priorities, and guide/mentor/lead others b Good communication skills, positive attitude, team player </li> </ul>",Informatica MDM Data Platform Engineer,900000,1400000,https://www.naukri.com/job-listings-informatica-mdm-data-platform-engineer-accenture-solutions-pvt-ltd-bangalore-bengaluru-4-to-6-years-091222910101,"['Outbound', 'Data validation', 'data governance', 'Management']",['Bengaluru'],Data Engineer,2022-12-09 18:33:46
071222500121,"<ul> <li> <span> <span> <span> <span> <span> <span> <span> <span> Design, build, test and deploy cutting edge solutions at scale, impacting millions of customers worldwide drive value from data at Scale. </span> </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> <span> Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery and machine learning algorithms, re-designing infrastructure for greater scalability and speed. </span> </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> <span> Identify right open source tools to deliver product features by performing research, POC/Pilot and interacting with various open source forums.  </span> </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> <span> Interact with engineering teams across geographies to leverage expertise and contribute to the tech community. </span> </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> <span> Engage with Project Management and Business to drive the agenda, set your priorities and deliver solutions by leveraging the power of data. </span> </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> <span> Assemble large and complex data sets that meet functional / non-functional business requirements </span> </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> <span> Building and optimizing data pipelines, architectures and data sets. </span> </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> <span> Building and optimizing Machine Learing workflows to deliver actionable insights to the business. </span> </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> <span> Ability to work on multiple assignments and communication with stake holders with minimal supervision </span> </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> <span> Flexibility to support project issues during weekend and non-working hours. </span> </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> <span> Self-starter, lifelong learner and curiosity are also key qualities that help to define successful candidates. </span> </span> </span> </span> </span> </span> </span> </span> </li> </ul> <p> <span> <span> <span> <span> </span> </span> </span> </span> </p> <p> <span> <span> <span> <span> <b> <span> <span> <span> Work Experience- </span> </span> </span> </b> </span> </span> </span> </span> </p> <p> <span> <span> <span> <span> </span> </span> </span> </span> </p> <ul> <li> <span> <span> <span> <span> <span> <span> <span> <span> 2 to 8 years of experience in software development/Data & Machine Learning Engineering and minimum 2+ years of experience in Big Data engineering. </span> </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> <span> Experience working with Big data Ecosystem like (AWS cloud native tooling, HDFS, MapReduce, Hive, TEZ, Spark, Oozie, Sqoop, Kafka and Any NoSQL databases). </span> </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> <span> Hands on experience in at-least one of the programming languages ? Python (preferred), , PySpark, Java, Scala along with non-procedural language such as SQL. </span> </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> <span> Experience working with LLAP and/or spark ? LLAP will be a plus </span> </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> <span> Experience in creating data and machine learning workflows to control Java, Hive, Spark, SSH and Shell actions. </span> </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> <span> Experience in Linux/Unix shell scripting. </span> </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> <span> Experience working with large data sets in distributed computing to perform Massive parallel processing which Contributes to build analytics pipeline. </span> </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> <span> Experience in building python modules and packages to make reusable components </span> </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> <span> Experience handling workflow failures for the running data & machine learning pipelines </span> </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> <span> Troubleshooting application issues by analysing the logs. </span> </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> <span> Experience in implementing performance optimization techniques in HQL, Spark applications, Python, Scala and Sqoop jobs as well as machine learning hyper-parameter tuning. </span> </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> <span> Should be a thought leader and also a good team player. </span> </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> <span> Knowledge in data privacy and security and being able to develop practices here together with the respective business functions such as IT Security and Data Privacy Office </span> </span> </span> </span> </span> </span> </span> </span> </li> </ul>",Sr Data Engineer - Group Functions,400000,1000000,https://www.naukri.com/job-listings-sr-data-engineer-group-functions-milliporesigma-bangalore-bengaluru-2-to-8-years-071222500121,['Sr Data Engineer - Group Functions'],['Bengaluru'],Data Engineer,2022-12-07 12:54:47
071222012927,"<p> </p><p>Neiman Marcus Group has an immediate opening for a Data Engineer. Data engineer will have the unique combination of business acumen needed to interface directly with key stakeholders to understand the problem along with the skills and vision to translate the need into a world-class technical solution using the latest technologies</p><p>This person will be a hands-on role who is responsible for building data engineering solutions for NMG Enterprise using cloud-based data platform. Data engineer will provide day-to-day technical deliverables and participate in technical design, development and support for data engineering workloads. In this role, you need to be equally skilled with the whiteboard and the keyboard.</p><p><strong>Essential Duties & Responsibilities:</strong></p><ul><li>Understand and Analyze data from multiple data sources and develop technology to integrate the enterprise data layer</li><li>Create robust and automated pipelines to ingest and process structured and unstructured data from source systems into analytical platforms using batch and streaming mechanisms leveraging cloud native toolset</li><li>Work activity includes processing complex data sets, leveraging technologies used to process these disparate data sets and understanding the correlations as well as patterns that exist between these different data sets</li><li>Implement orchestrations of data pipelines and environment using Airflow</li><li>Implement custom applications using the Kinesis, Lambda and other AWS toolset as required to address streaming use cases</li><li>Implement automation to optimize data platform compute and storage resources</li><li>Develop and enhance end to end monitoring capability of cloud data platforms</li><li>Participate in educating and cross training other team members</li><li>Provide regular updates to all relevant stakeholders</li><li>Participate in daily scrum calls and provide clear visibility to work products</li></ul><p><strong>Qualifications/Competencies:</strong></p><ul><li>Bachelors degree with emphasis on Computer Science, Engineering, Information Systems, Mathematics, Statistics, or related discipline.</li><li>6+ years of experience in the data engineering and analytic space</li><li>5+ years of Python experience. Solid programing experience in Python - needs to be an expert in this 4/5 level. (Must have strong Python skills, along with lambdas and Airflow Dag processing.)</li><li>8+ years of RDBMS concepts with strong data analysis and SQL experience</li><li>3+ years of Linux OS command line tools and bash scripting proficiency</li><li>1+ year of experience working on Big Data Processing Frameworks and Tools</li><li>Exposure to software engineering such as parallel data processing, data flows, REST</li><li>2+ years worked on real-time data capture, processing and storing using technologies like Kafka, AWS Kinesis.</li><li>2+ year worked on AWS technology and services</li><li>APIs, JSON, XML, and micro service architectures</li><li>Certification –preferably AWS Certified Big Data or any other cloud data platforms, big data platforms</li></ul><p><strong>Nice to have: </strong></p><ul><li>Cloud data warehouse experience - Snowflake is a plus</li><li>Data Modeling experience a plus</li></ul><p><strong>Essential Duties & Responsibilities:</strong></p><ul><li>Understand and Analyze data from multiple data sources and develop technology to integrate the enterprise data layer</li><li>Create robust and automated pipelines to ingest and process structured and unstructured data from source systems into analytical platforms using batch and streaming mechanisms leveraging cloud native toolset</li><li>Work activity includes processing complex data sets, leveraging technologies used to process these disparate data sets and understanding the correlations as well as patterns that exist between these different data sets</li><li>Implement orchestrations of data pipelines and environment using Airflow</li><li>Implement custom applications using the Kinesis, Lambda and other AWS toolset as required to address streaming use cases</li><li>Implement automation to optimize data platform compute and storage resources</li><li>Develop and enhance end to end monitoring capability of cloud data platforms</li><li>Participate in educating and cross training other team members</li><li>Provide regular updates to all relevant stakeholders</li><li>Participate in daily scrum calls and provide clear visibility to work products</li></ul>",Senior Data Engineer | Neiman Marcus Group,2500000,4000000,https://www.naukri.com/job-listings-senior-data-engineer-neiman-marcus-group-talent500-bangalore-bengaluru-6-to-11-years-071222012927,"['AWS', 'SQL']",['Bengaluru'],Data Engineer,2022-12-07 18:28:23
071222501072,"<div> <p> Metamap is looking to hire a Senior Data Engineer for our Data team. The candidate will be reporting to the Data Engineering and Tooling Manager. The candidate will be responsible for pulling our data together in a central data warehouse, from our disparate data sources today. This will involve conceptualizing, implementing and maintaining reliable, flexible and secure data pipelines. Great data pipelines will enable great data science and analytics. </p> </div> <p> <strong> Responsibilities </strong> </p> <ul> <li> Build a central data warehouse that serves as the ""source of truth"" for Metamap. </li> <li> Build data pipelines to integrate data from multiple sources, or to push data to the data warehouse. Ensure that these pipelines are reliable and can scale as the business scales. </li> <li> Work with core engineering teams to ensure that data analytics requirements are met in our production systems. </li> <li> Participating and collaborating with Product Owner/ Cross-functional teams in the organization to understand the business requirements and deliver solutions that can scale. </li> </ul> <p> <strong> Skills Experience </strong> </p> <ul> <li> 5 years of experience in data engineering with a strong background in software engineering. </li> <li> Strong fundamentals of object-oriented design, data structures, algorithms, and design patterns. </li> <li> Data engineering ETL processes, Kafka, Airflow, messaging systems e.g. Pubsub; in particular, building data warehouses, pipelines from scratch. </li> <li> Databases and data systems - MongoDB, ElasticSearch, data warehouses e.g. BigQuery, relational databases e.g. Postgres. </li> <li> Python, SQL, and experience with cloud platforms, such as GCP and AWS. </li> <li> Working with high volume and diverse data sources, and quickly evolving business requirements. </li> <li> Experience with graph databases, e.g. TigerGraph, dgraph would be a strong plus. </li> </ul>",Senior Data Engineer,500000,900000,https://www.naukri.com/job-listings-senior-data-engineer-metamap-bangalore-bengaluru-3-to-7-years-071222501072,"['Object oriented design', 'GCP', 'Data structures', 'MongoDB', 'Data analytics', 'Data warehousing', 'Financial services', 'SQL', 'Python', 'Recruitment']",['Bengaluru'],Data Engineer,2022-12-07 17:48:36
071222502615,"<p> <span> <span> <span> <b> <span> <span> </span> </span> </b> </span> </span> </span> </p> <ul> <li> <span> <span> <span> <span> <span> <span> <span> Develop, validate, scale, and deliver analytical solutions aimed at maintaining data quality  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> Work with  </span> </span> </span> <span> <span> <span> integrated data solutions such as “data lakes” and “data warehouses”  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> Collaborate with analytics and data science teams to design and plan data engineering solutions  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> Ensure data accuracy and completeness by performing quality data audits and analyses.  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> Support data-informed business decisions  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> Support ETL and other data engineering tasks for a customer data platform.  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> Develop and implement strategies and best practices to ensure appropriate access control and security of stored data for that customer data platform.  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> Perform explorative analyses to identify potential drivers for revenue or other substantial key business drivers or root causes of problems.  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> Support business decisions by providing data analysis and reporting of patterns, insights, and trends to decision-makers.  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> Support the development and evolution of marketing measurement frameworks, actionable reports and interactive dashboards using reporting and visualization tools  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> Translate data into actionable insights - make recommendations to team members & stakeholders to improve channel or functional health.  </span> </span> </span> </span> </span> </span> </span> </li> </ul> <p> </p> <p> </p> <p> <span> <span> <span> <b> <span> <span> <span> Minimum Qualifications:  </span> </span> </span> </b> </span> </span> </span> </p> <ul> <li> <span> <span> <span> <span> <span> <span> <span> Bachelor's Degree in Computer Science, Data Analytics, Statistics, Applied Math, Data Science, or technical discipline  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> 4+years of experience with data management, structuring, and engineering  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> 4+ years of experience with Cloud Computing Platforms (Google Cloud Platform preferred); Structured Query Language (SQL);SQL and NoSQL Databases; ETL Tools (Talend, Pentaho, DataFlow, Airflow, Kafka)  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> 3+ years of experience with programming skills such as Java, C/C++, Scala, and/or R with Python experience strongly preferred  </span> </span> </span> </span> </span> </span> </span> </li> </ul> <p> </p> <p> <span> <span> <span> <b> <span> <span> <span> Preferred Qualifications:  </span> </span> </span> </b> </span> </span> </span> </p> <ul> <li> <span> <span> <span> <span> <span> <span> <span> Master’s Degree in Computer Science, Data Analytics, Statistics, Applied Math, Data Science, or technical discipline  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> Degree in a life science related discipline and MBA or Ph.D. is a bonus  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> Data Engineering Certification (Google’s Certified Professional-Data-Engineer, IBM Certified Data Engineer – Big Data, CCP Data Engineer for Cloudera, etc)  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> Experience with Digital Analytics tools such as Google Analytics or Adobe Analytics  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> Experience with and demonstrated ability to apply reporting and data visualization tools such as Tableau or Google Data Studio  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> Working understanding of machine learning methods  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> Ability to conduct complex, important work independently  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> Highly skilled in developing data pipelining solutions and architecting cloud data  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> Marketing technology such as marketing automation, customer data platforms, customer relationship management,   </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> General understanding of marketing channels and functions or deep, practical expertise of a single digital marketing channel and function.  </span> </span> </span> </span> </span> </span> </span> </li> </ul> <p> </p> <p> <span> <span> <span> <b> <span> <span> <span> Key Behaviors  </span> </span> </span> </b> </span> </span> </span> </p> <ul> <li> <span> <span> <span> <span> <span> <span> <span> Enthusiastic, self-motivated, and proactive, with a solution-oriented mindset  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> Ability to utilize the available time to organize and complete work within given deadlines  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> Meticulous attention to detail, with an overall passion for continual improvement  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> Logical and methodical approach to problem-solving  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> Ability to communicate effectively with others verbally and in writing clearly and concisely.  </span> </span> </span> </span> </span> </span> </span> </li> </ul>","Senior Specialist, Data Engineer & Analytics",1000000,1500000,https://www.naukri.com/job-listings-senior-specialist-data-engineer-analytics-milliporesigma-bangalore-bengaluru-4-to-9-years-071222502615,['Analytics'],['Bengaluru'],Data Engineer,2022-12-07 21:03:23
071222500043,"<ul> <li> <span> <span> <span> <span> <span> <span> <span> Develop, validate, scale, and deliver analytical solutions aimed at maintaining data quality  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> Work with  </span> </span> </span> <span> <span> <span> integrated data solutions such as ?data lakes? and ?data warehouses?  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> Collaborate with analytics and data science teams to design and plan data engineering solutions  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> Ensure data accuracy and completeness by performing quality data audits and analyses.  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> Support data-informed business decisions  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> Support ETL and other data engineering tasks for a customer data platform.  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> Develop and implement strategies and best practices to ensure appropriate access control and security of stored data for that customer data platform.  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> Perform explorative analyses to identify potential drivers for revenue or other substantial key business drivers or root causes of problems.  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> Support business decisions by providing data analysis and reporting of patterns, insights, and trends to decision-makers.  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> Support the development and evolution of marketing measurement frameworks, actionable reports and interactive dashboards using reporting and visualization tools  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> Translate data into actionable insights - make recommendations to team members & stakeholders to improve channel or functional health.  </span> </span> </span> </span> </span> </span> </span> </li> </ul> <p> </p> <p> </p> <p> <span> <span> <span> <b> <span> <span> <span> Minimum Qualifications:  </span> </span> </span> </b> </span> </span> </span> </p> <ul> <li> <span> <span> <span> <span> <span> <span> <span> Bachelor's Degree in Computer Science, Data Analytics, Statistics, Applied Math, Data Science, or technical discipline  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> 4+years of experience with data management, structuring, and engineering  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> 4+ years of experience with Cloud Computing Platforms (Google Cloud Platform preferred); Structured Query Language (SQL);SQL and NoSQL Databases; ETL Tools (Talend, Pentaho, DataFlow, Airflow, Kafka)  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> 3+ years of experience with programming skills such as Java, C/C++, Scala, and/or R with Python experience strongly preferred  </span> </span> </span> </span> </span> </span> </span> </li> </ul> <p> </p> <p> <span> <span> <span> <b> <span> <span> <span> Preferred Qualifications:  </span> </span> </span> </b> </span> </span> </span> </p> <ul> <li> <span> <span> <span> <span> <span> <span> <span> Master?s Degree in Computer Science, Data Analytics, Statistics, Applied Math, Data Science, or technical discipline  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> Degree in a life science related discipline and MBA or Ph.D. is a bonus  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> Data Engineering Certification (Google?s Certified Professional-Data-Engineer, IBM Certified Data Engineer ? Big Data, CCP Data Engineer for Cloudera, etc)  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> Experience with Digital Analytics tools such as Google Analytics or Adobe Analytics  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> Experience with and demonstrated ability to apply reporting and data visualization tools such as Tableau or Google Data Studio  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> Working understanding of machine learning methods  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> Ability to conduct complex, important work independently  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> Highly skilled in developing data pipelining solutions and architecting cloud data  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> Marketing technology such as marketing automation, customer data platforms, customer relationship management, ?  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> General understanding of marketing channels and functions or deep, practical expertise of a single digital marketing channel and function.  </span> </span> </span> </span> </span> </span> </span> </li> </ul> <p> </p> <p> <span> <span> <span> <b> <span> <span> <span> Key Behaviors  </span> </span> </span> </b> </span> </span> </span> </p> <ul> <li> <span> <span> <span> <span> <span> <span> <span> Enthusiastic, self-motivated, and proactive, with a solution-oriented mindset  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> Ability to utilize the available time to organize and complete work within given deadlines  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> Meticulous attention to detail, with an overall passion for continual improvement  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> Logical and methodical approach to problem-solving  </span> </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> <span> Ability to communicate effectively with others verbally and in writing clearly and concisely  </span> </span> </span> </span> </span> </span> </span> </li> </ul>","Senior Specialist, Data Engineer & Analytics",600000,1000000,https://www.naukri.com/job-listings-senior-specialist-data-engineer-analytics-milliporesigma-bangalore-bengaluru-4-to-8-years-071222500043,"['Data Science', 'Data Engineering', 'Analysis']",['Bengaluru'],Data Engineer,2022-12-07 12:54:41
061222007151,"<p> </p><p><strong>What youll discover</strong></p><ul><li>Inclusive culture and career growth opportunities </li><li>A truly Global IT Organization that collaborates across North America, Europe, Asia and Australia, click here to learn more </li><li>Challenging, collaborative, and team-based environment <br /><br /> </li></ul><p><strong>What youll do</strong></p><p>The Global Merchandising Solutions Team is responsible for managing various merchandising, stores and distribution related solutions within TJX IT. The organization delivers capabilities that enrich the customer experience and provide business value. We seek a motivated, talented Senior Staff Engineer with good understanding of cloud base, database and BI concepts to help architect enterprise reporting solutions across global buying, planning and allocations. </p><p><strong>What you’ll need</strong></p><p>The Global Merchandising Solutions Team thrives on strong relationships with our business partners and working diligently to address their needs which supports TJX growth and operational stability. On this tightly knit and fast-paced solution delivery team you will be constantly challenged to stretch and think outside the box.  </p><p>You will be working with product teams, architecture and business partners to strategically plan and deliver the product features by connecting the technical and business worlds. You will need to break down complex problems into steps that drive product development while keeping product quality and security as the priority. You will be responsible for most architecture, design and technical decisions within assigned scope. In addition, you will be actively coaching and mentoring engineers and developers in the product teams to help identify and resolve issues with technology and product processes. </p><p><strong>Minimum Qualifications </strong></p><ul><li>Bachelor’s Degree or equivalent Engineering skillset / training / work experience in relevant technical domain </li><li>8+ years of strong development experience with large data warehouse solutions with Power BI capabilities while working in an Agile (Scrum/Kanban/SAFe) environment</li><li>Experience in Snowflake databases and Power BI solutions</li><li>Hands-on experience in leading and delivering large scale projects end to end whilst maintaining good understanding of Coding standards, Performance tuning and database concepts</li><li>Demonstrated leadership in the fields of data warehousing, database or data science</li><li>Strong communication and influence skills, to explain DevOps Processes with customers & management. Solid team player with mentorship skills</li><li>Ability to understand the work environment and competing priorities in conjunction with developing/meeting project goals</li><li>Shows a positive, open-minded and can-do attitude</li></ul><p><strong>Preferred Qualifications </strong></p><ul><li>Technical lead with strong automation and engineering mindset</li><li>Experience in building ETL solutions (Talend preferred) in Cloud</li><li>Experience with Snowflake database</li><li>Understanding of Data Modelling </li><li>Experience in programing languages like Python, Shell Scripting and Scala.</li><li>Experience in Linux, Windows and Spark platforms</li><li>Experience working in Agile team</li><li>Good communication skills and team work focus mindset.</li><li>Current with industry trends, IT Ops and industry best practices and able to identify the ones we should implement</li><li>Experience with tools such as JIRA, Jenkins, Ansible Tower, Service NOW</li><li>Knowledge of IT Security & Compliance</li><li>Experience with Oracle Exadata would be plus</li></ul>",Senior Data Engineer - ETL | TJX,3500000,7500000,https://www.naukri.com/job-listings-senior-data-engineer-etl-tjx-talent500-hyderabad-secunderabad-chennai-bangalore-bengaluru-8-to-13-years-061222007151,['ETL'],"['Chennai', 'Bengaluru', 'Hyderabad']",Data Engineer,2022-12-06 13:56:29
150622008691,"<br /><p>Makro Pro is an exciting <strong>new digital venture by the iconic Makro Pro</strong>. Our proud purpose is to build a <strong>technology platform that will help make business possible</strong> for restaurant owners, hotels, and independent retailers, and open the door for sellers. Makro Pro brings together the <strong>best talent across multi-nationals</strong> to transform the B2B marketplace ecosystem. We welcome <strong>bold, energetic, and thoughtful</strong> people who share our belief in collaboration, diversity, excellence, and putting customers at the heart of our work </p><br /><p><strong>Take your career to new heights in the future of B2B e-commerce</strong>. Join our team and help us build <strong>Southeast Asias next unicorn</strong>.</p><br /><p><strong>Your Challenge</strong></p><br /><p>As a Data Engineer, you will work closely with a multidisciplinary Agile team to build high quality data pipelines driving analytic solutions. These solutions will generate insights from our connected data, enabling <strong>Makro Pro</strong> to advance the data-driven decision-making capabilities of our enterprise. This role requires deep understanding of data architecture, data engineering, data analysis, reporting, and a basic understanding of data science techniques and workflows. The</p><p>ideal candidate is a skilled data / software engineer with experience creating data products supporting analytic solutions. The ideal candidate is a skilled data / software engineer with experience creating data products supporting analytic solutions. They are an Agile learner, possess strong problem-solving skills, work as part of a technical, cross functional analytics team, and want to Solve complex data problems and deliver the insights to enable analytics strategy.</p><br /><ul><li>Design, develop, optimize, and maintain data architecture and pipelines that adhere to ETL principles and business goals</li><li>Create data products for analytics and data scientist team members to improve their productivity</li><li>Lead the evaluation, implementation and deployment of emerging tools and process for analytic data engineering in order to improve our productivity as a team</li><li>Develop and deliver communication and education plans on analytic data engineering capabilities, standards, and processes</li></ul><br /><p><strong>Requirements</strong></p><ul><li>Previous experience as a data engineer or in a similar role</li><li>Technical expertise with data models, data mining, and segmentation techniques</li><li>Knowledge of programming languages (e.g. Java and Python)</li><li>Hands-on experience with SQL database design using Hadoop or BigQuery and experience with a variety of relational, NoSQL, and cloud database technologies</li><li>Great numerical and analytical skill</li><li>Worked with BI tools such as Tableau, Power BI, Looker, Shiny</li><li>Conceptual knowledge of data and analytics, such as dimensional modeling, ETL, reporting tools, data governance, data warehousing, structured and unstructured data.</li></ul><br /><p> </p><p><strong><u>Benefits:</u></strong> </p><br /><p><strong>Health Insurance</strong> At Makro Pro, we care about your health! Group insurance from a top insurance company is included in your benefitsOPD, IPD, Emergency OPD</p><p><strong>Provident Fund</strong> Makro Pro cares about your long-term plan! We offer 3% provident fund.</p><p><strong>Year-end bonus</strong> We include variable and performance bonus for our employees.</p><p><strong>Gym Facilities</strong> Our Head office has a fitness center, yoga room, and recreational space. Enjoy Bangkok scenery and work your body!</p><p><strong>Attractive Vacations days</strong> Enjoy our attractive annual leave. Lets say the minimum is 18 days!</p><p><strong>No overtime</strong> We work 5 days a week with. We set our own goals and deadlines.</p><p><strong>Cool hardware</strong> New MacBook. The tool to help you be the best of yourself.</p><br /><p> <strong><u>Best Culture</u></strong> </p><br /><ul><li>Clear focus.</li><li>Diverse Workplace (Our members are from around the world!)</li><li>Thai and Non-Thai are both welcome!</li><li>Non-hierarchical and agile environment</li><li>Growth opportunity and career path</li></ul><br />",Data Engineer - Remote,786,786,https://www.naukri.com/job-listings-data-engineer-remote-makro-pro-thailand-6-to-10-years-150622008691,"['python', 'Big Data', 'sql database']",['Thailand'],Data Engineer,2022-12-09 13:06:19
101222500712,"<p> <span> <span> <span> <span> <span> <span> </span> </span> </span> </span> </span> </span>   </p> <ul> <li> <span> Lead architectures design, data source acquisition and analysis, data extraction, transformation etc. </span> </li> <li> <span> Convert the business questions into data engineering projects, prepare data for modeling, conduct feature engineering and pattern analysis etc.  </span> </li> <li> <span> Working in Azure DevOps/Agile Scrum based execution methodologies. </span> </li> <li> <span> Develop monitoring technique for data quality and model performance; Manage the life cycle of the project deliverables through updated release.  </span> </li> <li> <span> Design and develop data ingestion pipelines in cloud frameworks. </span> </li> <li> <span> Knowledge of Conceptual, Logical and Physical data models required for Data Warehousing. </span> </li> <li> <span> Creating and maintaining Data Projects and deployments of the same.  </span> </li> <li> <span> Support our web-based software solutions/applications as required to serve various business productivity improvements. </span> </li> <li> <span> Deliver data visualization and interactive dashboards to internal/external customers </span> </li> </ul> <p> </p> <p> <b> Education Qualification:  </b> </p> <ul> <li> <span> Bachelor s/Master s/PhD Degree in Computer Science, Information Technology, Mathematics, Engineering, Computer Applications or related fields.  </span> </li> </ul> <p> </p> <p> <b> Technical Competence Skills: </b> </p> <ul> <li> <span> Working experiences in data infrastructures, warehouse, transform (ELT), schema models and dashboard. </span> </li> <li> <span> Working experience in any cloud platforms - Microsoft Azure / AWS/ Google Cloud. </span> </li> <li> <span> Strong programming/coding skills using Python, Java, MySQL, Spark etc. </span> </li> <li> Exposure to machine learning or machine learning pipelines is a plus. </li> </ul> <p> <b> <span> Leadership Competence: </span> </b> </p> <ul> <li> <span> Strong communication skills- ability to lead the technical and strategic discussions, both verbal and written, with global counterparts and leadership teams. </span> </li> <li> <span> Self-motivated- proactive in identifying the risks and issues in projects and taking lead in solving the problems quickly. </span> </li> <li> <span> Collaboration skills- drives collaborations with global teams while working on projects and develops trusted long-term professional relationships. </span> </li> <li> <span> Learning Agility- willing to continuously learn new products and technology. </span> </li> <li> </li> </ul> <p> <b> Relevant Experience: </b> </p> <ul> <li> <span> 1-3 years of relevant work experience in designing and maintaining enterprise data pipelines using CI/CD </span> </li> </ul> <p> <b> Others (% of travel, language, etc.):  </b> </p> <ul> <li> 30% </li> <li> English Hindi </li> </ul> <p>   </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> <br />   </p> <p> </p> <p>  </p>",Data Engineer,600000,1000000,https://www.naukri.com/job-listings-data-engineer-donaldson-gurgaon-gurugram-1-to-3-years-101222500712,"['Coding', 'MySQL', 'Schema', 'Machine learning', 'Data quality', 'Information technology', 'Monitoring', 'Data extraction', 'Python']",['Gurgaon'],Data Engineer,2022-12-10 23:47:28
101222003975,"<p><strong>Roles and Responsibilities</strong> </p><br /><p>This position is responsible for the successful delivery of business intelligence information to the entire organization and is experienced in BI development and implementations, data architecture and data warehousing. </p><br /><p> </p><p>Seeking AWS Cloud Engineer /Data Warehouse Developer for our Data CoE team to help us in configure and develop new AWS environments for our Enterprise Data Lake, migrate the on-premise traditional workloads to cloud.</p><p>Must have a sound understanding of BI best practices, relational structures, dimensional data modelling, structured query language (SQL) skills, data warehouse and reporting techniques. </p><br /><ul><li>Extensive experience in providing AWS Cloud solutions to various business use cases. </li><li>Creating star schema data models, performing ETLs and validating results with business representatives</li><li>Supporting implemented BI solutions by: monitoring and tuning queries and data loads, addressing user questions concerning data integrity, monitoring performance and communicating functional and technical issues.</li></ul><br /><p><strong>Desired Candidate Profile</strong> </p><br /><ul><li> <strong><u>Expertise in Lake Formation, Python programming, PySpark, Shell scripting</u></strong></li><li>AWS Certified Database Specialty or AWS Certified Data Analytics </li><li>Knowledge in all aspects of DevOps (source control, continuous integration, deployments, etc.)</li><li>Experience in SQL Server, SSIS, SSAS, SSRS</li><li>Ability to collaborate on a team with infrastructure, BI report development and business analyst resources, and clearly communicate solutions to both technical and non-technical team members </li></ul><br /><br /><br />",Godrej & Boyce - Data Engineer (Digital),1500000,2500000,https://www.naukri.com/job-listings-godrej-boyce-data-engineer-digital-godrej-boyce-g-b-mumbai-4-to-7-years-101222003975,"['Pyspark', 'Data Engineering', 'ssas', 'sql server']",['Mumbai'],Data Engineer,2022-12-10 12:05:21
021222500073,"<div> This position requires someone to work on complex technical projects and closely work with peers in an innovative and fast-paced environment. For this role, we require someone with a strong product design sense specialized in Hadoop and Spark technologies.  </div> <div> <b> <u> </u> </b>   </div> <div> <b> <u> Requirements: </u> </b> <u> </u> </div> <div> Minimum 3-6 years of experience in Big Data technologies.  </div> <div> The position Grow our analytics capabilities with faster, more reliable tools, handling petabytes of data every day.  </div> <div> Brainstorm and create new platforms that can help in our quest to make available to cluster users in all shapes and forms, with low latency and horizontal scalability. Make changes to our diagnosing any problems across the entire technical stack. Design and develop a real-time events pipeline for Data ingestion for real-time dash- boarding.  </div> <div> Develop complex and efficient functions to transform raw data sources into powerful, reliable components of our data lake. </div> <div> Design implement new components and various emerging technologies in Hadoop Eco- System, and successful execution of various projects. </div> <div> Be a brand ambassador for Paytm Stay Hungry, Stay Humble, Stay Relevant!  </div> <div> <b> <u> </u> </b>   </div> <div> <b> <u> Preferred Qualification </u> </b> <u> :  </u> </div> <div> Bachelor's/Master's Degree in Computer Science or equivalent. </div> <div> <b> <u> </u> </b>   </div> <div> <b> <u> Skills that will help you succeed in this role: </u> </b> <u> </u> </div> <div> Fluent with Strong hands-on experience with Hadoop, MapReduce, Hive, Spark, PySpark etc. </div> <div> Excellent programming/debugging skills in Python/Java/Scala. </div> <div> Experience with any scripting language such as Python, Bash etc. </div> <div> Good to have experience of working with noSQL databases like Hbase, Cassandra. </div> <div> Hands-on programming experience with multithreaded applications. </div> <div> Good to have experience in Database, SQL, messaging queues like Kafka. </div> <div> Good to have experience in developing streaming applications e.g. Spark Streaming, Flink, Storm, etc. </div> <div> Good to have experience with AWS and cloud technologies such as S3Experience with caching architectures like Redis etc. </div>",Data Engineer/Sr. Data Engineer,700000,1100000,https://www.naukri.com/job-listings-data-engineer-sr-data-engineer-paytm-noida-2-to-3-years-021222500073,"['Computer science', 'NoSQL', 'cassandra', 'spark', 'Debugging', 'SCALA', 'Product design', 'Analytics', 'Python', 'HBase']",['Noida'],Data Engineer,2022-12-02 13:45:26
011222502664,"<p> We are looking for a  <span> </span> <strong> Senior Data Engineer  </strong> to help our customers explore their healthcare data, understand how to improve the health of the population and bring down the cost of healthcare.  </p> <p> <strong> A Day in the Life at Innovaccer:  </strong> </p> <ul> <li> Play with and transform data.  </li> <li> Work towards creating easy-to-digest analytical reports for US healthcare customers.  </li> <li> Design and build interfaces that facilitate workflows between Data Activation Platform and client third party systems as scoped while complying with respective standards and industry best practices.  </li> <li> Define and document best practices along with thorough message specifications.  </li> <li> Monitor and tune the configuration of interfaces for high availability once deployed in production environments.  </li> <li> Understand Innovaccer data warehousing concepts and implement best practices.  </li> <li> Work on creating in-house predictive models to forecast medical events.  </li> </ul> Preferred experience:  <ul> <li> 4+ years in an analytics role in data services/product firm.  </li> <li> Data modeling ability - knowledge of different data modeling concepts.  </li> <li> Hands-on experience in creating statistical models; understanding when to use which model to fit the data better.  </li> <li> Strong knowledge of SQL and able to write complex SQL queries.  </li> <li> Exposure to Python Libraries - Numpy, Scipy, Scikit-Learn.  </li> <li> Hands-on experience with BI tools such as Power BI / Tableau / Sisense.  </li> <li> Self-starter, curious, accountable, enjoys a healthy level of autonomy, strong work ethic, and success in a fast-paced, high-intensity startup environment.  </li> <li> Extensive experience relaying technical and non-technical information clearly and concisely.  </li> <li> Demonstrated expert problem-solving and analytical skills.  </li> <li> Excellent oral and written communication skills.  </li> <li> Excellence in multitasking and managing multiple high-priority customer engagements at once.  </li> <li> Ability to assess complex client environments and workflows and arrive at integration solutions to satisfy seamless experience between our platform and theirs.  </li> <li> Ability to mentor junior team members and introduce industry expertise and best practices across integration development.  </li> <li> Bachelors degree in Engineering, Computer Science. An advanced degree in any of the areas above would be a plus.  </li> <li> Knowledge of SQL, ETL, and PowerBI / Tableau / Sisense.  </li> <li> Experience in US Healthcare  </li> </ul>",Senior Data Engineer,900000,1400000,https://www.naukri.com/job-listings-senior-data-engineer-innovaccer-noida-4-to-5-years-011222502664,"['Analytical skills', 'SQL queries', 'tableau', 'Data modeling', 'power bi', 'US healthcare', 'Analytics', 'SQL', 'Python']",['Noida'],Data Engineer,2022-12-01 23:16:43
301122912961,"<span> </span> <p> </p> <ul> <li> <span> <span> Develop, test and support future-ready data solutions for customers across industry verticals </span> </span> </li> <li> <span> <span> Develop, test and support end-to-end batch and near real-time data flows/pipelines </span> </span> </li> <li> <span> <span> Demonstrate understanding in data architectures, modern data platforms, big data, ML/AI, analytics, cloud platforms, data governance and information management and associated technologies </span> </span> </li> <li> <span> <span> Communicates risks and ensures understanding of these risks. </span> </span> </li> </ul> <p> <span> Required Technical and Professional Expertise </span> <span> </span> </p> <ul> <li> <span> <span> Minimum of 2 years of related experience required </span> </span> </li> <li> <span> <span> Experience in modeling and business system designs </span> </span> </li> <li> <span> <span> Good hands on experience on DataStage, Alteryx, Informatica & SSIS </span> </span> </li> <li> <span> <span> Have great expertise in writing TSQL code </span> </span> </li> <li> <span> <span> Experience in writing advanced Python and Spark for Data Processing </span> </span> </li> <li> <span> <span> Well versed with data warehouse schemas and OLAP techniques </span> </span> </li> </ul> <p> <span> Preferred Technical and Professional Expertise </span> <span> </span> </p> <ul> <li> <span> <span> Ability to manage and make decisions about competing priorities and resources. </span> </span> </li> <li> <span> <span> Ability to delegate where appropriate. </span> </span> </li> <li> <span> <span> Must be a strong team player/leader. </span> </span> </li> <li> <span> <span> Ability to lead Data transformation project with multiple junior data engineers  </span> </span> </li> <li> <span> <span> Strong oral written and interpersonal skills for interacting and throughout all levels of the organization.  </span> </span> </li> <li> <span> <span> Ability to clearly communicate complex business problems and technical solutions. </span> </span> </li> </ul>",Data Engineer,400000,800000,https://www.naukri.com/job-listings-data-engineer-ibm-india-pvt-limited-bangalore-bengaluru-2-to-4-years-301122912961,"['Interpersonal skills', 'Datastage', 'data governance', 'OLAP', 'Python']",['Bengaluru'],Data Engineer,2022-11-30 18:09:52
301122912954,"<ul><li>Develop, test and support future-ready data solutions for customers across industry verticals </li><li>Develop, test and support end-to-end batch and near real-time data flows/pipelines </li><li>Demonstrate understanding in data architectures, modern data platforms, big data, ML/AI, analytics, cloud platforms, data governance and information management and associated technologies </li><li>Communicates risks and ensures understanding of these risks. </li></ul><p><span>Required Technical and Professional Expertise </span><span></span></p><ul><li>Minimum of 2 years of related experience required </li><li>Experience in modeling and business system designs </li><li>Good hands on experience on DataStage, Alteryx, Informatica & SSIS </li><li>Have great expertise in writing TSQL code </li><li>Experience in writing advanced Python and Spark for Data Processing </li><li>Well versed with data warehouse schemas and OLAP techniques </li></ul><p><span>Preferred Technical and Professional Expertise </span><span></span></p><ul><li>Ability to manage and make decisions about competing priorities and resources. </li><li>Ability to delegate where appropriate. </li><li>Must be a strong team player/leader. </li><li>Ability to lead Data transformation project with multiple junior data engineers </li><li>Strong oral written and interpersonal skills for interacting and throughout all levels of the organization. </li><li>Ability to clearly communicate complex business problems and technical solutions. </li></ul>",Data Engineer,400000,800000,https://www.naukri.com/job-listings-data-engineer-ibm-india-pvt-limited-bangalore-bengaluru-2-to-4-years-301122912954,"['Interpersonal skills', 'Datastage', 'data governance', 'OLAP', 'Python']",['Bengaluru'],Data Engineer,2022-11-30 18:09:45
291122915361,"<div><div><div><b>Job Purpose and Impact</b><br /><span><span>The Data Engineer will design, build and execute high-performance, data-centric solutions using the our comprehensive big data capabilities. We seek a professional who can build and optimize data products to bring data and analytics products and solutions to our businesses.</span></span></div></div><div><div><br /><br /><b>Key Accountabilities</b><br /></div><div><ul><li>Facilitate data extraction, collection, cleansing and preparation for consumption.</li><li>Proficiency with reporting, data visualization, extract, transform and load, data management (<b>Hadoop/Cloudera, Knime, Alteryx, SQL Server</b>)</li><li>Experience in <b>advanced Hadoop capabilities including Hive, Spark, Parquet, Kudu</b></li><li>Respond to complex ad-hoc data requests.</li><li>Ensure timely and accurate data models that provide management information and analysis of function performance (key performance indicators, metrics).</li><li>Conduct analyses of performance variables, data management, measurement and analytics, data design principles, concept and practices.</li><li>Develop, with colleagues, detailed understanding of the business, and offer mentorship or technical solutions to solve business problems.</li><li>Independently solve moderately complex issues with minimal supervision, while escalating more complex issues to appropriate staff.</li><li>Other duties as assigned</li></ul></div></div><div><div><br /><br /><b>Qualifications</b><br /></div><div><p><span><span><span><b><span>Minimum Qualifications</span></b></span></span></span></p><ul><li>Bachelor's degree in a related field </li><li>5-8 years of experience in ETL / Data Engineering / Creating Data Pipelines</li><li>Technical expertise regarding data models, database design development, data mining and segmentation techniques</li><li>Minimum of two years of related work experience</li></ul><p><span><span><span><b><span>Preferred Qualifications</span></b></span></span></span></p><ul><li>Drive reporting through the use of available technology (Tableau, Power BI, Alteryx, Knime) and develop technical solutions for an effective, efficient data analytics platform.</li><li>Background in continuous improvement</li><li>Knowledge of statistics and experience using statistical packages for analyzing datasets.</li></ul></div></div></div>",Data Engineer,600000,1000000,https://www.naukri.com/job-listings-data-engineer-cargill-india-pvt-ltd-bangalore-bengaluru-5-to-8-years-291122915361,"['Data management', 'Database design', 'Alteryx', 'Hadoop', 'power bi', 'Data analytics', 'Tableau', 'Data mining', 'ETL', 'SQL', 'Data extraction']",['Bengaluru'],Data Engineer,2022-11-29 21:19:16
291122915092,"<div><div><div><br /><b>Job Purpose and Impact</b><br /></div><div><p>The Data Engineer III will design, build and operate high performance data centric solutions utilizing the comprehensive big data capabilities for the company's data platform environment. In this role, you will act as an authority for data access pathways and techniques working with analysts within the functional data analytics team. You will design data structures and pipelines to collect data and design and implement data transformations, combinations or aggregations.</p></div></div><div><div><br /><br /><b>Key Accountabilities</b><br /></div><div><ul><li>Collaborate with businesses, application and process owners, and product team members to define requirements and design solutions for the company's big data and analytics solutions. </li><li>Participate in the decision-making process related to architecting solutions. </li><li>Develop technical solutions utilizing big data and cloud based technologies and ensuring they are designed and built to be sustainable and robust. </li><li>Perform data modeling and prepare data in databases for use in various analytics tools and configurate and develop data pipelines to move and optimize data assets. </li><li>Provide necessary technical support through all phases of solution life cycle. </li><li>Build prototypes to test new concepts and be a key contributor of ideas and code that improve the core software infrastructure, patterns and standards. </li><li>Help drive the adoption of new technologies and methods within the functional data and analytics team and be a role model and mentor for data engineers. </li><li>Independently handle complex issues with minimal supervision, while escalating only the most complex issues to appropriate staff. </li><li>Other duties as assigned </li></ul></div></div><div><div><br /><br /><b>Qualifications</b><br /></div><div>Minimum Qualifications<ul><li>Bachelor's degree in a related field or equivalent experience </li><li>Minimum of four years of related work experience </li><li>Other minimum qualifications may apply </li></ul>Preferred Qualifications <ul><li>Experience developing data or software applications including analysis, design, coding, testing, deploying and supporting of applications. </li><li>Experience working with big data platform. </li><li>Experience with reporting tools and data sources. </li></ul></div></div></div><div></div>",Data Engineer III,600000,800000,https://www.naukri.com/job-listings-data-engineer-iii-cargill-india-pvt-ltd-bangalore-bengaluru-4-to-6-years-291122915092,"['coding', 'data sources', 'software infrastructure', 'testing', 'Data Engineer', 'reporting tools']",['Bengaluru'],Data Engineer,2022-11-29 20:07:55
301122912527,"<ul><li>Mentor or coach for scrum teams </li><li>Expert into Agile Scrum principals, Task meeting/Retrospective </li><li>Proven in Relative estimation, Story-based development </li><li>Proficient in leading Iteration/sprint planning meeting, Conflict Resolution </li><li>Strong into Business Analysis planning and monitoring, Enterprise Analysis, Requirement management and communication </li><li>Provide objective guidance without personal or political considerations </li><li>Experienced in implementing agile techniques in different cultures and environments </li><li>Focus on people and Improvement by providing team a platform for improving not only during the retro but all the time. Create a safe environment for healthy conflict and meaningful collaboration. </li><li>Experience to provide training to the team on the agile methodologies </li><li>Implement the winning strategy according as per the ground conditions. </li><li>Agile processes in each sprint at user story level as per the Definition of Done (DoD). </li><li>Successfully run agile projects of varying size and complexity </li><li>Identify project risks and raise them dedicatedly </li><li>Agile process during the project execution; (on the ground to answer all the questions immediately). </li></ul><p><span>Required Technical and Professional Expertise </span><span></span></p><ul><li>Overall industry experience of 12 years </li><li>Hadoop, Hive, Spark / PySpark, SQL, Oozie </li><li>Data Modelling in Hive </li><li>Programming Languages: Java / Python / Scala </li><li>Should have done micro / macro designing </li><li>Familiar with Unix Commands and basic work experience in Unix Shell Scripting </li><li>Data Modeling SQL Scripting (Teradata), BDW Data modelling </li><li>Excellent communication skills </li><li>Excellent client facing skills </li></ul>",Data Engineer: Data Modeling,600000,1000000,https://www.naukri.com/job-listings-data-engineer-data-modeling-ibm-india-pvt-limited-bangalore-bengaluru-12-to-15-years-301122912527,"['Conflict resolution', 'Programming', 'SQL scripting', 'Unix shell scripting', 'Python']",['Bengaluru'],Data Engineer,2022-11-30 18:01:50
071222502198,"<ul> <li> 6+ years of experience in data engineering with an emphasis on architecting and implementing data analytics, data integration and reporting. </li> <li> 3+ years of experience with Microsoft Azure cloud platform specifically worked on Azure data engineering services like ADF, IOT HUB, Event hubs, Function App. </li> <li> 2+ years of experience in SQL, data transformations, and troubleshooting across more than one Database Platform (Azure Synapse Analytics, MySQL, Snowflake, PostgreSQL, Redshift, Azure SQL Warehouse, etc). </li> <li> 3+ years of experience in the design and build of data extraction, transformation, and loading processes by writing custom data pipelines or ETL/ELTs. </li> <li> 3+ years of experience with one or more of the follow scripting languages: Python, SQL, and/or other. </li> <li> 1+ years of experience designing and building solutions utilizing Azure Cloud services such as ADF, IOT Hub, Event hubs, Streaming Analytics, Storage account, data lake, API gateway, etc </li> <li> Bachelor s degree or equivalent work experience. </li> </ul> <p> <b> Preferred </b> </p> <ul> <li> Working knowledge on IAAC; eg using Terraform, ARM template </li> <li> Designed data warehouse or data vaults. (Data modeling) experience. </li> <li> Working knowledge on setting up Azure DevOps, (CI/CD), repositories processes </li> <li> Azure Cloud Platform Certification. </li> <li> Experience working with agile development methodologies. </li> </ul>",Azure Data engineer,700000,1100000,https://www.naukri.com/job-listings-azure-data-engineer-tieto-software-technologies-pvt-ltd-pune-6-to-8-years-071222502198,"['Engineering services', 'Data modeling', 'Postgresql', 'MySQL', 'Cloud', 'Troubleshooting', 'Analytics', 'SQL', 'Python', 'Data extraction']",['Pune'],Data Engineer,2022-12-07 19:48:32
061222501156,"<div> <ul> <li> <span> <span> <span> Work as a team member and deliver quality work for tasks assigned in a timely manner and on time. Matillion is used for transformation. You will find yourself dealing with shell scripts, Informatica workflows, and sometimes stored procedures.  </span> </span> </span> </li> <li> <span> <span> <span> Looking for a tech savvy Data Engineer to Design, Develop and Support ETL interfaces of a big data marketing technology platform built on AWS.  </span> </span> </span> </li> <li> <span> <span> <span> Understand the existing landscape, document and optimize the pipelines for best performance  </span> </span> </span> </li> <li> <span> <span> <span> Interact with Business and Marketing users, Data Scientists and other developers.  </span> </span> </span> </li> <li> <span> <span> <span> 2 years of experience on Bigdata solutions like Hadoop, Hive and PySpark.  </span> </span> </span> </li> <li> <span> <span> <span> 1 years of experience in AWS using services like S3, EC2, RDS, EMR, Redshift.  </span> </span> </span> </li> <li> <span> <span> <span> Expertise with Python Language is preferred.  </span> </span> </span> </li> <li> <span> <span> <span> Experience building and optimizing RDBMS/BIGDATA data pipelines, architecture and data sets.  </span> </span> </span> </li> <li> <span> <span> <span> Good to have knowledge in ETL tools such as informatica or pentaho. </span> </span> </span> </li> </ul> </div>",BI Data Engineer,500000,900000,https://www.naukri.com/job-listings-bi-data-engineer-ntt-global-delivery-services-limited-hyderabad-secunderabad-2-to-7-years-061222501156,"['hive', 'Architecture', 'RDBMS', 'Hadoop', 'Informatica', 'Stored procedures', 'big data', 'AWS', 'Pentaho', 'Python']",['Hyderabad'],Data Engineer,2022-12-06 18:26:15
301122914183,"<br /> Data Engineer-  <br /> 3 years minimum experience with software engineering (Python), preferably in cloud implementations  <br /> Data engineer with proven experience (portfolio) setting up Big Data streaming pipelines using Apache Flink or other streaming technologies using time series data Proficiency with Kubernetes Experience setting up and managing Kubernetes clusters containing Flink jobs is a plus (configuration, memory management, log management, checkpointing, etc.) Coding proficiency in Python and Scala  <br /> Nice to Have: experience with the Azure Cloud solutions hands-on experience with Linux/UNIX based Systems experience working in agile / scrum development processes proven knowledge and mastery of DevOps tools and processes proven knowledge and mastery of big data challenges and tools specialization in implementation of state-of-the-art big data management and processing algorithms for practical business applications.",Industry X - Software Engineering - Data Engineer - 11,1200000,1600000,https://www.naukri.com/job-listings-industry-x-software-engineering-data-engineer-11-accenture-solutions-pvt-ltd-bangalore-bengaluru-3-to-6-years-301122914183,"['Data management', 'Memory management', 'Consulting', 'SCALA']",['Bengaluru'],Data Engineer,2022-11-30 18:35:31
011222904345,"<p><strong></strong></p><ul><li>You will take ownership of the technical aspects of implementing data pipeline & migration requirements, ensuring that the platform is being used to its fullest potential through designing and building applications around our customer’s needs. </li><li>Migrate on-premise data applications & pipelines to GCP cloud leveraging technologies such as Terraform, Spark, Airflow etc. </li><li>Work directly with our internal product/technical teams to ensure that our technology infrastructure is seamlessly and effectively integrated with our third-party software, conceive, and build the necessary applications to make this happen. </li><li>Provide on-call support for migration-related issues wherever applicable. </li><li>Interface directly with stakeholders to gather requirements and own the automated end-to-end data engineering solutions. </li><li>Implement data pipelines to automate the ingestion, transformation, and augmentation of both structured and unstructured data sources, and provide best practices for pipeline operations </li><li>Troubleshoot and remediate data quality issues raised by pipeline alerts or downstream consumers. </li><li>Provide advice and ideas for technical solutions and improvements to data systems </li><li>Create and maintain clear documentation on data models/schemas as well as transformation/validation rules </li><li>Implement tools that help data consumers to extract, analyze, and visualize data faster through data pipelines </li><li>Leading the entire software lifecycle including hands-on development, code reviews, testing, deployment, and documentation for batch ETL's. </li><li>Engage with stakeholders to gather requirements to deliver data solutions </li></ul><p><strong>We’re excited if you have: </strong></p><ul><li>Bachelor’s degree in Computer Science or equivalent with substantial data engineering experience. </li><li>8+ years of recent hands-on experience with a modern programming language (Scala, Python, Java) is required; Spark/ Pyspark is preferred. </li><li>Experience with version control apps (ie: Github) and experience working within a CI/CD framework is a plus. An even bigger plus if you have experience building framework </li><li>8+ years of recent hands-on SQL programming experience in a Big Data environment is required; Hadoop/Hive experience is preferred. </li><li>Experience with GCP Cloud, Teradata Vantage is a plus </li><li>Experience developing and maintaining ETL applications and data pipelines using big data technologies is required; Airflow experience is a plus! </li><li>Experience in managing multiple projects and stakeholders with excellent communication and interpersonal skills </li><li>Experience building data solutions for visualization softwires (ie: Tableau) is a plus </li><li>Ability to develop and organize high-quality documentation </li><li>Superior analytical skills and a strong sense of ownership in your work </li><li>Ability to thrive in a fast-paced start-up environment, and to manage multiple, competing priorities simultaneously </li><li>Prior e-commerce experience is a big plus. </li></ul><p><strong>We value engineers who are: </strong></p><ul><li>Customer-focused: We believe that doing what’s right for the customer is ultimately what will drive our business forward. </li><li>Obsessed with quality: Your production code just works & scales linearly </li><li>Team players. You believe that more can be achieved together. You listen to feedback and also provide supportive feedback to help others grow/improve. </li><li>Fast learners: We are willing to disrupt our existing business to trial new products and solutions. You love learning how to use new technologies and then rapidly apply them to new problems. </li><li>Pragmatic: We do things quickly to learn what our customers desire. You know when it’s appropriate to take shortcuts that don’t sacrifice quality or maintainability. </li><li>Owners: Engineers at Groupon know how to positively impact the business. </li></ul><br />",Senior Data Engineer,1000000,1200000,https://www.naukri.com/job-listings-senior-data-engineer-groupon-shared-services-pvt-ltd-bangalore-bengaluru-8-to-10-years-011222904345,"['Java', 'Pyspark', 'Scala', 'Spark', 'Tableau', 'ETL', 'Python', 'SQL']",['Bengaluru'],Data Engineer,2022-12-01 10:10:17
091222501442,"<ul> <li> Contribute to define feasibility (technical options...), functional design and solution validation.  </li> <li> Performs impact analysis related to data captation (system performance in production, security, etc.)  </li> <li> Identifies the source of physical data  </li> <li> Enforces security and confidentiality rules for data on its perimeter.  </li> </ul> <p> <b> <u> <b> Implementation and Deployment  </b> </u> </b> </p> <ul> <li> Completes and executes data collection procedures  </li> <li> Builds data collection infrastructure (IoT: Internet of Things, sensors, database, files, etc.)  </li> <li> Deploys data collection infrastructure and procedures  </li> <li> Measures the impact on the technical chain implemented (from the connected object to the storage system)  </li> <li> Completes the necessary tests to validate the solution  </li> <li> Documents the implemented solutions  </li> </ul> <p> <b> <u> <b> Support and troubleshooting  </b> </u> </b> </p> <ul> <li> Analyzes and understands the origin of a complex malfunction, incident or bug.  </li> <li> Adopts a proactive approach to avoid or identify root causes of problems.  </li> <li> Provides technical assistance to users.  </li> </ul>",DATA-Sr Data Engineer,700000,1100000,https://www.naukri.com/job-listings-data-sr-data-engineer-michelin-pune-2-to-6-years-091222501442,"['Data collection', 'Infrastructure', 'Database', 'Deployment', 'Sensors', 'Troubleshooting']",['Pune'],Data Engineer,2022-12-09 19:45:09
091222502851,"<ul> <li> Effectively guide the team for Maintenance, Reliability & Turnaround Data, and the documentation process and deliver it successfully by ensuring quality assurance, to meet/achieve their plant maintenance data request management KPIs to meet 100% by bringing process improvements through automation and continuously improving the process.  </li> <li> The primary responsibility in preparing/developing comprehensive maintenance strategy from overhaul reports, maintenance reports, general tasklist, equipment-specific task lists, spare part lists, equipment drawings, circuit diagrams, P&IDs, operation and maintenance manuals of OEMs, etc., for equipment like engines, pumps, compressors, turbines, etc., as per shell standards.  </li> <li> These revised maintenance strategies and reliability improvement plans should include a detailed description of the tasks that need to be performed for overhauling/maintenance of each equipment/installation.  </li> <li> Constantly engage with site maintenance leads, original equipment manufacturers, and vendors, and conduct reliability studies in consultation with site maintenance personnel.  </li> <li> Interact and communicate with the site focal for reliability studies and maintenance strategies, understanding the intricacies of how the actual maintenance activities for rotating equipment have been planned and performed in various Operating Units of Shell.  </li> <li> Understanding the functions of the various equipment and instruments and also a thorough knowledge of reading/ interpreting the engineering drawings (P&IDs, PEFS, PFDs).  </li> <li> Managing stakeholders/ representatives of various operating units through effective communication and identifying & resolving the bottlenecks for data analysis, maintaining issue logs, and using the same in implementing changes/improvements in the processes.  </li> <li> Expected to work in collaboration with the MTO team to identify, prioritize, and take action to mitigate threats and realize opportunities to meet the Strategic Asset Management Plan and business plan delivery.  </li> <li> Prepare and update optimized equipment care strategies and plans for the systems within operating units and support them to sustain asset performance.  </li> <li> To achieve business Context, Performance targets, Asset Integrity, Availability, and costs, risk-based methodologies like (RCM/RBI/SIS) are utilized to Optimize Maintenance Activities.  </li> <li> The Asset Register shall be complete for all equipment in MEC scope and maintained in a Computerized Maintenance Management System (CMMS). Defining Functional System Priority considering the Production Loss Equation, Asset, and Environment Consequences. RAM Matrix is used for this activity.  </li> <li> Based on the prioritization of the Functional System, RCM Study is conducted using tools like w-IMS, w-RCM, etc. Equipment care tasks shall be defined and documented in a GPO-approved IMS application. </li> <li> The Engineering disciplines shall be accountable for defining equipment care tasks and optimizing them for a benchmarked competitive cost. After proper review of these risk-based recommended strategies/tasks, by Reliability Team, the same shall be updated/implemented in CMMS/SAP.  </li> <li> Can be assigned as a trainer for the area of process tools expertise. May share the responsibility with the supervisor in staff development of other team members.  </li> <li> Create/Develop Standard Work Instructions/Standard ways of working for PM data management for assigned processes viz. Plant maintenance Master data management Process, Run and maintain Processes for Operating Units and other assigned processes.  </li> <li> Quality: Maintain Shell data minimum standards for data input to ERP and CMMS systems across the operating unit. Manage relationships with Asset, Functional, and TAO-based technical authorities, subject matter experts from the business, engineering managers and leads, operation managers, etc.  </li> <li> Leadership. Coach and mentor staff on technical topics and AMS work processes. Expected to be able to coach MEC topics to skill level and others to awareness.  </li> </ul> <p> </p> <p> <b> Skills & Requirements:  </b> </p> <ul> <li> University Degree in Mechanical / Electrical / Instrumentation / Chemical and other oil and gas-related degree.  </li> <li> Degrees in other fields will be considered with the relevant experience.  </li> <li> 5 years+ or more of work experience in the Oil & Gas / Petrochemicals industry past degree in engineering with experience in one or more of the following:  </li> <li> Shall be able to prepare/develop asset structure, SCE review and implementation, and bill of materials (BoM) using the equipment's drawings, circuit diagrams, PEFS, P&IDs, operation, and maintenance manual of OEMs.  </li> <li> Data gathering for the reliability analysis from CMMS, maintenance manuals, best practices, etc. Perform Gap analysis between the P&ID s and CMMS maintainable data.  </li> <li> Prepare & update the schedules for maintenance strategy reviews and conduct the reviews based on Risk and Reliability Management principles with tools such as PM Optimization (PMO), REM strategy reviews, RCM, and Equipment Care Strategy Library where applicable.  </li> <li> Shall be able to conduct reliability-centered maintenance studies for the rotating equipment, including package equipment. Able to interpret the RCM study results and convert them into PM routines for better maintenance efficiency.  </li> <li> Participation/Leading end-to-end RCM process starting with identifying Threats and Opportunities to implementing RCM outcome with a mindset of CI (Continuous Improvement) is preferred.  </li> <li> Integrity Assurance: Work with Technical Authorities (TA) on critical data for safety-critical elements/ equipment; identification & review of Performance Standards for the SCE. Assist Reliability Technician/CMMS team on data upload to CMMS.  </li> <li> Follow the AMS maintenance execution process (ME). Use SAP Preventative Maintenance (PM) notifications and work orders to manage work processes to ensure all equipment is maintained per scheduled maintenance plans on time.  </li> <li> Led multidiscipline engineers team to conduct RCM study and discussion with Site discipline & reliability engineers. E2E Implementation knowledge of the AMS MEC process.  </li> <li> Hands-on Experience in Plant Maintenance in the Oil & Gas and Petro-chemical fields as a part of the Refinery / Petrochemical / Oil& Gas Platform maintenance team and familiar with the daily operations, preventive maintenance, corrective maintenance activities  </li> <li> Has exposure to Plant Turnaround / Pitstop / Major maintenance activities as either part of the maintenance team, planning team, and/or engineering support team.  </li> <li> Knowledgable in field inspection processes, corrosion monitoring, and related technical know-how, knowledge of various NDT processes, inspection management plans, task lists, piping integrity, and maintenance of static equipment like pressure vessels, columns, heat exchangers, etc, & knowledge in Risk-Based Inspection (RBI) Methodology is an added value.  </li> <li> Effective communication skills and stakeholder management is a necessary skills for the job.  </li> <li> Experience in leading change in complex environments and a track record in building, influencing, and sustaining natural teams.  </li> <li> Able to demonstrate in-depth technical knowledge on rotating equipment, valves, and reliability engineering.  </li> <li> Solid experience in maintenance project management and facilitating continuous improvement.  </li> <li> Able to deal with a wide range of stakeholders at different seniority levels and experience working with different cultures.  </li> <li> Support the operational units via a virtual environment.  </li> <li> Professional Engineering Certification (API, SIL/SIF, etc.) is preferred.  </li> <li> Knowledge/Proficiency in the SAP PM/MM module is an added value. </li> <li> Knowledge/Proficiency in MS Office, Power BI, Azure DevOps, etc. is highly desirable. </li> </ul>",Senior Process Data Engineer,500000,800000,https://www.naukri.com/job-listings-senior-process-data-engineer-shell-pvt-ltd-chennai-3-to-6-years-091222502851,"['Engineering services', 'Automation', 'Manager Quality Assurance', 'Pumps', 'Project management', 'Instrumentation', 'Refinery', 'Principal', 'Petrochemical', 'Preventive maintenance']",['Chennai'],Data Engineer,2022-12-09 21:09:37
091222502496,"<ul> <li> Primary responsibility in preparing daily, weekly, monthly preventive planning activities in SAP PM Module for Rotary, Static equipment s and General E&I instruments adherence with site requirements. </li> <li> Building Preventive planning that includes detailed description of the tasks that need to be performed for overhauling / maintenance of each machine, parts required, resources, tools required, time required and relationship between tasks. </li> <li> Creating and updating preventive maintenance plans based on input from engineering studies (f.e. Equipment Care Strategies) and feedback from execution: </li> <li> RCM - Living programme Changes related to the Assurance tasks defined by RCM (e.g., add/delete or change interval (of) an assurance task) </li> <li> IPF Changes related to the assurance tasks defined by IPF (e.g., add/delete or change interval (of) an IPF task) </li> <li> Feedback from execution Other changes, not impacting the assurance task, but impacting the task list or workpack contents (e.g., updating the task list with additional information regarding scaffolding). </li> <li> Convert engineering input (i.e., assurance tasks) into maintenance plans, items and task lists which form a complete workpack ready for execution by operations and maintenance </li> <li> Preventive Planner should determine (in consultation with maintenance/operations) what is required in terms of resources, tools, etc.) and build the workpack/preventive plan accordingly </li> <li> The preventive plans/workpacks are multi-disciplinary (E/I, Mechanical, scaffolding etc.); Preventive planner will have basic multi-disciplinary knowledge; for detailed knowledge he/she should work together with colleagues from operations and maintenance. </li> <li> Ensure accordance with local BBS procedures for (preventive) planning, which in its turn are based on the Global AMS-PME RP02) </li> <li> Involvement with preparation and follow-up of preventive maintenance in the production units by: </li> <li> Weekly check and release of preventive maintenance orders </li> <li> Preparing input and (co-) hosting periodic (monthly) preventive maintenance review meeting </li> <li> Pro-actively identify and realize cost reduction / efficiency improvement initiatives within preventive maintenance plans.  </li> <li> Planning of end-to-end work orders to enable efficient scheduling and ensure compliance of executable work orders. </li> <li> Handle bulk changes in our preventive plans by VBA GUI scripting. Currently these requests are being handled through the TAO work request tool but could then potentially be handled by the candidate. </li> <li> All work completed will be as Site HSSE controls and practiced with overall compliance to Shell HSSE Control Framework. The staff conducting the work in TAO will have access to key HSSE documents and tools, will be trained in the relevant areas as it relates to their scope of work and participate in the required Site safety meetings. A safe working environment is to be provided and compliance to the E&C framework is a prerequisite. </li> <li> Investigating and analysis of PM work request to prepare work orders with the detailed tasks to repair the damage with as much as detailed information required to complete the job effectively including procedures, sketches, specifications, or drawings deemed necessary. </li> <li> Understanding the functions of the various equipment and instruments and thorough knowledge of reading/ interpreting the engineering drawings (P&IDs, PEFS, PFDs). </li> <li> Identifying & resolving the bottlenecks for data analysis, maintaining issue logs and use the same in implementing changes/improvements in the Preventive planning process. </li> <li> Shall be able to demonstrate the capabilities of supporting remote assets with their Preventive maintenance planning. Develop work planning through SAP with clear understanding of all required information and documentation as per stakeholder requirements. </li> <li> Clear understanding of maintenance strategies for preventive maintenance and their implementation in GSAP. </li> <li> Develop Preventive planning through SAP with clear understanding of all required information and documentation to execute a job in the field. </li> <li> Understand the concept of work clustering for complex multi-discipline scopes using scheduling tools like Primavera to be able to optimize timeline for a shutdown or campaign. </li> </ul> <p> </p> <p> <b> Dimensions  </b> </p> <ul> <li> No Annual budgets which you directly control but can have influence over department and production area budgets. </li> <li> Exposes the individual to all cultural backgrounds and organizational levels, across diverse time zones. </li> <li> The role will involve working in shifts (Afternoon/Evening/Late Evening Shifts) to have enough interface with the asset/plant to be supported remotely/virtually. </li> <li> Ability to work under pressure and perform multitask simultaneously. </li> <li> May involve travelling to various operating units/sites across the globe as per requirements. </li> </ul> <p> </p> <p> </p> <p> <b> Skills & Requirements  </b> </p> <ul> <li> University Degree in Mechanical Engineering </li> <li> Minimum 5 plus years of overall working experience in Field operations and maintenance in Oil & Gas / Refinery/Petrochemicals Industry/equivalent industry. </li> <li> Should have experience in Field Maintenance, Preventive Planning experience of heavy rotating equipment like turbines, engines, compressors (Centrifugal/ reciprocating compressors), centrifugal pumps etc. </li> <li> Preventive maintenance of static equipment like columns, safety valves, valves, pressure vessels, heat exchangers etc. Also have experience in general Electrical & instruments preventive maintenance planning. </li> <li> Experience in preparing the preventive maintenance planning work instructions, task lists, scaffoldings, Insulation packages of the rotary/static as well as E&I equipment s planning. </li> <li> Hands on Experience in Plant Maintenance Area as an engineering/end user of SAP PM Module is must / Knowledge on Computerized Maintenance Management Systems (CMMS)/Document Management System etc., </li> <li> Strong aptitude for Learner Mindset, Eager to make the things better without supervision. </li> <li> Communication proficiency in English is must, Comprehension in speaking in English language is must. </li> <li> Ability to independently, resourcefully, and creatively research and implement new solutions. </li> <li> A good understanding of the Upstream/DS/IG business and how it works. </li> <li> Ability to engage and effectively communicate at all levels inside and outside Shell and within different cultural settings. </li> <li> Strong interpersonal skills, ability to challenge and ability to build internal and external relationships based on trust and to integrate across multiple functions. </li> <li> Knowledge on VBA GUI Scripting is an added advantage. </li> </ul>",Senior Process Data Engineer,1200000,1600000,https://www.naukri.com/job-listings-senior-process-data-engineer-shell-pvt-ltd-chennai-5-to-10-years-091222502496,"['Data analysis', 'Pressure vessels', 'Cost reduction', 'Valves', 'Centrifugal pumps', 'Scheduling', 'Heat exchangers', 'Refinery', 'Technical support', 'Recruitment']",['Chennai'],Data Engineer,2022-12-09 21:00:52
091222502495,"<div> <ul> <li> <span> <span> <span> <span> <span> <span> Develop the strategy for the domain and roadmaps for the supporting technologies along with the business stakeholders, IT, and suppliers. </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Contributed to improving plant availability and process safety by supporting sites with product communication, sharing best practices, model deployment, monitoring, troubleshooting, and identifying monitoring gaps </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Set-up SmartConnect monitoring platform for critical rotating equipment in collaboration with site engineers; resolved long-standing issue with PI data trending speeds in collaboration with PI CoE. </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Responsible for proactive fault detection, reporting and model - quality control & improvement analysis.  </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Regular engagement with site REQ focal points to communicate long term observations, improvement plans and finetuning proactive limits.  </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Support sites in performing feasibility studies, troubleshooting and identify monitoring gaps.  </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Driving data analytics-based engineering projects to identify anomalies and predict failures. </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Enhanced SmartConnect capabilities by developing new thermodynamic calculation modules, user-friendly interface and troubleshooting problem cases for Centrifugal Compressors, Gas Turbines and Pumps.  </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Built model algorithms and responsible for performing reliability/acceptance checks on models.  </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Communicated new development and trained the teams on new features.  </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Responsible for maintaining monitoring tools and troubleshooting issues flagged by site engineers. </span> </span> </span> </span> </span> </span> </li> </ul> <p> </p> <p> <span> <strong> Experience and qualifications required: </strong> </span> </p> <ul> <li> <span> <span> Bachelors degree in Discipline Engineering, Mechanical Engineering </span> </span> </li> <li> <span> <span> 5+ years of experience in Operations. </span> </span> </li> <li> <span> <span> 2+ years of experience in the PTM tools </span> </span> </li> <li> <span> <span> Consulting experience. </span> </span> </li> <li> <span> <span> Stakeholder management and effective business relationships </span> </span> </li> </ul> </div>",Senior Process Data Engineer,1200000,1600000,https://www.naukri.com/job-listings-senior-process-data-engineer-shell-pvt-ltd-chennai-5-to-10-years-091222502495,"['Pumps', 'Consulting', 'Process safety', 'Manager Quality Control', 'Troubleshooting', 'Continuous improvement', 'Stakeholder management', 'Principal', 'Technical support', 'Recruitment']",['Chennai'],Data Engineer,2022-12-09 21:00:52
091222502494,"<ul> <li> Data analytics and reporting using BI tools like Power BI, Spotfire and develop/ automate new reports/ dashboards. </li> <li> Propose best end to end automation solutions for developing reports and dashboards </li> <li> Work with Global teams and Asset teams to find different dimensions of data and produce the analysis, reports & insights accordingly. </li> <li> Engagement with the Asset stakeholders prepare requirements, timelines, progress update and project closeout </li> <li> Interpret and present data of maintenance, reliability and TA in an easy to understand way and build new insights from data. </li> <li> Apply data analytics to find value added dimensions in maintenance, reliability and TA </li> </ul> <p> <b> <u> <span> Key Challenges </span> </u> </b> </p> <ul> <li> Business knowledge and operational skills on activities at the Upstream, Downstream and IG operating units. Includes specific methods, software and hardware.  </li> <li> Confidence to work in virtual environment and managing engagement across different levels of stakeholders. Includes strong proficiency in using English for both spoken and written communication.  </li> <li> Shift work following operating hours of the upstream, downstream and IG operation units of shell </li> <li> Sound understanding of business/process workflow and having mature mindset to deal tough/challenging situations.  </li> <li> A self-starter leader and reliable deliverer, with very good verbal and written skills in English.  </li> <li> Exposes the individual to all cultural backgrounds and organizational levels across the globe, across diverse time-zones.  </li> </ul> <p> <b> <span> Mandatory Education/Experience:  </span> </b> </p> <ul> <li> University Degree in Mechanical/ Industrial/  <span> Production/Petroleum/ </span> <span> Instrumentation/Electrical/ </span>  Chemical Engineering. </li> <li> Required 5 to 7 yrs. of maintenance work experience in Oil & Gas/ Petrochemicals/ Heavy Industries/Power Plants/Steel Plants/Fertilizer Plants or any equivalent industry. </li> <li> Skill/Proficiency in data analytics tools Spotfire, Power BI , R, Python, SQL, Alteryx and concepts of data analytics are must.  </li> <li> Skill/Proficiency in SAP MM/PM module is must. </li> <li> Skill/Proficiency in AMS processes ME,MEC,MTO and TA are must </li> </ul> <p> </p> <p> <b> <span> Desirables/Added Value: -  </span> </b> </p> <ul> <li> <b> <span> </span> </b> Effective communication skill and stakeholder management is a necessary skill for the job. </li> </ul> <p> <b> <span> Other Skills, Competence and Behavior:  </span> </b> </p> <ul> <li> <b> <span> </span> </b> Developed engagement and communication skills.  </li> <li> Knowledge or higher-level awareness of the context of use of technical data and documentation within technical data processes, </li> <li> Ability to deal effectively with complexity and detail.  </li> <li> Ability to prioritize and ensure delivery of priorities for the area of responsibility.  </li> <li> Flexible and adaptable to change, with track record of demonstrating initiative, analytical capabilities and problem solving. </li> </ul>",Senior Process Data Engineer,700000,900000,https://www.naukri.com/job-listings-senior-process-data-engineer-shell-pvt-ltd-chennai-5-to-7-years-091222502494,"['Business process', 'Automation', 'Analytical', 'Workflow', 'Instrumentation', 'Operations', 'Principal', 'Technical support', 'Recruitment', 'SQL']",['Chennai'],Data Engineer,2022-12-09 21:00:52
091222502493,"<ul> <li> Proactively assess, identify, and plan Marine risks for mobile offshore structures </li> <li> Be a part of specialist support team in actively managing and mitigating the marine risk </li> <li> Be part of a 24 X 7 support system; Understand skills of the specialists / SMEs in the team to reach out for help / support </li> <li> Provide quick support in mitigation and recovery activities in case of any risk materialized </li> <li> Share previous experience and learnings </li> <li> Build a knowledge repository for quick identification of probable solutions for a risk </li> <li> Provide coaching to the new joiners and / or other asset team members </li> <li> Support in DP DEP Implementation </li> </ul> <p> <b> <u> <span> Key Challenges </span> </u> </b> </p> <ul> <li> Business knowledge and operational skills on activities of Marine risk assessments </li> <li> Confidence to work in virtual environment and managing engagement across different levels of stakeholders. Includes strong proficiency in using English for both spoken and written communication.  </li> <li> Shift work following operating hours of the upstream, downstream and IG operation units of shell </li> <li> Sound understanding of business/process workflow and having mature mindset to deal tough/challenging situations.  </li> <li> A self-starter leader and reliable deliverer, with very good verbal and written skills in English.  </li> <li> Exposes the individual to all cultural backgrounds and organizational levels across the globe, across diverse time-zones.  </li> </ul> <p> <b> <span> Mandatory Education/Experience:  </span> </b> </p> <ul> <li> <b> <span> </span> </b> University Degree in Mechanical/ Marine Engineering </li> <li> Required 4 to 7+ yrs. of relevant work experience in Oil & Gas or any equivalent industry </li> </ul>",Senior Process Data Engineer,600000,900000,https://www.naukri.com/job-listings-senior-process-data-engineer-shell-pvt-ltd-chennai-4-to-7-years-091222502493,"['Business process', 'HP data protector', 'Marine engineering', 'Workflow', 'Representative', 'Management', 'Operations', 'Technical support', 'Downstream', 'Recruitment']",['Chennai'],Data Engineer,2022-12-09 21:00:52
091222502492,"<div> <ul> <li> Perform engineering activities for the assigned Production Asset, covering all aspects of base layer control strategies HMI design, compressor control, and alarm management. </li> <li> Carry out Proactive Technical Monitoring activities to identify unit performance improvement opportunities and reliability risks, and work with Area team to determine and implement cost effective solutions. </li> <li> Understand the process safety requirements for the production Area and collaborate with SIS specialists in designing or modifying Safety Instrumented Functions. </li> <li> Author plant changes (MOC) within the Process Control discipline and provide Technical Assurance of plant changes by other disciplines and departments </li> <li> Support or lead failure analysis and causal investigation of Base Layer control failures or incidents. Own, lead and resolve MTO threats with root cause solutions. </li> <li> Network with other sites TAO personnel to maintain a high quality of work and proactively seek best practices and new technology opportunities (ie. Wireless technology, Procedural Automation, MDPro) </li> <li> Collaborate with Instrument and Maintenance specialists in developing and owning the Turnaround worklist for assigned production areas. Provide active support for the implementation of process control scope during turnarounds. </li> </ul> <p> </p> <p> <u> <b> Skills and Requirements </b> </u> </p> <ul> <li> <span> Professional qualifications: Chemical Engineering degree from a recognized institution  </span> </li> <li> <span> 6+ years relevant industrial experience in the areas of design and engineering in the process control discipline; preferably in a refinery/upgrader or large industrial facility. </span> </li> <li> <span> Solid skills in developing base layer control applications on Foxboro I/A DCS, or Honeywell Experion systems. </span> </li> <li> <span> Good knowledge of safe work practices and engineering obligations in an industrial environment including change management (MOC) work processes and procedures. </span> </li> <li> <span> Familiarity with maintenance practices, project scope development, turn-around and long term planning is an asset </span> </li> <li> <span> Ability to analyze, troubleshoot, and resolve process control problems with maintenance and operations.  </span> </li> <li> <span> Utilization of problem-solving techniques to resolve design & engineering issues. </span> </li> <li> <span> Organized, thorough, and professional. </span> </li> <li> <span> Ability to communicate clearly and effectively with peers and clients. </span> </li> <li> <span> Ability to articulate and defend a case for change or improvement. </span> </li> <li> A high level of comfort in using computer-based tools as well as unit simulators. Advanced spreadsheet skill required. Understanding of IT technologies, Data Analysis techniques, databases, and reasonable computer programming skills is a plus. </li> <li> Fully versed with MS Office suite of software (Word, Powerpoint and Outlook) </li> <li> Ability to make decisions based on incomplete and changing information and work under pressure. Strong influencing, communication, negotiation and team working skills, including an ability to deal cross-functionally and with senior management. </li> <li> Strong aptitude for Learner Mindset </li> <li> Flexibility to move quickly across changing priorities and manage multiple Projects/Programmes </li> <li> Ability to independently, resourcefully, and creatively research and implement new solutions </li> <li> A good understanding of the Upstream/DS/IG business and how it works. </li> <li> Ability to engage and effectively communicate at all levels inside and outside Shell and within different cultural settings. </li> <li> Strong interpersonal skills, ability to challenge and ability to build internal and external relationships based on trust and to integrate across multiple functions. </li> <li> Continuous Improvement </li> </ul> </div>",Senior Process Data Engineer,1200000,1600000,https://www.naukri.com/job-listings-senior-process-data-engineer-shell-pvt-ltd-chennai-6-to-11-years-091222502492,"['Wireless', 'Automation', 'Data analysis', 'Process control', 'Failure analysis', 'Process safety', 'Refinery', 'Technical support', 'Monitoring', 'Recruitment']",['Chennai'],Data Engineer,2022-12-09 21:00:52
091222502491,"<ul> <li> Primary responsibility in preparing/developing work plan with comprehensive work packs from Calibration reports, maintenance reports, general task list, equipment specific task lists, spare part list, equipment s drawings, circuit diagrams, single line diagrams and control circuit logics, cause & effects, Equipment data sheets, operation, and maintenance manuals of OEMs etc., for Electrical equipment in field and in substation for operating units/plants as per Shell Standards.  </li> <li> Planning PM & CM work orders in Maximo/SAP and building library work packs that include detailed description of the tasks that need to be performed for time & condition-based maintenance, breakdown maintenance, ordering parts required, service or tools required, estimating time required, Estimating cost of work order, network mapping and quality check points for minimum assurance tasks. </li> <li> Following up for services & materials with venders for mobilization and smooth execution of the activity.  </li> <li> Customize the equipment make and model specific work packs into site specific work pack by adding site conditions and parameters. Accommodate the PM library changes by converting the turnaround documents to the global work packs.  </li> <li> Planning of end-to-end work orders to enable efficient scheduling and ensure compliance of executable work orders. Improving productivity by ensuring the resource, special tool, material, and service requirements availability before the job begins.  </li> <li> Building work packs that include detailed description of the tasks that need to be performed for maintenance of each equipment, parts required, tools required, time required and relationship between tasks including the pre-work preparation steps.  </li> <li> Job hazard assessment, setting out risks anticipated required while performing tasks and creating Permit to work.  </li> <li> Identifying the system condition for each operation activity tasks which would require long term un-availability of equipment & capturing these opportunities in the upcoming pit stop/ shutdown windows or suitable opportunities for maintenance activities.  </li> <li> First level diagnosis of damage history, determining the failure mode based on anomaly reports and information given in corrective maintenance work request/ notification.  </li> <li> Investigating and analysis of CM work request to identify the true cause of the problem and prepare work orders with the detailed tasks to repair the damage with as much as detailed information required to complete the job effectively including procedures, sketches, specifications, or drawings deemed necessary.  </li> <li> Constantly engage with site maintenance leads, original equipment manufacturers, vendors and modify the work packs as recommended/required by the site maintenance personnel.  </li> <li> Will be assigned as trainer for area of development of comprehensive work packs from equipment specific task lists and work instructions. May share the responsibility with supervisor in staff development of 2 to 10 members.  </li> <li> Confidence to work in virtual environment and managing engagement across different levels of stakeholders, including senior management for day-to-day operations. Includes initiative to identify and help resolve business and technical issues within areas of responsibility.  </li> <li> Sound understanding of business/process workflow and having mature mindset to deal and behave under tough/challenging situations.  </li> <li> Strong proficiency in using English for both spoken and written communication, as well as using Skype for Business, MS Teams, Instant Messaging, Video Conferencing, Outlook, etc.  </li> <li> Shift work following operating hours of OU s.  </li> <li> Resilient under pressure and able to work with people different culture and working style.  </li> <li> A self-starter leader and reliable deliverer, with very good verbal and written skills in English, able to negotiate with people, and able to resist undue influence that might otherwise compromise integrity of data quality.  </li> <li> Exposes the individual to all cultural backgrounds and organizational levels  </li> <li> Sound understanding of business/process workflow and having mature mindset to deal and behave under tough/challenging situations.  </li> </ul> <p> </p> <p> </p> <p> <b> Dimensions  </b> </p> <ul> <li> No direct staff, but the candidate is expected to coach/mentor less experienced staff  </li> <li> The role will involve working in shifts (Afternoon/Evening/Late Evening Shifts) to have enough interface with the asset/plant to be supported remotely/virtually.  </li> <li> May involve travelling to various operating units/sites across the globe as per requirements.  </li> </ul> <p> </p> <p> <b> Skills & Requirements  </b> </p> <ul> <li> University Degree in Electrical/ Electrical & Electronics / Electronics & Communication Engineering.  </li> <li> Minimum 5 plus years of overall working experience in in Oil & Gas/ Petrochemicals with experience in One or more of following areas:  </li> <li> Field electrical equipment viz motors, transformers, switchgears, VCB, ACB, Contactors, batteries, battery charger, VFDs, areas at HV/LV voltage levels etc.  </li> <li> Experience in operation and maintaining electrical equipment, procurement, and equipment selection.  </li> <li> Installation & commissioning/operations, design, engineering & selection of instruments for electrical equipment s would be an added advantage.  </li> <li> Experience in preparing the maintenance work instructions, task lists, work packs, work, and manpower estimation for major maintenance/overhauling of electrical equipment available in oil and gas industry.  </li> <li> Experience in materials or resources to be ordered, in advance of the work order being issued, such that the scheduler can immediately bring the work order into the schedule without any intervention, Participated and supported ISO/safety-MS audits, as well as other related external audits, as part of Company Safety Control framework and management system.  </li> <li> Experience in Diagnosing the cause of the problem in field equipment, troubleshooting and determines the scope of activity required to return the equipment back into service by packaging the work to the tasks, disciplines, materials and services to an effective completion, utilizes feedback from past work plan implementation to improve future work plans, Carry out root cause analysis on repeated failures and collaborate with operations and technologist to identify process related degradation/failure mechanism.  </li> <li> Experience in Contribution to site reliability improvements would be an added advantage. Identify, propose, and execute improvement initiatives to eliminate plant reliability threats. Identify bad actors and make improvements plans to minimize or eliminate bad actors. Troubleshoot equipment failures to address and eliminate the root cause. Provide feedback on effectiveness of preventive maintenance, corrective generated from maintenance execution process.  </li> <li> Exposure to Electrical equipment condition monitoring whenever required & liaise with relevant subject matter experts/specialist/OEM to address any arising issues, problems & concerns.  </li> <li> Provide support to other maintenance discipline, provide support, and allocate required resources for Turnaround execution in related Process Units e.g., scaffold erecting, lifting, excavation, etc.  </li> <li> Experience in leading team/process/projects is preferred  </li> <li> Skills in MS Office tools like Excel are highly desirable  </li> <li> Hands on Experience in Plant Maintenance Area as an engineering/end user of SAP PM Module/ Computerized Maintenance Management Systems (CMMS)/Document Management System etc.  </li> <li> A good understanding of the Upstream/DS/IG business and how it works.  </li> <li> Effective communication skills and stakeholder management are necessary for the job.  </li> <li> Knowledge of Lean CI methodology is an added value.  </li> <li> Able to multi-task, prioritize and ensure delivery of priorities as promised, work without close supervision, and work through others to deliver results.  </li> <li> Virtual working experience is highly desirable.  </li> <li> Professional Engineering certification is an added value. </li> </ul>",Senior Process Data Engineer,700000,1200000,https://www.naukri.com/job-listings-senior-process-data-engineer-shell-pvt-ltd-chennai-5-to-10-years-091222502491,"['Procurement', 'Substation', 'ISO', 'Packaging', 'Scheduling', 'Windows', 'Risk management', 'Troubleshooting', 'Technical support', 'Recruitment']",['Chennai'],Data Engineer,2022-12-09 21:00:52
091222502490,"<ul> <li> <span> The Projects Portfolio Cost Estimator scope involves the translation of Project scope into the estimate of capital and operational expenditure cost during the project phases (Identify, Assess, Select, Define and Execute).  </span> </li> <li> <span> The estimates completed by the cost estimator not only cover the costs associated with the engineering, procurement, construction, implementation commissioning and start-up but also address the translation of the project risk profile into cost contingency via deterministic and probabilistic methods, the potential effect of market movements over the project timescale, and the complete suite of Owners Costs that Shell will experience  </span> </li> <li> <span> Develop conceptual (Type 1) project cost estimates to help the business determine which investment ideas warrant further definition. Assess prospect scope and main components for each concept under consideration.  </span> </li> <li> <span> Develop (Type 2) cost estimates to assist the project team in determining which selected concept is economically viable.  </span> </li> <li> <span> Develop (Type 3) cost estimates to establish the project budget upon approved funding authorization.  </span> </li> <li> <span> Translate the identified project scope and engineering deliverables into the approved work breakdown structure for design, procurement, construction, installation, commissioning and start-up of the hardware elements.  </span> </li> <li> <span> Estimate composed of detailed MTOs matched with detailed pricing and construction unit hours. Identify areas of uncertainty and apply for appropriate allowances.  </span> </li> <li> <span> Provide assurance and challenge, as needed, on detailed project cost estimates (Type 4). Incorporate project actual costs to date and detailed MTOs from IFC (Issue For Construction) design packages matched with detailed pricing and construction unit hours to establish remaining project costs.  </span> </li> <li> <span> Benchmarking of Estimates- Develop benchmarking packages at an appropriate level to check estimate results from top-down versus both internal and external comparable data. Work with IPA forms and reports to understand the comparison of Shell projects to industry datasets.  </span> </li> <li> <span> Connect with site field leads to understanding project scope and execution.  </span> </li> <li> <span> Ensure all project cost estimates, submitted by engineering and construction contractors, comply with Shell Project Services Standards, Guides and Procedures.  </span> </li> <li> <span> Perform uncertainty analysis and sensitivities on cost estimates, reflecting analysis of project risks. Utilize appropriate deterministic or probabilistic risk assessment tools to develop the appropriate cost contingencies  </span> </li> <li> <span> Participate in and comply with all safety programs.  </span> </li> <li> <span> Assist with the continuous improvement and implementation of standardized work processes for effective cost estimating.  </span> </li> <li> <span> Stay current with mandatory training  </span> </li> </ul> <p> <span> <span> <strong> Education/Experience:  </strong> </span> </span> </p> <ul> <li> Engineering Degree and/or Certification as a Cost Estimator or equivalent experience.  </li> <li> Minimum 5-10 years of experience in cost estimating positions in the Oil & Gas industry (operating refinery or petrochemicals plants preferred) on small capital projects portfolios.  </li> <li> Fully versed with Cost Estimating Tools and Methodologies.  </li> <li> Well versed with ASPEN TECH estimating tool  </li> <li> Experience in cost risk analysis, preferably with both deterministic and probabilistic methods.  </li> <li> Experience in sourcing and using benchmark data  </li> <li> Excellent analytical, oral and written communication skills  </li> <li> Team player and self-motivated.  </li> <li> Advance skill level in MS Excel.  </li> <li> Fully versed with MS Office suite of software (Word, PowerPoint and Outlook)  </li> </ul>",Senior Process Data Engineer,700000,1200000,https://www.naukri.com/job-listings-senior-process-data-engineer-shell-pvt-ltd-chennai-5-to-10-years-091222502490,"['Procurement', 'Assurance', 'Cost Estimator', 'Analytical', 'Risk assessment', 'Refinery', 'Continuous improvement', 'Risk management', 'Aspen', 'Recruitment']",['Chennai'],Data Engineer,2022-12-09 21:00:52
091222502489,"<div> <ul> <li> <span> Support the delivery of standard, consistent and robust cost management frameworks, and procedures for medium to mega-complexity turnaround events at Scotford.  </span> </li> <li> <span> Assist with the roll-out of the company standard tools and processes and apply them to turnaround events.  </span> </li> <li> <span> Support the implementation of turnaround event Cost Breakdown Structures (CBS) in alignment with company and Downstream recommended practices, the estimate breakdown and Primavera (as applicable). Support the set up in SAP Blueprint (Upgrader) and GSAP (Refinery/Chemicals) with CBS and budget information accordingly.  </span> </li> <li> <span> Specific activities may include work order creation and closure, Purchase Order (PO) status report generation (from GSAP, Blueprint (BP) and TRACK), BP/GSAP cost reporting generation, productivity reporting, support of Forecaster tool for cost forecasting, and the review, follow up and approval of Craft Team Member (CTM) time-writing.  </span> </li> <li> <span> Support implementation of Cost Control processes consistent with Shell procedures for: commitment capture, VOWD (Value of Work Done), cost forecasting/phasing and earned value management.  </span> </li> <li> <span> Establish cost report content and frequency, prepare, and conduct budget holder reviews, and publish reporting.  </span> </li> <li> <span> Support Contracting & Procurement group by providing detailed cost and performance analysis.  </span> </li> <li> <span> Monitor the work hour consumption and associated cost of contractor activities against contractual targets such that potential deviations are identified, and the appropriate corrective actions are taken.  </span> </li> <li> <span> Connect with Field Leads, as needed, to validate physical progress and productivity. Act as a focal point for the contractor offboarding plans and ensure changes are captured via the change management process.  </span> </li> <li> <span> Deliver accurate and high-quality monthly Latest Estimate forecasts for turnaround events.  </span> </li> <li> <span> Maintain and promote realism in the cost forecasts in the face of commercial pressure and optimism from turnaround teams.  </span> </li> <li> <span> Assist in the maintenance of change/trend management and contingency management: ensure that appropriate changes/trends are reflected in the regular cost forecast updates. Prepare forecasts and review with budget holders. Establish linkage between change/trend management and contingency management, perform the contingency drawdown and ensure accurate current status is reflected in reports.  </span> </li> <li> <span> Cost reporting: generation of accurate cost reports, including variances from the plan on commitment, VOWD, management of changes and against approved budget. Support preparation of cost reporting in Power Bi dashboard.  </span> </li> <li> <span> Maintain consistency in cost basis through rigorous adherence to the controls processes.  </span> </li> <li> <span> Participate and comply with all safety programs.  </span> </li> <li> <span> Assist with the continuous improvement and implementation of standardized work processes for effective cost control.  </span> </li> <li> <span> Stay current with mandatory training.  </span> </li> </ul> <p> </p> <p> <b> <span> Dimensions  </span> </b> </p> <ul> <li> <span> Working with multiple stakeholders, tools and workflows  </span> </li> <li> <span> Analyzing and providing most reliable data on time for various review process  </span> </li> <li> <span> Demystify the process complexity and identify simplification opportunities  </span> </li> </ul> <p> </p> <p> <b> <span> Skills& Requirements  </span> </b> </p> <ul> <li> <span> Engineering/Technologist degree, Finance professional and/or certification as a Cost Engineer or equivalent experience.  </span> </li> <li> <span> Minimum 4 years experience in the energy sector.  </span> </li> <li> <span> Experience in Cost Engineering/ Cost Controls/ Finance positions for portfolio capital projects or turnaround events.  </span> </li> <li> <span> Preferred experience in refinery or petrochemicals plant turnarounds.  </span> </li> <li> <span> Works effectively in a team environment and builds consensus.  </span> </li> <li> <span> Working knowledge of a broad range of project controls systems, tools, and practices specifically in the energy industry.  </span> </li> <li> <span> Strong leadership, communication, and presentation skills.  </span> </li> <li> <span> Excellent customer focus or stakeholder engagement.  </span> </li> <li> <span> High competency level required in Excel, SAP Blueprint/ GSAP.  </span> </li> <li> <span> Good knowledge of MS suite of tools PowerPoint, Word, Outlook, MS Teams.  </span> </li> <li> <span> Excellent analytical, oral and written communication skills.  </span> </li> <li> <span> Preferred experience in following tools: PowerBI, cost management software, data bases (Access).  </span> </li> </ul> </div>",Senior Process Data Engineer,600000,1100000,https://www.naukri.com/job-listings-senior-process-data-engineer-shell-pvt-ltd-chennai-4-to-9-years-091222502489,"['Procurement', 'Change management', 'Assurance', 'SAP', 'Analytical', 'Asset management', 'Refinery', 'Risk management', 'Forecasting', 'Recruitment']",['Chennai'],Data Engineer,2022-12-09 21:00:52
091222502487,"<div> <ul> <li> Develop comprehensive Plant Maintenance Schedules in SAP and Primavera P6 with look ahead and catch-up schedules (as applicable) with optimization and allocation of the resources (man, materials & machines/tools).  </li> <li> Assure the Equipment Maintenance Activities are planned, scheduled, and executed within Latest Allowable Finish dates, Mange Maintenance overdues and backlogs with approved mitigation measures.  </li> <li> Integration of different schedules (vendor schedule, mobilization schedule, material availability schedules) to make a master schedule and periodic update of the same in Primavera P6.  </li> <li> Assign Schedule baselines, update the activity progress, prepare, and analyze the KPIs.  </li> <li> Enable the assigned operating units, and its on-site employees in producing and compiling high value maintenance information and data, as measured by improvement trends in data quality.  </li> <li> Maintenance, assurance, technical support and standardization of technical data and documents of the operating units. Includes site engagement & leadership on technical & tools forums.  </li> <li> Analyze technical data and documents and processes by working with engineering, maintenance, operations, and project teams & partners of respective OUs.  </li> <li> Understanding the functions of the various equipment and instruments and thorough knowledge of reading/ interpreting the engineering drawings (P&IDs, PEFS, PFDs).  </li> <li> Keeping track of improvement initiatives and ensuring that it progressed & asset & TAO is getting benefitted through those improvement activities.  </li> </ul> <p> </p> <p> <b> Dimensions  </b> </p> <ul> <li> No Annual budgets which you directly control but can have influence over department and production area budgets.  </li> <li> No direct staff, but the candidate is expected to coach/mentor less experienced staff  </li> </ul> <p> </p> <p> </p> <p> <b> Skills & Requirements  </b> </p> <ul> <li> University Degree (B.E./B. Tech.) in Mechanical / Industrial/ Production/ Electrical/ Instrumentation Engineering.  </li> <li> 5 plus years of Industrial/field experience in Operations, Maintenance, and Scheduling exposure in Oil & Gas/ Refinery / Petrochemicals industry in the following fields: -  </li> <li> Experience working with multi-discipline & multi-cultural teams in a virtual environment  </li> <li> Strong communication and coordination skill is desirable.  </li> <li> Experience with Routine and Major Maintenance/ Campaign Scheduling is essential  </li> <li> Resource allocation and leveling, Baselining, Progress updates, Reporting, Manpower planning and allocation. Understanding the safety requirement and Risk Assessment.  </li> <li> Knowledge/Proficiency in SAP PM module or MAXIMO is highly desirable.  </li> <li> Knowledge/Proficiency in Primavera P6 is highly desirable.  </li> <li> Knowledge/Proficiency in MS Word, Excel, MS Access etc., is highly desirable.  </li> <li> Knowledge/Proficiency in Microsoft Tools such as Word, Excel, PowerPoint, Access is highly desirable.  </li> <li> Effective communication skill and stakeholder management is a necessary skill for the job.  </li> <li> Virtual working experience highly desirable.  </li> <li> Strong aptitude for Learner Mindset  </li> <li> Flexibility to move quickly across changing priorities and manage multiple Projects/Programmes  </li> <li> Ability to independently, resourcefully, and creatively research and implement new solutions  </li> <li> A good understanding of the Upstream/DS/IG business and how it works.  </li> <li> Ability to engage and effectively communicate at all levels inside and outside Shell and within different cultural settings.  </li> <li> Strong interpersonal skills, ability to challenge and ability to build internal and external relationships based on trust and to integrate across multiple functions.  </li> <li> Continuous Improvement </li> </ul> </div>",Senior Process Data Engineer,700000,1200000,https://www.naukri.com/job-listings-senior-process-data-engineer-shell-pvt-ltd-chennai-5-to-10-years-091222502487,"['Assurance', 'Risk assessment', 'Scheduling', 'Refinery', 'Asset management', 'Continuous improvement', 'Risk management', 'Technical support', 'Downstream', 'Recruitment']",['Chennai'],Data Engineer,2022-12-09 21:00:52
091222502488,"<div> <ul> <li> Maintain, improve, develop and design MCTA metrics, dashboards, and reports to support business needs  </li> </ul> <div> <div> <ul> <li> Recognize and implement opportunities to streamline and improve internal and external reporting.  </li> <li> Conduct validation & Quality Control of data, metrics, and reports  </li> <li> Conduct performance and variance analysis: quantitatively compare measured results to expected values and / or targets  </li> <li> Understand MCTA work processes and their impact on data and metrics.  </li> <li> Utilize data and reports to identify trends and improvement opportunities within MCTA  </li> <li> Provide consistent and transparent data, analysis, conclusions, insights and recommendations to stakeholders to support forecasting, decision making, and improvements  </li> <li> Directly engage business leaders and stakeholders to discuss findings and recommendations and to request support and action.  </li> <li> Influence stakeholders to consider and address recommendations by providing insights on impact or benefit  </li> <li> Interface regularly with MCTA Assurance Analyst and Work Process Focal and Continuous Improvement colleagues to: share knowledge, information and updates; collaborate, investigate, and understand reasons for results and trends; and provide insights and recommendations for potential work process gaps or opportunities based on data assessment and findings.  </li> <li> Develop/maintain job aids and ways of working documents for Analysts role.  </li> <li> Provide one-on-one and group coaching to stakeholders on reports and metrics.  </li> <li> Work directly and collaboratively with SBO Chennai colleagues to enable an effective single Scotford support team  </li> <li> Train and onboard new team members as required  </li> </ul> <p> <strong> Special Challenges: </strong> </p> <ul> <li> Adapting local metrics, reports and data analysis tools and methods to meet Global AMS requirements  </li> <li> Analyzing large volumes of data and comparing against measurable targets  </li> <li> Independently finding ways to supplement analysis findings with additional or alternate information to help stakeholders better understand root cause and / or significance of findings.  </li> <li> Interfacing with multiple  <span> stakeholders/Leadership  </span> </li> <li> Translating business requests for information into metrics or reports that can be generated from existing data and tools.  </li> <li> Functioning and succeeding within a team that works virtually from multiple locations.  </li> <li> Understanding data from multiple sources without all the underlying information. Analysis of ambiguous information  </li> </ul> <p> <strong> Skills and Requirements </strong> </p> <ul> <li> Strong Skills in compiling and analyzing data as well as recognizing trends and patterns relating to maintenance workflow and processes.  </li> <li> Knowledge and understanding of AMS Perform Maintenance Execution (PME) and Perform Turnarounds (PTA) work processes  </li> <li> Proficient with and skilled in business computing applications used to capture and assess data including, but not limited to:  <span> SAP (GSAP and Blueprint platforms  </span> ),  <span> Power BI  </span> ., and Microsoft Office (particularly Excel).  </li> <li> Knowledge of business computing applications that will be used directly or that impact those primarily used to capture and assess data, including but not limited to: Primavera (with respect to how it interfaces with and affects Blueprint and SAP), Spotfire reports, and SharePoint.  </li> <li> <span> Strong  <span> communication/interpersonal  </span> skills  </span> </li> <li> Can manage evolving priorities and deliver on multiple commitments in a timely manner.  </li> <li> Self-motivated needing minimal direction.  </li> <li> Comfortable challenging leaders and colleagues respectfully  </li> <li> Strong  <span> communication/interpersonal  </span> skills  </li> <li> Proven analytical background  </li> <li> Advanced Excel skills </li> </ul> </div> </div> </div>",Senior Process Data Engineer,500000,800000,https://www.naukri.com/job-listings-senior-process-data-engineer-shell-pvt-ltd-chennai-3-to-6-years-091222502488,"['Analyst', 'Data analysis', 'SAP', 'Analytical', 'Workflow', 'Manager Quality Control', 'Forecasting', 'Variance analysis', 'Technical support', 'Recruitment']",['Chennai'],Data Engineer,2022-12-09 21:00:52
051222500400,"<span> <ul> <li> <span> Should have 4 years experience in data engineering. </span> </li> <li> <span> Should have 2 years relevant experience in AWS Stack. </span> </li> <li> <span> Should have 3 years relevant experience in Python and python libraries. </span> </li> <li> <span> Should have good experience in data pipeline. </span> </li> <li> <span> Team player, Motivated, able to grasp things quickly with analytical and problem-solving skills. </span> </li> </ul> <p> <span> <b> <u> <span> Good to have: </span> </u> </b> </span> </p> <ul> <li> <span> Experience in Redshift, Postgres. </span> </li> <li> <span> Experience in PySpark. </span> </li> <li> <span> Experience in database (e.g., SQL, MYSQL etc.) </span> <br /> </li> </ul> </span> <div> <br /> </div>",Sr/Lead Data Engineer,1000000,1400000,https://www.naukri.com/job-listings-sr-lead-data-engineer-wavelabs-technologies-private-limited-gurgaon-gurugram-3-to-10-years-051222500400,"['Managed services', 'Analytical', 'MySQL', 'Machine learning', 'Agile', 'Video conferencing', 'Life sciences', 'MSP', 'SQL', 'Python']",['Gurgaon'],Data Engineer,2022-12-05 17:59:04
101222005982,"<p> </p><p><strong>Salary: 15 to 30 LPA</strong></p><p><strong>Exp: 3 to 8 years</strong></p><p><strong>Location : Bangalore/Gurgaon</strong></p><p><strong>Notice: Immediate to 30 days..!!</strong></p><br /><p><strong>Candidate Profile:</strong></p><ul><li>Required skills: AWS/AZURE, ETL, SQL, SPARK, PYTHON</li></ul><p> </p><ul><li>Basic and Advance SQL </li><li>ETL tool  Azure data factory, Informatica/Alteryx</li><li>Excellent communication skills and stakeholder management</li><li>Ability to work independently in IC role</li><li>Good understanding of Azure SQL Database</li></ul><br /><p>experience in analytics, Oracle, ETL, Python and associated data engineering jobs.</p><ul><li>Must have experience with managing and transforming big data sets using pyspark, spark-scala</li><li>Experience with AWS services  Glue, Lambda, EMR, Redshift, RDS</li></ul><br /><br />",Data Engineer Azure/AWS -- US MNC (analytics),1500000,3000000,https://www.naukri.com/job-listings-data-engineer-azure-aws-us-mnc-analytics-aspyra-hr-services-gurgaon-gurugram-bangalore-bengaluru-3-to-7-years-101222005982,"['hive', 'Aws Cloudformation', 'Big Data', 'Azure Databricks', 'pyspark', 'Informatica', 'sql', 'Azure Data Factory', 'HADOOP', 'Spark', 'AWS', 'Python']","['Bengaluru', 'Gurgaon']",Data Engineer,2022-12-10 17:08:58
091222009794,"<p><strong>Dear Candidates, </strong></p><br /><br /><br /><p><strong>WARM GREETINGS FROM CONVATE CONSULTANCY</strong></p><br /><br /><br /><p>We do have an urgent position opening for <strong>AWS Data Engineer</strong> for our renowned and corporate organization in <strong>Bangalore & Pune.</strong></p><p> <br /> </p><p><strong>*JOB DETAILS: *</strong></p><ul><li><strong>Location: Bangalore / Pune</strong></li><li><strong>Position: AWS Data Engineer</strong> </li><li><strong>Experience: 10 to 14 Years.</strong></li><li><strong>Educational Qualification: BE/B. Tech</strong></li><li><strong>Key Skill:</strong> <strong>AWS, Spark, Big Data Technologies, Git</strong></li><li><strong>Note- Only Female candidates</strong></li><li><strong>Notice Period: Immediate</strong></li></ul><br /><br /><p><strong>About the Company*</strong></p><p>The company in India are established under the laws of India and are owned and managed (as the case may be) by established <strong>Indian</strong> professionals. Established in August 1993, the companys entities have rapidly built a significant competitive presence in the country. Today we operate from offices across 14 cities including in Ahmedabad, Bengaluru, Chandigarh, Chennai, Gurugram, Hyderabad, Jaipur, Kochi, Kolkata, Mumbai, Noida, Pune, Vadodara and Vijayawada.</p><p>If you are interested, please share updated resume or feel free to contact us. </p><p> <br /> </p><p> <br /> </p><p><strong>Thanks and Best Regards, </strong><br /><strong>Barsha Parija</strong></p><p><strong>Email: barsha@convate.com</strong></p><p><strong>Convate Consultancy Services Pvt Ltd</strong></p>",Urgent Openings For AWS Data Engineer @Bangalore/Pune,1700000,2500000,https://www.naukri.com/job-listings-urgent-openings-for-aws-data-engineer-bangalore-pune-convate-consultancy-services-pune-bangalore-bengaluru-10-to-14-years-091222009794,"['Git', 'Big Data Technologies']","['Pune', 'Bengaluru']",Data Engineer,2022-12-09 18:19:42
091222010743,"<p><strong>External Description</strong><br /><strong>Location- Bangalore (Return to Office)</strong><br /> <br /><strong>Description - External</strong><br /><strong>With a startup spirit</strong> <strong>and</strong> <strong>90,000+ curious and courageous minds, we have the expertise to</strong> <strong>go deep with the worlds biggest brandsand we have fun doing it. Now, we’re calling all you rule-breakers and risk-takers who see the world differently and are bold enough to reinvent it.</strong><br /> <br /><strong>Transformation happens here. Come, be a part of our exciting journey!</strong><br /><strong>Are you the one we are looking for?</strong><br /><strong>Inviting applications for the role of Data</strong> <strong>Engineer!</strong><br /><strong>Responsibilities</strong><br /></p><ul><li><strong>Work with other Azure stack modules like Azure Data factory and Lakes, SQL DW, etc.</strong></li><li><strong>Compute management, AZ pipelines with Batch scheduling</strong></li><li><strong>Build simple to complex pipelines & Dataflows</strong></li><li><strong>Automate data access with logic apps</strong></li></ul><p> <br /><strong>Qualifications we seek in you!</strong><br />Minimum qualifications<br /></p><ul><li><strong>Should have a clear understanding of one or more of the below technologies –</strong></li><li><strong>Database: Oracle, MS SQL Server, Teradata, MySQL, PostgreSQL etc.</strong></li><li><strong>Knowledge of ETL scripts, SSIS packages, SSRS Reporting, and MSSQL Profiling tools</strong></li><li><strong>Exposure and hands-on experience in Azuresuch as Data lake, No SQL Databases, Data factory and Databricks(Apache spark), synapse analytics, HDInsight or Azure analytics</strong></li><li><strong>Exposure to Azure Dev ops and Github</strong></li><li><strong>Good knowledge of other cloud platforms like GCP and AWS</strong></li><li><strong>Knowledge of DevOps</strong></li><li><strong>Big Data – Spark SQL, Scala, pySpark, Red Shift, Hive, HDFS, Cloudera Hadoop</strong></li><li><strong>Data Flow tools – Alteryx, SSIS</strong></li><li><strong>Relevant experience in IT industry</strong></li><li><strong>Sound knowledge of framework and lifecycle of Technology Projects</strong></li><li><strong>A good understanding of Business Intelligence and Data warehousing end to end architecture</strong></li><li><strong>Solid understanding of object-oriented programming (OOP) and computer science foundations, such as memory management and low-level algorithm performance.</strong></li><li><strong>Strong knowledge on RDBMS concepts</strong></li><li><strong>Professional experience in Database development, Data modeling and ETL Design</strong></li><li><strong>Ability to design and optimize SQL queries and stored procedures.</strong></li><li><strong>Ability to interact with end-users and translate business language into technical requirements</strong></li><li><strong>Excellent written and oral communication skills and ability to express complex technical concepts effectively, both verbally and in writing</strong></li><li><strong>Excellent customer service skills</strong></li><li><strong>Ability to work in a team-based environment, as well as, the ability to work independently.</strong></li><li><strong>Ability to work effectively with people of many different disciplines with varying degrees of technical experience</strong></li><li><strong>Personal drive and positive work ethic to deliver results within tight deadlines and in demanding situations</strong> </li><li><strong>Flexibility to adapt to a variety of engagement types, working hours and work environments and locations</strong></li><li><strong>Superb communication and negotiation skills</strong></li></ul><p> <br />Preferred qualifications<br /></p><ul><li>    <strong>Relevant experience as a Data Analyst/Oracle/MSBI Developer with extensive experience in design, prototyping and developing robust applications for a wide variety of consumer and enterprise products</strong></li></ul><p> <br /><strong>Genpact is an Equal Opportunity Employer and considers applicants for all positions without regard to race, color, religion or belief, sex, age, national origin, citizenship status, marital status, military/veteran status, genetic information, sexual orientation, gender identity, physical or mental disability or any other characteristic protected by applicable laws.</strong> <strong>Genpact is committed to creating a dynamic work environment that values diversity and inclusion, respect and integrity, customer focus, and innovation. For more information, visit</strong> <strong><u>www.genpact.com</u>. Follow us on</strong> <strong><u>Twitter</u>,</strong> <strong><u>Facebook</u>,</strong> <strong><u>LinkedIn</u>, and</strong> <strong><u>YouTube</u>.</strong><br /> </p>",Genpact hiring For Azure Data Engineer,900000,1900000,https://www.naukri.com/job-listings-genpact-hiring-for-azure-data-engineer-genpact-bangalore-bengaluru-4-to-9-years-091222010743,"['Azure Databricks', 'SSIS', 'SQL']",['Bengaluru'],Data Engineer,2022-12-09 19:00:28
071222010562,"<p> </p><p>Lululemon is looking to hire a dynamic Data Engineer for Flow project to work closely with internal technical teams as well as different facets of the lululemon MPA division. This individual will provide on-going analytical and ETL supports to meet the project needs.<br /></p><p><strong>Experience: </strong>8-12 years of experience with Oracle/POstgreSQL (Required)<br /><strong>Education: </strong>Min. Bachelor's in Computer Science or Engineering, Information Systems, or related fields</p><p><strong>Responsibilities</strong></p><ul><li>Uses structured tools for analysis and presentation of concepts and models to enhance the BRD</li><li>Develop, maintain and deliver training materials to the supply chain end-users</li><li>Work collaboratively with external consultants, internal & external resources throughout the project lifecycle to ensure system modifications meet business needs</li><li>Support day to day reporting needs where required</li><li>Support production issues as relate to application functionality and integrations</li></ul><p><strong>Qualifications:</strong></p><ul><li>Excellent spoken and written communication skills (verbal and non-verbal)</li><li>Proven experience in managing data warehouses and ETL pipelines (Min. 2 years)</li><li>Solid scripting capability for analysis and reporting (Strong PL/SQL and expert SQL) coming with a performance mindset as well as functionally precise</li><li>Expert SQL development skills with ability to write complex efficient queries for data integration</li><li>Strong analytical skills to support BAs and ability to translate user stories / work closely with the tech team</li><li>Strong problem-solving skills (Math skills required for data modeling)</li><li>Ability to work as a back-end developer (PL/SQL)</li><li>Ability to manage and complete multiple tasks within tight deadlines</li><li>Possess expert level understanding of software development practices and project life cycles.</li><li>Working experience with Java</li><li>Working Experience with cloud-native technologies</li><li>Must have: Working experience in dealing with big data and data manipulation.</li><li>Highly Desired: Familiarity with Oracle retail data structures (Retail Management System / Retail Planning Application System)</li><li>Desired: Familiarity with DevOps practices like CICD pipeline</li><li>Desired: Retail experience is a plus. (fashion retail experience would be ideal)</li><li>Desired: Working experience with cloud platforms namely AWS</li></ul><p><strong>Culture Add</strong></p><ul><li>Good awareness of lululemon culture/core values</li><li>Leads with courage, knowing the possibility of greatness is bigger than the fear of failure</li><li>Communicates with honesty and kindness, and creates the space for others to do the same</li><li>Possesses an entrepreneurial spirit and continuously innovates to achieve great results</li><li>Fosters connection by putting people first and building trusting relationships</li><li>Integrates fun and joy as a way of being and working, aka doesnt take themselves too seriously</li></ul>",Lead Data Engineer - ETL | Lululemon,3500000,6000000,https://www.naukri.com/job-listings-lead-data-engineer-etl-lululemon-talent500-bangalore-bengaluru-11-to-20-years-071222010562,['Data Warehousing'],['Bengaluru'],Data Engineer,2022-12-07 16:12:21
021222501554,"<div> <li> <p> The Big Data Analytics Engineer will help develop a new platform and pipelines for a telecom services provider for massive data from network users, using the Hadoop ecosystem, and various other technologies. </p> </li> <ul> <li> Analytics Platform creation on on-premise system using Flink pipelines </li> </ul> <ul> <li> Establish DevOps procedures for ETL, </li> </ul> <ul> <li> Develop proof of concepts of new technologies and evaluate the state of the art in large-scale data processing and distributed computing </li> </ul> <ul> <li> Support Data Analysts with Data Engineering tasks to build an ML use cases by providing data in SQL/NoSql databases </li> </ul> <ul> <li> Demonstrate excellent oral and written communication skills to explain and document ideas </li> </ul> <p> Skills: </p> <p> <b> Must Have </b> : Java, SQL, ETL, bigdata framework, python or any programming languages, spark , NoSQL databases </p> <p> <b> Good to Have </b> : kafka or any streaming processing frameworks , apache flink , mongodb , kubernetes , serverless knowledge, devops </p> </div>",Lead Data Engineer,900000,1400000,https://www.naukri.com/job-listings-lead-data-engineer-brillio-technologies-pvt-ltd-bangalore-bengaluru-6-to-10-years-021222501554,"['Telecom', 'Front office', 'Product engineering', 'NoSQL', 'Data processing', 'MongoDB', 'Apache', 'Analytics', 'SQL', 'Python']",['Bengaluru'],Data Engineer,2022-12-02 18:02:34
081222503364,<ul> <li> Look at data and undertake reconciliation  </li> <li> Identify areas where the backlog is increasing and go after addressing those  </li> <li> Work with Tech Leads from Fusion WMI SOA to address all backlogs  </li> <li> Keep Order / invoice / Inventory process moving  </li> <li> Deep analytic thought process  </li> <li> Read the data  </li> <li> Draw interpretation cross system  </li> </ul> <p> </p> <p> <strong> Required Skills  </strong> </p> <ul> <li> Data Engineering  </li> <li> Advanced SQL & Programming  </li> <li> Experience with Big Data  </li> </ul> <p> <strong> Nice to Have  </strong> </p> <p> <strong> </strong> Working experience on ERP (Oracle SOA & Fusion)  </p> <div> <div> <div> <strong> Key Skills :  </strong> <ul> <li> Big Data  </li> <li> Advance Sql  </li> </ul> </div> </div> </div> <div> <div> <div> <div> </div> </div> </div> </div>,Lead Data Engineer - Mongo DB,800000,1200000,https://www.naukri.com/job-listings-lead-data-engineer-mongo-db-mm-staffing-career-consultants-private-limited-bangalore-bengaluru-5-to-8-years-081222503364,"['IT services', 'Automation', 'WMI', 'Reconciliation', 'Agile', 'Oracle SOA', 'Customer experience', 'big data', 'Operations', 'SQL']",['Bengaluru'],Data Engineer,2022-12-08 21:55:57
071222500054,"<p> </p><ul> <li> As a Lead Data Engineer in the Healthcare Digital & Data - Data Governance & Architecture team you will work hands-on to deliver and maintain the pipelines required by the Healthcare business functions to derive value from their data </li> <li> For this, you will bring data from a varied landscape of source systems into our cloud-based analytics stack and implement necessary cleaning and pre-processing steps in close collaboration with our business customers </li> <li> Furthermore, you will work closely together with our Data Governance and Quality & Compliance teams to ensure that all data assets are governed according to the FAIR principles </li> <li> To keep the engineering team scalable, you and your peers will create reusable components, libraries, and infrastructure that will be used to accelerate the pace with which future use-cases can be delivered </li> </ul> <p> </p> <p> <strong> Who you are </strong> </p> <ul> <li> M.Sc./PhD in Computer Science or related field and 9+ years of work experience in a relevant capacity <br /> </li> <li> Agile mindset, a spirit of initiative, and desire to work hands-on together with your team <br /> </li> <li> Interest in solving challenging technical problems and developing the future data architecture that will enable the implementation of innovative data analytics use-cases <br /> </li> <li> Experience in leading small to medium-sized teams <br /> </li> <li> Experience in creating architectures for ETL processes for batch as well as streaming ingestion <br /> </li> <li> Knowledge of designing and validating software stacks for GxP relevant contexts as well as working with PII data <br /> </li> <li> Familiarity with the data domains covering the Pharma value-chain (e.g. research, clinical, regulatory, manufacturing, supply chain, and commercial) <br /> </li> <li> Strong, hands-on experience in working with Python & R codebases, proficiency in additional programming languages (e.g. C/C++, Rust, Typescript, Java, ?) is expected <br /> </li> <li> Knowledge of database technologies for OLTP and OLAP workloads and a firm grasp of SQL <br /> </li> <li> Experience working with Apache Spark and the Hadoop ecosystem <br /> </li> <li> Working with heterogenous compute environments and multi-platform setups <br /> </li> <li> Experience in working with cloud environments such as AWS, GCP, and Azure <br /> </li> <li> Basic knowledge of Statistics and Machine Learning algorithms is favorable. </li> </ul>",Lead Data Engineer - HC,900000,1200000,https://www.naukri.com/job-listings-lead-data-engineer-hc-milliporesigma-bangalore-bengaluru-4-to-8-years-071222500054,"['Supply chain', 'Computer science', 'C++', 'Pharma', 'Machine learning', 'Agile', 'Healthcare', 'OLAP', 'SQL', 'Python']",['Bengaluru'],Data Engineer,2022-12-07 12:54:42
081222904435,"<p>     Design   & Build reliable, scalable, CICD driven streaming and batch data   engineering pipelines.<br /> </p><p>    Work in collaboration with Data scientists, ML engineers, Stakeholders to   build a platform for enabling data-driven decisions.<br /> </p><p>    Oversee and govern the expansion of the current data architecture and the   optimization of query and data warehouse.<br /> </p><p>    Create a conceptual data model to identify key business entities and   visualize their relationships<br /> </p><p>    Create detailed logical models using business intelligence logic by   identifying all the entities, attributes, and their relationships<br /> </p><p>    Create a taxonomy/data dictionary to communicate data requirements that are   important to business stakeholders work on acquiring external data sets   through APIs and/or Websockets and prepare physical data models on top of   that<br /> </p><p>    Acts as team lead stay current with new and evolving tech stack. Guide and   mentor team of Data Engineers.<br /> </p><p>    JOB REQUIREMENTS<br />     What youll bring <br /> </p><p>    7+ years of Experience in building Data Engineering pipelines & Data   Governance using modern Cloud Architecture.<br /> </p><p>    Proficient in Databricks, Spark, Data Lake, Kaka/Kinesis<br /> </p><p>    Experience in Any Cloud DW Redshift/Snowflake/BigQuery/Synapse<br /> </p><p>    Experts Programming in Any one - Python/Scala/Java (Python preferred)<br /> </p><p>    Design, Test-driven development, code review and implement CICD using   Github/Gitlab/Docker<br /> </p><p>    Good understanding of ETL/ELT technology and processes<br /> </p><p>    Basic knowledge of Apache Airflow would be a plus<br /> </p><p>    Basic knowledge of Data Modeling tools (dbt, dataform, Alteryx, Informatic,etc)   would be a plus<br /> </p><p>    Big Plus<br /> </p><p>    Experience in ML ops and tools for Model Reproducibility, Deployment,   packaging,<br /> </p><p>    monitoring and Model retraining.<br /> </p><p>    Experience in Lakehouse Architecture using Databricks.<br /> </p>",Lead Data Engineer,3500000,5000000,https://www.naukri.com/job-listings-lead-data-engineer-coindcx-bangalore-bengaluru-10-to-12-years-081222904435,"['DataLake', 'Azure', 'Data Engineering', 'BigQuery', 'Apache', 'Kaka', 'Design', 'Kinesis', 'GCP', 'Snowflake', 'CICD', 'Data Lake', 'Spark', 'Databricks', 'ETL', 'AWS', 'park']",['Bengaluru'],Data Engineer,2022-12-08 10:18:49
081222502074,"<div> <ul> <li> <p> <span> <span> 5 years of Bachelor\s or Master\s degree with hands on experience in building data engineering pipelines in a distributed environment </span> <br /> <span> Good understanding of Data warehousing principles </span> <br /> <span> Expertise in working with  </span> <span> Big Data Technologies like Hadoop ,Hive, Spark(Scala/Java/Python), Sqoop, Oozie/Airflow </span> <br /> <span> Good exposure to writing complex SQL ,shell scripting </span> <br /> <span> Exposure to streaming data pipelines using  </span> <span> Spark, Kafka </span> <br /> <span> Exposure to Performance optimization of batch pipelines written in Hive/Spark </span> <br /> <span> Experience on CI/CD (Continuous Integration/Delivery) i.e. Jenkins, GIT/BitBucket </span> <br /> <span> Familiarity with working in an agile software development framework </span> </span> </p> <p> <span> </span> </p> </li> </ul> <div> <div> <div> <div> <div> <div> </div> </div> </div> </div> </div> </div> </div>",Sr. Data Engineers,600000,1000000,https://www.naukri.com/job-listings-sr-data-engineers-purview-india-consulting-and-services-llp-remote-4-to-8-years-081222502074,"['hive', 'continuous integration', 'GIT', 'spark', 'Shell scripting', 'Performance optimization', 'big data', 'Data warehousing', 'SQL', 'Python']",['remote'],Data Engineer,2022-12-08 20:42:20
291122501566,"<p> </p><ul> <li> We are looking for an experienced data engineer who use various methods to transform raw data into useful data systems create algorithms and conduct statistical analysis </li> <li> Overall, able to align data systems with business goals </li> <li> A detail-oriented person, with excellent organizational skills, who have strong analytical skills, and the ability to combine data from different sources </li> <li> familiarity with several programming languages like Python, Java and knowledge of machine learning algorithms </li> </ul> <p> </p> <p> <strong> Responsibilities  </strong> </p> <p> </p><ul> <li> Analyse and organize raw data Build data systems and pipelines Evaluate business needs and objectives Interpret trends and patterns </li> <li> Conduct complex data analysis and report on results Prepare data for prescriptive and predictive modelling </li> <li> Build algorithms and prototypes </li> <li> Combine raw information from different sources </li> <li> Explore ways to enhance data quality and reliability </li> <li> Identify opportunities for data acquisition </li> <li> Develop analytical tools and programs </li> <li> Collaborate with data scientists and architects on several projects </li> </ul> <p> <strong> Requirements and skills </strong> </p> <ul> <li> Previous experience as a data engineer or in a similar roleHands on experience in developing ETL pipeline using Apache airflow or similar tools </li> <li> Experience in working on cloud technologies AWSGCP especially related to data orchestration </li> <li> Experience on big data HDFS, spark technical expertise with data models, data mining, data migration data architect and segmentation techniques  </li> <li> Knowledge of programming languages eg, Java and Python Strong analytical and reasoning skills that results in clear technical execution. </li> <li> Hands on and efficient in writing complex SQL queries for all test cases Unit System functional Data reconciliation. </li> <li> Good understanding of BI DWH development methodologies Strong Experience with Data governance Data Quality, Metadata Management, Security etc  </li> <li> Engineer and orchestrate data flows and pipelines in a cloud environment using a progressive tech stack Skilled at translating requirements into clean, efficient, quality code which is scalable and easy to maintain. </li> <li> Coordinate with multiple stakeholders like Operations, Data Science and other IT Data teams to effectively manage the data environment  </li> <li> Excellent organizational and time management with ability to manage multiple priorities to accomplish the objectives and goals anticipating and adjusting for problems and interruption roadblocks  </li> <li> Solid understanding and experience practicing Agile software development methodologies  </li> <li> Great numerical and analytical skills Degree in Computer Science, IT, or similar field a Masters is a plus Data engineering certification eg IBM Certified Data Engineer is a plus </li> </ul> <p> </p> <div> <b> SOFT SKILLS: </b> </div> <ul> <li> Excellent communication skills, verbal as well as written </li> <li> Positive attitude with flexibility and maturity to work in a challenging client environment </li> <li> Ability to drive project responsibilities in a dynamic and proactive manner </li> <li> Ability to showcase three Mahindra RISE Pillars eg, Accepting No Limits, Driving Positive Change and Alternative Thinking </li> </ul>",Senior Data Engineer,900000,1300000,https://www.naukri.com/job-listings-senior-data-engineer-bristlecone-mumbai-4-to-7-years-291122501566,"['Procurement', 'SAN', 'Automation', 'Data analysis', 'Data migration', 'Information security', 'Agile', 'Test cases', 'Apache', 'Python']",['Mumbai'],Data Engineer,2022-11-29 18:37:34
061222010655,"<p>Hi Folks,</p><br /><p>""Great opportunity Come join us in reshaping the Future Of Neutrino Tech Systems""</p><br /><p>We have requirement open for <strong>Senior Data Engineer</strong></p><p>Total Exp- 6+</p><p>Location-Pune (Kharadi)</p><p>Notice Period-Immediate to Max 15 Days</p><br /><p> <strong>Skills</strong>   </p><ul><li> Strong experience and exposure to SQL programming including Stored Procedures</li><li>Ability to create views, schemas and tables as per business needs</li><li>Experience in Snowflake and Big Query</li><li>Technical expertise with data models, data mining, and segmentation techniques. </li><li>Great numerical and analytical skills. GCP/Azure/AWS cloud hands-on expertise would be preferred.</li><li> Experience in the design, creation, management, and business use of large datasets</li><li> Excellent comm skills Oversee Data integration work including Development of data model, maintaining a data warehouse and analytics environment, and writing scripts for data integration and analysis.</li><li> Experience on any Visualisation tool like Tableau/ Power BI   </li></ul><br /><p> <strong>Roles and Responsibilities</strong> </p><ul><li>Collaborate on projects and work independently when required </li><li>Understand the business processes and provide relevant data to the teams as and when needed as per business scenarios</li><li> Work closely with Data Analyst and Scientist </li><li>Build dashboards and visualizations as per Business needs Build Views/ Table in Relational DBs.   </li></ul><br /><br />",Senior Data Engineer,2500000,2500000,https://www.naukri.com/job-listings-senior-data-engineer-nts-automation-labs-pune-4-to-9-years-061222010655,"['data mining', 'Bigquery', 'Data Warehousing', 'Python']",['Pune'],Data Engineer,2022-12-06 17:46:55
051222906526,"<p> </p><p><strong>Profile: Data Engineer</strong></p><p><strong>Experience: 3+ Years</strong></p><p><strong>Location: Permanent Remote</strong></p><br /><p><strong>What is Uplers Talent Network?</strong></p><p>Uplers Talent Network is a place where top talents meet the right opportunities. It is a platform for every candidate looking for a perfect opportunity to work with global companies on a contractual basis. Our talent network is a place for top Indian talents who can benefit from the platform and gain access to global career exposure.</p><br /><p>With us, you'll get the support, guidance, and opportunities that you need to take your career to the next level. So, if you're ready to embark on the journey of your next challenge, we're ready to be your engine!</p><br /><p><strong>Contractual Position</strong></p><p>A contractual position usually requires you to sign and agree to the terms of a contract before you begin working. This structure can offer a variety of commitments that allow you to refine established skills and create new ones. </p><br /><p><strong>Uplers Talent Network brings contractual positions with benefits like:</strong> </p><ul><li>Higher pay than industry standards</li><li>Full-time position</li><li>Ability to gain different skills in a short period</li><li>Control over your career</li></ul><p> </p><p><strong>Perks of joining Uplers Talent Network:</strong></p><ul><li><strong>Talent Success Coach:</strong> Get connected with a dedicated coach to guide you before, during as well as after your assignments with our clients.</li><li><strong>Payout:</strong> Get paid in global currencies and earn more than industry standards.</li><li><strong>Opportunity:</strong> Work with international companies and get global exposure with exciting projects.</li><li><strong>Mobility:</strong> Work from the comfort of your living room couch or even a breezy beach.</li></ul><p><strong>How to become a part of our Talent Network?</strong></p><ol type=""1""><li>Take the first step, register on our portal</li><li>Clear the decks and fill out the application form</li><li>Gear up, clear the 3-stage assessment process</li><li>And yes! Become a part of Uplers Certified Talent Network</li></ol><p><strong>Requirements:</strong></p><ul><li>3+ years of experience in a Data Engineer role,</li><li>Experience with big data tools: Hadoop, Spark, Kafka, etc.</li><li>Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.</li><li>Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.</li><li>Experience with AWS cloud services: EC2, EMR, RDS, Redshift</li><li>Experience with stream-processing systems: Storm, Spark-Streaming, etc.</li><li>Experience with object-oriented/object function scripting languages: Python, R, Java, C++, Scala, GoLang, etc.</li><li>Hands-on experience with ETL tools</li><li>BI tools knowledge</li><li>Experience in Warehouse: SQL/NoSQL, Amazon Redshift, Panoply, Oracle, Talend, Informatica, Apache Hive, etc</li><li>Bachelor's degree in data engineering, big data analytics, computer engineering, or related field.</li></ul><p><strong>Uplift with Uplers</strong></p><p>Uplers believes in connecting people. Being a people-first organization, Uplers constantly strives for individuals who won't just keep up, but break new ground. Helping you find whats next is what were all about. It doesn't surprise us that we are growing and becoming one of the most popular companies in the industry.</p><br /><p><strong>All set to enter the #WorldOfAwesomeness? </strong></p><p><strong>Hit the Apply button!</strong></p>",Senior Data Engineer,80000,150000,https://www.naukri.com/job-listings-senior-data-engineer-uplers-chennai-3-to-8-years-051222906526,"['Java', 'Postgresql', 'Python']",['Chennai'],Data Engineer,2022-12-05 17:30:54
051222906525,"<p> </p><p><strong>Profile: Data Engineer</strong></p><p><strong>Experience: 3+ Years</strong></p><p><strong>Location: Permanent Remote</strong></p><br /><p><strong>What is Uplers Talent Network?</strong></p><p>Uplers Talent Network is a place where top talents meet the right opportunities. It is a platform for every candidate looking for a perfect opportunity to work with global companies on a contractual basis. Our talent network is a place for top Indian talents who can benefit from the platform and gain access to global career exposure.</p><br /><p>With us, you'll get the support, guidance, and opportunities that you need to take your career to the next level. So, if you're ready to embark on the journey of your next challenge, we're ready to be your engine!</p><br /><p><strong>Contractual Position</strong></p><p>A contractual position usually requires you to sign and agree to the terms of a contract before you begin working. This structure can offer a variety of commitments that allow you to refine established skills and create new ones. </p><br /><p><strong>Uplers Talent Network brings contractual positions with benefits like:</strong> </p><ul><li>Higher pay than industry standards</li><li>Full-time position</li><li>Ability to gain different skills in a short period</li><li>Control over your career</li></ul><p> </p><p><strong>Perks of joining Uplers Talent Network:</strong></p><ul><li><strong>Talent Success Coach:</strong> Get connected with a dedicated coach to guide you before, during as well as after your assignments with our clients.</li><li><strong>Payout:</strong> Get paid in global currencies and earn more than industry standards.</li><li><strong>Opportunity:</strong> Work with international companies and get global exposure with exciting projects.</li><li><strong>Mobility:</strong> Work from the comfort of your living room couch or even a breezy beach.</li></ul><p><strong>How to become a part of our Talent Network?</strong></p><ol type=""1""><li>Take the first step, register on our portal</li><li>Clear the decks and fill out the application form</li><li>Gear up, clear the 3-stage assessment process</li><li>And yes! Become a part of Uplers Certified Talent Network</li></ol><p><strong>Requirements:</strong></p><ul><li>3+ years of experience in a Data Engineer role,</li><li>Experience with big data tools: Hadoop, Spark, Kafka, etc.</li><li>Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.</li><li>Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.</li><li>Experience with AWS cloud services: EC2, EMR, RDS, Redshift</li><li>Experience with stream-processing systems: Storm, Spark-Streaming, etc.</li><li>Experience with object-oriented/object function scripting languages: Python, R, Java, C++, Scala, GoLang, etc.</li><li>Hands-on experience with ETL tools</li><li>BI tools knowledge</li><li>Experience in Warehouse: SQL/NoSQL, Amazon Redshift, Panoply, Oracle, Talend, Informatica, Apache Hive, etc</li><li>Bachelor's degree in data engineering, big data analytics, computer engineering, or related field.</li></ul><p><strong>Uplift with Uplers</strong></p><p>Uplers believes in connecting people. Being a people-first organization, Uplers constantly strives for individuals who won't just keep up, but break new ground. Helping you find whats next is what were all about. It doesn't surprise us that we are growing and becoming one of the most popular companies in the industry.</p><br /><p><strong>All set to enter the #WorldOfAwesomeness? </strong></p><p><strong>Hit the Apply button!</strong></p>",Senior Data Engineer,80000,150000,https://www.naukri.com/job-listings-senior-data-engineer-uplers-delhi-ncr-3-to-8-years-051222906525,"['Java', 'Postgresql', 'Python']",['Delhi NCR'],Data Engineer,2022-12-05 17:30:53
081222503508,"<p> The Senior Data Engineer will oversee the departments data integration work, including developing a data model, maintaining a data warehouse and analytics environment, and writing scripts for data integration and analysis. </p> <p> </p> <p> Responsibilities </p> <ul> <li> Analyze and organize raw data. </li> <li> Build data systems and pipelines. </li> <li> Evaluate business needs and objectives. </li> <li> Interpret trends and patterns. </li> <li> Conduct complex data analysis and report on results. </li> <li> Prepare data for prescriptive and predictive modeling. </li> <li> Build algorithms and prototypes. </li> </ul> <p> </p> <p> <strong> Must Have skills: </strong> </p> <p> Azure Data Lakes, Synapse(Serverless Model) and Azure Data Factory ,Concepts of ETL and Data Warehouse, SQL Server </p> <p> </p> <p> <strong> Good to have Skills: </strong> </p> <p> Power BI </p> <div> <div> <div> <strong> Key Skills : </strong> <ul> <li> Azure Data Lakes  </li> <li> Synapse  </li> <li> Azure Data Factory  </li> <li> Data Warehouse  </li> </ul> </div> </div> </div> <div> <div> <div> <div> </div> </div> </div> </div>",Sr. Data Engineer,500000,900000,https://www.naukri.com/job-listings-sr-data-engineer-mm-staffing-career-consultants-private-limited-remote-5-to-8-years-081222503508,"['BPO', 'Data analysis', 'Script writing', 'Data modeling', 'CMMI', 'power bi', 'Predictive modeling', 'Data warehousing', 'Analytics', 'SQL']",['Remote'],Data Engineer,2022-12-08 21:56:09
271022006554,"<p> </p><p><strong>Senior Data Engineer</strong></p><br /><br /><p>Better is looking for a Sr Data Engineer to help with our data platform. We are responsible for ingesting dozens of on site and 3rd party data sources and maintaining the data processing and data warehousing infrastructure to provide insights for the rest of the organization. This is a team lead position.</p><p>In this role within Data Engineering, your responsibilities include but not limited to the following:</p><ul><li> take ownership of managing Data Pipeline and Data Warehouse, while improving and following the best practices to provide actionable insights to the key stakeholders</li><li> analyze and improve the data pipeline quality, such as data optimize, service stabilities and high availability, etc</li><li> analyze the data and generate reports for stakeholders and help the team to improve product performance</li><li> interface and collaborate with both technical and business stakeholders</li></ul><br /><br /><p><strong>Hard Skills:</strong></p><br /><br /><ul><li> strong python and pandas</li><li> experience in Airflow or other scheduler</li><li> experience with a major data warehouse</li><li> SQL optimization, query plan analysis</li><li> experience with stream processing pipeline</li><li> experience with a major BI stack: looker, chartio, tableau, metabase</li><li> understanding of data concepts: durability, transactions, optimizers,</li></ul><br /><br /><p><strong>Soft Skills:</strong></p><br /><br /><ul><li> articulating requirements concisely and accurately</li><li> good code review etiquette</li><li> mentoring jr data engineers into technical depth</li><li> liaison with analytics teams, core infra teams, and data producing application teams</li><li> run agile rituals (standup, backlog grooming, retrospectives)</li></ul><br /><br /><p><strong>Nice to have:</strong></p><br /><br /><ul><li> experience with kubernetes a plus</li><li> experience with dbt a plus</li><li> systems fundamentals (memory / IO / CPU tradeoffs, networking)</li><li> numpy/sklearn familiarity</li><li> familiarity with pyspark</li></ul><br /><br />",Senior Data Engineer,3500000,5500000,https://www.naukri.com/job-listings-senior-data-engineer-better-com-gurgaon-gurugram-7-to-12-years-271022006554,"['Snowflake / Redshift', 'ETL', 'Python', 'sql']",['Gurgaon'],Data Engineer,2022-12-06 15:11:20
291122501713,"<ul> <li> Diverse Data Engineering activities on Azure Data platform. </li> <li> Build & Deliver Data pipeline connecting various enterprise data sources both RDBMS, NoSQL & APIs. Develop data mappings to existing data sources. </li> <li> Design and develop big data processing notebooks using Azure Databricks </li> <li> Scripting and programming using programming languages such as Python, PySpark etc. </li> <li> Lead the data identification, data analysis efforts for data sources and work directly with data owner teams </li> <li> Lead a team of junior Data Engineers in an Agile/Scrum setting </li> <li> Develop data requirements for new data sources. </li> <li> Design and development of data extraction, data ingestion, data quality rules implementation  </li> <li> Understand the data model, transform data to target schema from relational and semi structured source data. </li> <li> Clean and process the data for Machine Learning consumption. </li> <li> Provide Business Intelligence (PowerBI) and Data Warehousing (DW) solutions and support by leveraging project standards and leading analytics platforms </li> <li> Evaluate and define functional requirements for BI and DW solutions </li> <li> Design and development of data extraction, data ingestion, data quality rules implementation  </li> <li> Involvement in architecture and design of data </li> <li> Work with Data Governance team to implement Data Quality and Security guidelines </li> <li> Work with DevOps team to implement CI/CD pipelines </li> </ul> <p> <strong> Required Skills :- </strong> </p> <ul> <li> 8+ years overall experience preferably in data domain (data analysis, database developer) </li> <li> Minimum 5 years’ experience as a cloud-based Data Engineer </li> <li> Minimum 3 years’ experience and strong knowledge of Azure Data Services such as Azure Databricks, Delta Lake, Azure Data Lake Storage, Blob Storage, Azure Data Factory </li> <li> Minimum 2 years’ experience on Analytics Visualization tools like Power BI, Qlik Sense etc. </li> <li> Strong knowledge of Big Data and Analytics, including Apache Spark, Delta Lake </li> <li> Good understanding of data integration and warehousing (ETL, ELT) processes </li> <li> Strong knowledge of Database concepts, SQL and experience in working with Database clients like, TOAD, SQL developer, SQL Server Management Studio </li> <li> Good knowledge of NoSQL/ Document DB. </li> <li> Good understanding of Enterprise Information Model and Enterprise Data Warehouse </li> <li> Proficient in analyzing business requirements and mapping them to technical requirements </li> <li> Data Analysis and Interpretation, and Data Issue Debugging Skills </li> <li> Good knowledge of DevOps tools and processes as it applies to Data Engineering </li> </ul>",Azure Data Engineer,500000,900000,https://www.naukri.com/job-listings-azure-data-engineer-ntt-global-delivery-services-limited-bangalore-bengaluru-8-to-10-years-291122501713,"['Data analysis', 'data domain', 'Debugging', 'Data quality', 'Business intelligence', 'big data', 'Analytics', 'SQL', 'Python', 'Data extraction']",['Bengaluru'],Data Engineer,2022-11-29 18:37:37
221121002548,"<p> </p><p><strong>Position</strong>: Data Engineer</p><p><strong>Package</strong>: Best in class(Depending on the skillsets, experience & fitment)</p><p>_______________________________________________________________________________________________</p><p><strong>People who are serving their notice period or have served their notice period and who can demonstrate joining date in the next***** 10 to 12 *****days are only expected to apply for this job</strong></p><p>_______________________________________________________________________________________________</p><br /><p><strong>Core Responsibilities :</strong></p><br /><ul><li> The candidate is expected to lead one of the key analytics area end to end. This is a pure hands on role.</li><li> Ensure the solutions built meet the required best practices and coding standards.</li><li> Ability to adapt any new technology if situation demands.</li><li> Requirement gathering with business and get this prioritized in the sprint cycle.</li><li> Should be able to take end to end responsibility of assigned task</li><li> Ensure quality and timely delivery.</li></ul><br /><p><strong>Preference and Experience:</strong></p><br /><ul><li> Strong at PySpark, Python, Java fundamentals</li><li> Good understanding of Data Structure</li><li> Good at SQL query/optimization</li><li> Strong fundamental of OOPs programming</li><li> Good understanding of AWS Cloud,Big Data.</li><li> Nice to have Data Lake,AWS Glue, Athena, S3, Kinesis, SQL/NoSQL DB</li></ul><br /><p>Academic qualifications :</p><ul><li> Must be a Technical Graduate Btech / Mtech Tier 1/2 colleges.</li></ul><br /><p><strong>Experience Range: </strong>2 to 6 year</p><br /><br />",Data Engineer,400000,900000,https://www.naukri.com/job-listings-data-engineer-vantageiq-technologies-opc-pune-bangalore-bengaluru-2-to-6-years-221121002548,"['Data Structures', 'Data Analysis', 'Spark', 'ETL', 'Python', 'Data Integration']","['Pune', 'Bengaluru']",Data Engineer,2022-12-05 12:16:08
051222501820,"<div> <ul> <li> Perform design and write technical specifications according to requirements </li> </ul> <ul> <li> Build code as per design and coding standards </li> </ul> <ul> <li> Manage release and deployment </li> </ul> <ul> <li> Ensure usage of best practices and reuse </li> </ul> <ul> <li> Coordinate and interact with BA, Architect, Platform Administrator etc. </li> </ul> <ul> <li> 4 6 years IT experience with minimum 2 years of experience in Big Data technologies using Spark/Scala, HDFS, Hive, Oozie, HDP. </li> </ul> <ul> <li> Hands-on experience on the all the different layers of BigData ecosystem. </li> </ul> Behavioral Skills  <ul> <li> Ability to communicate (verbal and written) effectively with counterparts in SG Paris </li> </ul> <ul> <li> Ability to handle technical issues and escalations </li> </ul> <ul> <li> Ability to support the project manager to resolve operational issues </li> </ul> <div>   </div> </div>",Data Engineer,800000,1100000,https://www.naukri.com/job-listings-data-engineer-kg-invicta-services-kgis-bangalore-bengaluru-france-canada-united-states-usa-6-to-11-years-051222501820,"['hive', 'Administration', 'Coding', 'spark', 'oozie', 'SCALA', 'hdfs', 'big data', 'Operations']","['United States (U.S)', 'France', 'Bengaluru', 'Canada']",Data Engineer,2022-12-05 21:56:33
091122014765,"<p>Hi,</p><br /><p>We have an open position with is for data scientist, details follow:</p><br /><p>Experience: 7-10 years</p><br /><p>Location: Bangalore (Hybrid Model)</p><br /><p><strong>Job Description:</strong></p><br /><ul><li>Incumbent needs to have good hands on experience into bigdata (scoop, hive and pyspark)</li><li>Candidate should have experience in writing SQL queries</li><li>Candidate should have exposure to Python (numpy, Pandas)</li><li>Should have some data analytics background and statistical knowledge</li></ul><br /><br /><br />",EMIDs Hiring||Data Engineer||Bangalore Location,225000,475000,https://www.naukri.com/job-listings-emids-hiring-data-engineer-bangalore-location-emids-technologies-pvt-ltd-bangalore-bengaluru-7-to-10-years-091122014765,"['hive', 'sql queries', 'data analytics', 'Data Engineering', 'Big Data', 'pyspark', 'Mapreduce', 'numpy', 'Hdfs', 'Sqoop', 'Apache Pig']",['Bengaluru'],Data Engineer,2022-12-05 23:44:49
081222006147,"<p><strong>Job Overview </strong></p><p>Designing, building and operationalizing large-scale enterprise data solutions and applications using one or more of Google Cloud Platform data and analytics services in combination with technologies like Spark, Cloud DataProc, Cloud Dataflow, Apache Beam, BigTable, Cloud BigQuery, Cloud PubSub, Cloud Functions, Airflow.<strong> </strong></p><br /><p><strong>Responsibilities : </strong></p><p>• Designing and implementing data transformation, ingestion and curation functions on GCP cloud using GCP native or custom programming </p><p>• Designing and building production data pipelines from ingestion to consumption within a hybrid big data architecture, using Java, Python etc. </p><p>• Performing detail assessments of current state data platforms and creating an appropriate transition path to GCP cloud </p><p>• Analyzing, re-architecting and re-platforming on-premise data warehouses to data platforms on GCP cloud using GCP/3rd party services </p><p>• Optimizing data pipelines for performance and cost for large scale data lake<strong>s </strong></p><br /><p><strong>Desired Qualifications: </strong></p><p>• Hands-on GCP experience with a minimum of 1 solution designed and implemented at production scale </p><p>• 3+ years of experience writing complex SQL queries, stored procedures, etc </p><p>• Hands-on experience architecting and designing data lakes on GCP cloud serving analytics and BI application integrations </p><p>• Experience in designing and optimizing data models on GCP cloud using GCP data stores such as BigQuery, BigTable </p><p>• Experience integrating GCP or 3rd party KMS, HSM with GCP data services for building secure data solutions </p><p>• Experience introducing and operationalizing self-service data preparation tools (e.g. Trifacta, Paxata) on GCP</p><ul><li> Experience architecting and implementing metadata management on GCP </li><li>Architecting and implementing data governance and security for data platforms on GCP </li><li>Agile development skills and experience. </li><li>Experience with CI/CD pipelines such as Concourse, Jenkins </li><li>Dimensional modeling using tools like AtScale </li><li>Google Cloud Platform certification is a plu<strong>s</strong></li></ul>",Data Engineer,1800000,3000000,https://www.naukri.com/job-listings-data-engineer-cardinal-health-bangalore-bengaluru-5-to-9-years-081222006147,"['airflow', 'Informatica', 'ETL', 'big data', 'data engineering']",['Bengaluru'],Data Engineer,2022-12-08 12:45:39
071222500728,"<div> <p> </p> <ul> <li> Drive stakeholder engagement with product and business teams by gathering requirements and building inclusive culture with them while having quality interactions.  </li> </ul> <ul> <li> Developing and maintaining reliable and scalable ETL pipelines for big data using Python, Spark, SQL and AWS services.  </li> </ul> <ul> <li> Responsible for designing and creating the data warehouse and all related extraction, transformation and load of data functions in the company.  </li> </ul> <ul> <li> Work closely with the data platform team to review the development or modification of highly complex Data or BI or Platform solutions.  </li> </ul> <ul> <li> Deploying data pipelines in production following CI/CD  </li> </ul> <ul> <li> Build analytics dashboards and tools that utilize the data pipeline to provide actionable insights into product performance and other important business metrics.  </li> </ul> <ul> <li> Ensure the quality of architecture and design of data infrastructure, monitoring for data quality.  </li> </ul> <ul> <li> Brainstorm, share insights and incorporate best practices to ensure data correctness and reliability. Provide reasonable approaches on solving complex test scenarios.  </li> </ul> <strong> Ideally, you should have:  </strong> <strong> </strong> <ul> <li> At Least 8+ years of experience as a Data or BI Engineer dealing with large complex data scenarios.  </li> </ul> <ul> <li> Data Warehousing experience with any database like Redshift etc  </li> </ul> <ul> <li> Demonstrated ability in data modeling, ETL development, and data warehousing.  </li> </ul> <ul> <li> Ability to understand basic query profiles and execution plans. Experience in query performance tuning is a plus.  </li> </ul> <ul> <li> Coding proficiency in at least one modern programming language (Python, Java, Scala, etc)  </li> </ul> <ul> <li> Experience with Big Data Technologies (Presto, Hadoop, Hive, Spark, Airflow, etc)  </li> </ul> <ul> <li> Experience in large-scale data warehousing projects using Redshift, S3, Spark, Presto, Hive.  </li> </ul> <ul> <li> Learning ability : Is self-reflective, Has a hunger to improve, Has a keen interest to drive their own learning. Applies theoretical knowledge to practice.  </li> </ul> <ul> <li> Nice to have: Attends and participates in conferences/meetups/user groups, Contributes to open source projects.  </li> </ul> <strong> About the Benefits  </strong> <br /> <p> You will be joining the worlds first social learning platform where you can have a real impact in a smart, low-ego, multi-cultural team. We provide an environment where you can develop your skills and deliver meaningful work that matters. You ll be able to enjoy a competitive salary as well as a full spectrum of generous perks and rewards:  </p> <ul> <li> Not a morning person? No problem! We offer flexitime and a great work-life balance, including a full remote role, allowing you the flexibility to work from the location you feel most productive and comfortable.  </li> </ul> <ul> <li> 21 days annual leave with 12 Casual/sick leaves and 12 government holidays  </li> </ul> <ul> <li> Full premium Medical coverage including Family Dependants  </li> </ul> <ul> <li> Tax benefits on expenses incurred towards Broadband, Phone, Books & Wellness  </li> </ul> <ul> <li> Learning budget of INR 20K per year  </li> </ul> </div>",Principal Data Engineer,1000000,1500000,https://www.naukri.com/job-listings-principal-data-engineer-noon-the-social-learning-platform-bangalore-bengaluru-8-to-13-years-071222500728,"['Performance tuning', 'Data modeling', 'Coding', 'Test scenarios', 'Data quality', 'Open source', 'Analytics', 'Monitoring', 'SQL', 'Python']",['Bengaluru'],Data Engineer,2022-12-07 17:47:46
061222501358,"<div> <p> We are looking for a savvy Data Engineer to join our growing team of analytics experts. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross-functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up.  </p> <p> The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company s data architecture to support our next generation of products and data initiatives. </p> Responsibilities  <ul> <li> Create and maintain optimal data pipeline architecture, </li> <li> Assemble large, complex data sets that meet functional / non-functional business requirements. </li> <li> Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. </li> <li> Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS big data technologies. </li> <li> Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics. </li> <li> Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. </li> <li> Keep our data separated and secure across national boundaries through multiple data centers and AWS regions. </li> <li> Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader. </li> <li> Work with data and analytics experts to strive for greater functionality in our data systems. </li> </ul> Qualifications  <ul> <li> Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. </li> <li> Experience building and optimizing big data data pipelines, architectures and data sets. </li> <li> Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. </li> <li> Strong analytic skills related to working with unstructured datasets. </li> <li> Build processes supporting data transformation, data structures, metadata, dependency and workload management. </li> <li> A successful history of manipulating, processing and extracting value from large disconnected datasets. </li> <li> Working knowledge of message queuing, stream processing, and highly scalable big data data stores. </li> <li> Strong project management and organizational skills. </li> <li> Experience supporting and working with cross-functional teams in a dynamic environment. </li> <li> We are looking for a candidate with 5 years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:  <ul> <li> Experience with big data tools: Hadoop, Spark, Kafka, etc. </li> <li> Experience with relational SQL and NoSQL databases, including Postgres and Cassandra. </li> <li> Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc. </li> <li> Experience with AWS cloud services: EC2, EMR, RDS, Redshift </li> <li> Experience with stream-processing systems: Storm, Spark-Streaming, etc. </li> <li> Experience with object-oriented/object function scripting languages: Python, Java, C , Scala, etc. </li> </ul> </li> </ul> <p> </p> </div>",Data Engineer,300000,600000,https://www.naukri.com/job-listings-data-engineer-zuma-bangalore-bengaluru-2-to-2-years-061222501358,"['metadata', 'NoSQL', 'cassandra', 'Project management', 'Data structures', 'Analytics', 'SQL', 'Python', 'Data architecture']",['Bengaluru'],Data Engineer,2022-12-06 18:26:25
051222007034,"<p>We are Falabella. We deliver seamless customer experiences across seven countries in Latin America, with our niche technology center based in Bangalore, India. We enable business strategy in a multi-format, integrated retail business through the effective use of technology and innovation.</p><br /><p>Our people are at the heart of what we do and play a pivotal role in coding the future of the Falabella Group.</p><br /><p>At Falabella India, we are looking for passionate Data Engineers who are proficient in Big Data technologies, have a strong cloud background and excellent analytical and communication skills. We are looking for minds to work on Nextgen frameworks and platforms, to develop features in the eCommerce and Retail domains. We welcome people who are creative-thinkers, self-driven and passionate.</p><br /><p><strong>We are looking for</strong></p><ul><li>Graduate or Postgraduate with 3-6 Years of experience in Data Engineering.</li><li>Person should have experience in coding data pipeline on GCP.</li><li>Prior experience on Hadoop systems is ideal as candidate may not have total GCP experience.</li><li>Strong on programming languages like Scala, Python, Java.</li><li>Good understanding of various data storage formats and its advantages.</li><li>Should have Business mindset to understand data and how it will be used for BI and Analytics.</li><li>Data Engineer Certification preferred</li><li>Should have exposure on GCP tools to develop end to end data pipeline for various scenarios</li></ul><p>      (Including ingesting data from traditional data bases as well as integration of API based data</p><p>      sources).</p><br /><p><strong>Experience in working with GCP tools like -</strong></p><p><strong>Store:</strong> Cloud SQL, Cloud Storage, Cloud Bigtable, Big query, Cloud Spanner,</p><p>Cloud Datastore</p><p><strong>Ingest:</strong> Stack driver, Pub/Sub, App Engine, Kubernetes Engine, Kafka, Data Prep,</p><p>Micro services</p><p><strong>Schedule:</strong> Cloud Composer</p><p><strong>Processing:</strong> Cloud Dataproc, Cloud Dataflow, Cloud Data prep</p><p><strong>CI/CD:</strong> Bitbucket+Jenkinjs / Gitlab Atlassian Suite</p><br />",Data Engineer,100000,250000,https://www.naukri.com/job-listings-data-engineer-falabella-corporate-services-bangalore-bengaluru-3-to-8-years-051222007034,"['python', 'java', 'scala', 'GCP', 'hadoop', 'Gcp Cloud']",['Bengaluru'],Data Engineer,2022-12-05 17:55:02
051222000613,"<p><strong>Roles and Responsibilities</strong> </p><p> </p><ul><li>Create and maintain optimal data pipeline architecture. </li><li>Assemble large, complex data sets that meet functional / non-functional business requirements. </li><li>Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. </li><li>Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and GCP big data technologies. </li><li>Work with data and analytics experts to strive for greater functionality in our data systems </li></ul><br /><p><strong>Desired Candidate Profile</strong> </p><p> </p><ul><li>2+ Years in Data Engineering  </li><li>Should join within 30 days /Immediate joiner </li><li>Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases. </li><li>Experience building and optimizing big data data pipelines, architectures, and data sets. </li><li>Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement. </li><li>Strong analytic skills related to working with unstructured datasets. </li><li>Build processes supporting data transformation, data structures, metadata, dependency, and workload management. </li><li>Experience supporting and working with cross-functional teams in a dynamic environment. </li><li>Experience with relational SQL databases. </li><li>Experience with data pipeline and workflow management tools (Airflow etc.) </li><li>Experience with object-oriented/object function scripting languages </li><li>Python (libraries such as Pandas etc.) </li><li>Good to have experience in Core Java, Spring boot and Spring microservices</li><li>Knowledge in Spark and Spark SQL, Experience with Spark based data pipelines </li></ul><br /><p><strong>Perks and Benefits</strong> </p><br /><br />",Data Engineer,550000,1500000,https://www.naukri.com/job-listings-data-engineer-wingpoint-i2o-retail-hyderabad-secunderabad-chennai-bangalore-bengaluru-2-to-4-years-051222000613,"['Airflow', 'Workload Management', 'Data Structures', 'Metadata', 'Spring Boot', 'Spring Microservices', 'Dependency Injection', 'Data Pipeline', 'Pandas', 'SQL Database', 'Spark']","['Chennai', 'Bengaluru', 'Hyderabad']",Data Engineer,2022-12-05 09:51:46
081222007119,"<br /><p><strong>Software/Senior Software Engineer - Data : </strong></p><br /><p>Your responsibility is to work in a variety of settings to build systems that collect, manage, and convert raw data into usable information for all stakeholders, including data scientists and business analysts, to interpret. Your ultimate goal is to make data accessible so that organizations can use it to evaluate and optimize their performance.</p><br /><p><strong>What youll do -</strong></p><ul><li>Develop a top-notch platform to scale large amounts of data.</li><li>Determining, developing, and putting into practice internal process improvements, such as redesigning infrastructure for increased scalability, improving data delivery, and automating tedious procedures</li><li>Develop algorithms to transform data into useful, actionable information.</li><li>Conceptualizing and generating infrastructure that allows big data to be accessed and analyzed</li><li>Acquire datasets that align with non-functional and functional business requirements</li><li>Work with stakeholders including the Executive, Product, Business, CX, Data and Design teams to support their data infrastructure needs while assisting with data-related technical issues.</li><li>Create new data validation methods and data analysis tools.</li><li>Ensure compliance with data governance and security policies.</li></ul><br /><p><strong>Well, apart from a few cookies every day, itd be great if you come with these :</strong></p><ul><li>A computer science and engineering bachelor's degree or a master's degree with 1 - 4 years of experience in a relevant field is advantageous.</li><li>A proven track record of achievement as a data engineer, software developer, building data lake/warehouse and ETL pipeline</li><li>Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.</li><li>Experience building and optimizing big data data pipelines, architectures and data sets.</li><li>Working knowledge of message queuing, stream processing, and highly scalable big data data stores</li><li>Added advantage - Experience with big data tools: Hadoop, Spark, Kafka, etc.</li><li>Experience with relational SQL and NoSQL databases.</li><li>Experience with data pipelines and workflow management tools: Airflow or similar</li><li>Added advantage - Experience with AWS cloud services: EC2, EMR, Glue, RDS, Redshift</li><li>Added advantage - Experience with stream-processing systems: Spark (Structured Streaming) or Flink</li><li>Experience with object-oriented/object function scripting languages: Java, Scala, Python etc</li></ul><br /><p><strong>Why choose Gameskraft?</strong></p><p>Gameskraft is one of India’s fastest-growing gaming companies. We are building the world's most-loved online gaming ecosystem - one game at a time. Started by a group of passionate gamers in 2017, we have grown from a small team of only five members to a large family of 700+ Krafters, working out of our office in Prestige Tech Park, Bangalore.</p><p>As an organisation, we strongly believe in a culture that promotes a growing and nourishing environment for our Krafters. Every day, we work towards creating a comfortable, approachable, and transparent work environment, where we motivate and challenge each other to meet our crazy obsession for being the best in class. Our culture and values give us the extra edge to help us scale greater heights. We are a bunch of crazy people committed to the ethos of celebrating every win – small or big as part of our journey to creating unmatched experiences every day!</p><br /><br /><p><em>We wish you all the best!</em></p><br /><br /><br /><br />",Data Engineer,1000000,2000000,https://www.naukri.com/job-listings-data-engineer-gameskraft-bangalore-bengaluru-1-to-4-years-081222007119,"['java', 'spark', 'scala', 'hadoop', 'big data']",['Bengaluru'],Data Engineer,2022-12-08 14:34:05
021122009462,"<p><strong>What impact will you make?</strong></p><p>Every day, your work will make an impact that matters, while you thrive in a dynamic culture of inclusion, collaboration, and high performance. As one of the leading professional services organizations, Deloitte is where you will find numerous opportunities to succeed and realize your full potential.</p><br /><p><strong>The team</strong></p><p>Our Cloud Engineering team focuses on enabling our clients end-to-end journey from On-Premise to Cloud, with opportunities in the areas of: Cloud Strategy, Op Model Transformation, Cloud Development, Cloud Integration & APIs, Cloud Migration, Cloud Infrastructure & Engineering, and Cloud Managed Services. We help our clients see the transformational capabilities of Cloud as an opportunity for business enablement and competitive advantage.</p><p>Cloud Engineering supports our clients as they improve agility and resilience and identifies opportunities to reduce IT operations spend through automation by enabling Cloud. We accelerate our clients towards a technology-driven future, leveraging vendor solutions and Deloitte-developed software products, tools, and accelerators.</p><br /><p><strong>Responsibilities</strong></p><ul><li>The candidate would be required to lead a team of data and visualization engineers (8-10 members) who:</li><ul><li>Design & implement highly performant data ingestion pipelines from multiple sources using <strong>Spark Databricks</strong></li><li>Develop scalable and re-usable frameworks for ingestion and enrichment of datasets</li><li>Develop tabular models to provide a highly performant sematic layer able to handle high concurrency and low latency</li><li>Develop Power BI dashboards with NPS scores, are scalable and easy to maintain</li><li>Develop Integrated end to end data pipeline to take data from source systems to target data repositories ensuring the quality and consistency of data is maintained at all times</li></ul><li>The candidate would also be expected to define release plans, deployments schedules, cutover plans and roll back plans; and ensuring the team follows best practices of Azure devops for CI/CD</li><li>The candidate would also be expected to suggest and develop re-usable assets on Azure and also contribute to teams activities</li><li>The candidate would also be expected based on requirements, to suggest tools and approaches, evaluate them and prepare proposals</li><li>The candidate would also      be expected to manage the overall delivery of the project, preparing      project plans, calling out risks and possible mitigation approaches. The      role also requires a significant interaction with stakeholders from      clients</li><li>The candidate should be able to manage the team following agile concepts of scrum and Kanban. JIRA would be used to track and the candidate would be expected to ensure their teams adhere to it.</li><li>The candidate may also be expected to lead them following an Operate/DevSecOps methodology</li><li><u>The candidate would also be expected to provide estimation for a project delivery (cost and timelines) based on RFPs</u></li><li><u>The candidate would also be expected to identify the skill sets required and gaps in the team and also work with support services to staff the team accordingly (may need to drive recruitment in case of large specific demands)</u></li><li><u>The candidate would also be expected to define or modify a delivery execution framework, based on patterns observed on incidents in the team</u></li></ul><br /><p><strong><u>Qualifications</u></strong></p><ul><li>Experience of 4+ years in working end to end on cloud analytics platforms, with at least 2 years of experience in managing a delivery team</li><li>Strong knowledge of Data Management principles</li><li>Experience in building ETL/data warehouse transformation processes</li><li>Direct experience of building data pipelines using Azure Data Factory and Apache Spark on Azure Databricks</li><li>Experience in building conceptual and physical data models, applying normalization and      denormalization techniques, designing tabular models</li><li>Experience in working with Storages like DataLake Gen2, Azure SQLDB, Azure Synapse SQL Pool </li><li>Experience using Apache Spark and associated design and development patterns</li><li>Microsoft Azure Big Data Associate and Architecture certification.</li><li>Hands on experience designing and delivering solutions using Azure Storage, Azure Synapse      Analytics, Azure Analysis Services, Azure Data Lake, Azure Cosmos DB, </li><li>Experience working in a Dev/Ops environment with tools such as DevOps Services, Terraform etc.</li><li>Excellent client/stakeholder management skills</li><li>Must have excellent communication skills and decision making ability to confidently drive      projects towards success</li></ul><br /><p><strong><u>Experience Bracket</u></strong></p><ul><li>Preferably 9+ years of professional experience with 4+ years on cloud (Azure) and relevant exposure in client and team management</li><li><u>Preferably 12+ years of professional experience with 4+ years on cloud (Azure) and relevant exposure in client and team management.</u></li></ul><br /><p><strong>Your role as a leader</strong></p><p>At Deloitte India, we believe in the importance of leadership at all levels. We expect our people to embrace and live our purpose by challenging themselves to identify issues that are most important for our clients, our people, and for society, and make an impact that matters. </p><p>In addition to living our purpose, managers across our organization:</p><ul><li>Develop self by actively seeking opportunities for growth, share knowledge and      experiences with others, and act as a strong brand ambassador</li><li>Understand objectives for clients and Deloitte, align own work to objectives and set      personal priorities</li><li>Seek opportunities to challenge self</li><li>Collaborate with others across businesses and borders to deliver and take accountability for own and team results</li><li>Identify and embrace our purpose and values and put these into practice in their      professional life</li><li>Build relationships and communicate effectively in order to positively influence peers and other stakeholders</li></ul><br /><p><strong>Professional growth</strong></p><p>At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe theres always room to learn. From on-the-job learning experiences to formal development programs at Deloitte University, our professionals have a variety of opportunities to continue to grow throughout their career. Explore Deloitte University, The Leadership Center.</p><br /><p><strong>Benefits</strong></p><p>At Deloitte, we know that great people make a great organisation. We value our people and offer employees a broad range of benefits. Learn more about what working at Deloitte can mean for you.</p><br /><p><strong>Our purpose</strong> </p><p>Deloitte is led by a purpose: <em>To make an impact that matters</em>. </p><p>Every day, Deloitte people are making a real impact in the places they live and work. We pride ourselves on doing not only what is good for clients, but also what is good for our people and the communities in which we live and workalways striving to be an organization that is held up as a role model of quality, integrity, and positive change. </p><p>Learn more about Deloitte's impact on the world</p>",Azure Data Engineer,4000000,9000000,https://www.naukri.com/job-listings-azure-data-engineer-deloitte-shared-services-india-llp-bangalore-bengaluru-12-to-14-years-021122009462,[],['Bengaluru'],Data Engineer,2022-12-08 11:38:07
061222006872,"<p>  </p><p><strong>Company Name: Syren Cloud</strong></p><p><strong>Location  Hyderabad - Hybrid</strong></p><p><strong>JOB DESCRIPTION - GCP CLOUD ENGINEER </strong></p><p>GCP Big query and python are the Primary skills: Syren is looking for a Cloud Data engineer who has knowledge on GCP, Cloud Bigtable, Big query, Data Proc, cloud storage, cloud composer.<br /> GCP engineer with application development will be an added advantage.  <br /> Should be able to build end-to-end data pipelines which includes data ingestion, data transformations using different services in GCP.</p><p><strong>Responsibilities:</strong></p><ul><li> Act as a subject matter expert in data engineering and <strong>GCP </strong>data technologies.</li><li> Work with client teams to design and implement modern, scalable data solutions using a range of new and emerging technologies from the Google Cloud Platform.</li><li> Be required to showcase your GCP Data engineering experience when communicating with clients on their requirements, turning these into technical data solutions.</li><li> Be required to build and deliver Data solutions using GCP products and offerings.</li></ul><p><strong>Requirements:</strong></p><ul><li> Liaise and be part of our extensive GCP community, contributing in the knowledge exchange learning programmed of the platform.</li></ul><p>Skill:</p><ul><li> Hands on and deep experience working with <strong>Google Data Products</strong> (e.g., Big Query, Dataflow, DataPro, AI Building Blocks, Looker, Cloud Data Fusion, Data prep, etc.).</li><li> Experience in Spark (Scala/Python/Java).</li></ul><p>Experience in MDM, Metadata Management, Data Quality and Data Lineage tools.</p><ul><li> E2E Data Engineering and Lifecycle (including non-functional requirements and operations) management.</li><li> Regulatory and Compliance work in Data Management.</li><li> E2E Solution Design skills - Prototyping, Usability testing and data visualization literacy.</li></ul>",Data Engineer - Google Cloud - GCP,700000,1700000,https://www.naukri.com/job-listings-data-engineer-google-cloud-gcp-syren-technologies-hyderabad-secunderabad-chennai-bangalore-bengaluru-3-to-8-years-061222006872,"['scala', 'spark', 'data engineering', 'Python']","['Chennai', 'Bengaluru', 'Hyderabad']",Data Engineer,2022-12-06 13:30:33
081222904499,"<p> Design & Build reliable, scalable, CICD-driven streaming and batch data engineering pipelines.<br /> Work in collaboration with Data scientists, ML engineers, and Stakeholders to build a platform for enabling data-driven decisions.<br /> Oversee and govern the expansion of the current data architecture and the optimization of query and data warehouse.<br /> Create a conceptual data model to identify key business entities and visualize their relationships<br /> Create detailed logical models using business intelligence logic by identifying all the entities, attributes, and their relationships<br /> Create a taxonomy/data dictionary to communicate data requirements that are important to business stakeholders work on acquiring external data sets through APIs and/or Websockets and prepare physical data models on top of that<br /></p><p>What you’ll bring<br /> 2-4 years of experience building Data Engineering pipelines & Data Governance using modern Cloud Architecture.<br /> Proficient in Databricks, Spark, Data Lake, Kaka/Kinesis<br /> Experience in Any Cloud DW<br />Redshift/ Snowflake/ BigQuery/Synapse<br /> Experts Programming in Any one - Python/ Scala/Java (Python preferred)<br /> Design, Test-driven development, code review and implement<br />CICD using Github/ Gitlab/Docker<br /> Good understanding of ETL/ELT technology and processes<br /> Basic knowledge of Apache Airflow would be a plus<br /> Basic knowledge of Data Modeling tools (dbt, dataform, Alteryx,<br />Informatic,etc) would be a plus<br /></p><p>Big Plus<br />Experience in ML ops and tools for Model Reproducibility, Deployment, packaging,<br />monitoring and Model retraining.<br />Experience in Lakehouse Architecture using Databricks.</p>",Data Engineer,1500000,2500000,https://www.naukri.com/job-listings-data-engineer-coindcx-bangalore-bengaluru-3-to-5-years-081222904499,"['Java', 'Azure', 'Data Engineering', 'BigQuery', 'GCP', 'GitHub', 'Snowflake', 'Data Lake', 'Spark', 'Databricks', 'AWS']",['Bengaluru'],Data Engineer,2022-12-08 10:25:07
081222503254,"<p> </p> <ul> <li> 5 years of experience delivering solutions utilizing the entire Microsoft BI (SSIS,SQL) </li> <li> 5 years of experience working in a data warehouse environment and a strong understanding of dimensional data modelling concepts </li> <li> Additional knowledge of Informatica, AWS Redshift & Tableau is additional advantage </li> <li> Must be able to build Business Intelligence solutions in a collaborative, agile development environment. </li> <li> Working knowledge of GitHub is advantage. </li> <li> Demonstrated ability to use discretion and make self-decisions to solve complex design issues. </li> <li> Ability to work with a minimal amount of direction while being pro-active in keeping their management informed of project related issues </li> <li> Ability to manage multiple projects at the same time Support activities and New development activities handled parallel. </li> <li> Experience working on an Agile/Scrum team preferred </li> <li> Excellent written and verbal communication skills </li> <li> Strong aspiration to learn new technologies </li> <li> Ability to maintain confidentiality </li> <li> Experience in the Finance domain is additional advantage. </li> </ul> <div> <div> <div> <strong> Key Skills : </strong> <ul> <li> Data Engineer  </li> <li> Sql  </li> <li> Ssis  </li> <li> Data Warehousing  </li> </ul> </div> </div> </div> <div> <div> <div> <div> </div> </div> </div> </div>",Data Engineer,200000,600000,https://www.naukri.com/job-listings-data-engineer-mm-staffing-career-consultants-private-limited-bangalore-bengaluru-5-to-10-years-081222503254,"['github', 'Agile scrum', 'Data modeling', 'Agile development', 'Informatica', 'SSIS', 'Business intelligence', 'microsoft', 'Data warehousing', 'SQL']",['Bengaluru'],Data Engineer,2022-12-08 21:55:48
061222007227,"<p> </p><p><strong>What youll discover</strong></p><ul><li>Inclusive culture and career growth opportunities </li><li>A truly Global IT Organization that collaborates across North America, Europe, Asia and Australia, click here to learn more </li><li>Challenging, collaborative, and team-based environment <br /><br /> </li></ul><p><strong>What youll do</strong></p><p>The Global Merchandising Solutions Team is responsible for managing various merchandising, stores and distribution related solutions within TJX IT. The organization delivers capabilities that enrich the customer experience and provide business value. We seek a motivated, talented Principal Engineer with good understanding of cloud base, database and BI concepts to help architect enterprise reporting solutions across global buying, planning and allocations. </p><p><strong>What you’ll need</strong></p><p>The Global Merchandising Solutions Team thrives on strong relationships with our business partners and working diligently to address their needs which supports TJX growth and operational stability. On this tightly knit and fast-paced solution delivery team you will be constantly challenged to stretch and think outside the box.  </p><p>You will be working with product teams, architecture and business partners to strategically plan and deliver the product features by connecting the technical and business worlds. You will need to break down complex problems into steps that drive product development while keeping product quality and security as the priority. You will be responsible for most architecture, design and technical decisions within assigned scope. In addition, you will be actively coaching and mentoring engineers and developers in the product teams to help identify and resolve issues with technology and product processes. </p><p><strong>Minimum Qualifications </strong></p><ul><li>Bachelor’s Degree or equivalent Engineering skillset / training / work experience in relevant technical domain </li><li>10+ years of strong development experience with large data warehouse solutions with Power BI capabilities while working in an Agile (Scrum/Kanban/SAFe) environment</li><li>Experience in Snowflake databases and Power BI solutions</li><li>Hands-on experience in leading and delivering large scale projects end to end whilst maintaining good understanding of Coding standards, Performance tuning and database concepts</li><li>Demonstrated leadership in the fields of data warehousing, database or data science</li><li>Strong communication and influence skills, to explain DevOps Processes with customers & management. Solid team player with mentorship skills</li><li>Ability to understand the work environment and competing priorities in conjunction with developing/meeting project goals</li><li>Shows a positive, open-minded and can-do attitude</li></ul><p><strong>Preferred Qualifications </strong></p><ul><li>Technical lead with strong automation and engineering mindset</li><li>Experience in building ETL solutions (Talend preferred) in Cloud</li><li>Experience with Snowflake database</li><li>Understanding of Data Modelling</li><li>Experience in programing languages like Python, Shell Scripting and Scala.</li><li>Experience in Linux, Windows and Spark platforms</li><li>Experience working in Agile team</li><li>Good communication skills and team work focus mindset.</li><li>Current with industry trends, IT Ops and industry best practices and able to identify the ones we should implement</li><li>Experience with tools such as JIRA, Jenkins, Ansible Tower, Service NOW</li><li>Knowledge of IT Security & Compliance</li><li>Experience with Oracle Exadata would be plus</li></ul>",Principal Data Engineer - ETL | TJX,6000000,10000000,https://www.naukri.com/job-listings-principal-data-engineer-etl-tjx-talent500-hyderabad-secunderabad-chennai-bangalore-bengaluru-10-to-20-years-061222007227,['ETL'],"['Chennai', 'Bengaluru', 'Hyderabad']",Data Engineer,2022-12-06 14:06:18
051222004537,"<p> </p><ul><li> 7 years of experience in software design and development with 4 years of experience in the data engineering field is preferred</li><li>Act as a subject matter expert in data engineering and <strong>GCP data technologies.</strong></li><li>Work with client teams to design and implement modern, scalable data solutions using a range of new and emerging technologies from the Google Cloud Platform.</li><li>Be required to showcase your <strong>GCP Data engineering experience</strong> when communicating with clients on their requirements, turning these into technical data solutions.</li><li>Be required to build and deliver Data solutions using GCP products and offerings.</li><li> Hands-on experience in GCP cloud data implementation suite such as <strong>Big Query, Pub Sub, Data Flow/Apache Beam, Airflow/Composer, Cloud Storage</strong>, etc.</li><li> Strong experience and understanding of very large-scale data architecture, solutioning, and operationalization of data warehouses, data lakes, and analytics platforms.</li><li> Mandatory 1 year of software development skills using Java</li><li> Extensive hands-on experience working with data using <strong>SQL and Python</strong></li></ul><br /><br />",GCP Data Engineer,2000000,3500000,https://www.naukri.com/job-listings-gcp-data-engineer-latentview-chennai-7-to-12-years-051222004537,[],['Chennai'],Data Engineer,2022-12-05 15:35:10
171122011649,<p><strong>Roles and Responsibilities</strong> </p><br /><br /><p><strong>Desired Candidate Profile</strong> </p><br /><br /><p><strong>Perks and Benefits</strong> </p><br /><br />,Data Engineer - Azure & Data Warehousing,1500000,2500000,https://www.naukri.com/job-listings-data-engineer-azure-data-warehousing-v-soft-consulting-corporation-private-limited-hyderabad-secunderabad-bangalore-bengaluru-6-to-8-years-171122011649,"['ETL', 'SQL']","['Bengaluru', 'Hyderabad']",Data Engineer,2022-12-07 15:01:44
071222501253,"<div> <div> <b> </b>   </div> <div> <ul> <li> <p> Business requirements analysis  </p> </li> </ul> <ul> <li> <p> Work in an agile environment, primarily in Snowflake  </p> </li> </ul> <ul> <li> <p> Work to integrate, classify and report on marketing and digital commerce effectiveness  </p> </li> </ul> <ul> <li> <p> Technical assessment of Data availability and quality from multiple data source types that include Oracle EBS, Salesforce, MarTech systems (Adobe, Google analytics, Google ads, Marketo), Snowflake  </p> </li> </ul> <ul> <li> <p> Work with multiple business functions, while primarily in marketing but closely work with the Data Analytics team that is responsible for developing products using the latest technologies such as tools from Amazon Web Services, Boomi Flow, Snowflake, Qlik, Power BI and other Data Analytics solutions.  </p> </li> </ul> <p> </p> <p> <b> Who you are  </b> </p> <ul> <li> <p> Degree in Information Systems, Computer Science, or related technical discipline or equivalent Analytic, creative, and business focused problem solver  </p> </li> </ul> <ul> <li> <p> Ability to explain issues and resolutions to technical and non-technical staff  </p> </li> </ul> <ul> <li> <p> Ability to execute multiple projects simultaneously  </p> </li> </ul> <ul> <li> <p> Experience with SQL, advanced SQL, cloud data platforms or Snowflak  </p> </li> </ul> <ul> <li> <p> Experience with Cloud tools on AWS will be an asset  </p> </li> </ul> <ul> <li> <p> Experience with RunMyJobs or Terraform will be an asset  </p> </li> </ul> <ul> <li> <p> Experience with Python will be an asset  </p> </li> </ul> <p> <b> Qualifications  </b> </p> <ul> <li> Advanced proficiency in SQL, APIs, Excel, Salesforce and statistical tools  </li> <li> Experience in conducting database analytics / data-driven marketing with solid understanding of marketing automation concepts and practices  </li> <li> Strong attention to detail  </li> <li> Experience in business analytics, business intelligence (BI) or comparable analyst position handling large, complex data sets  </li> </ul> <p> <b> Responsibilities  </b> </p> <ul> <li> You will drive measurement, analysis, and testing initiatives supporting global and regional marketing programs  </li> <li> Provide timely output of various ad-hoc analyses and processes in support of multiple departments  </li> <li> Performs data mining using SQL query skills, MicroStrategy and workflow designers  </li> <li> Identify, analyze, and interpret trends or patterns in complex data sets  </li> <li> Assist in analytics integrations and ensure accurate data collection  </li> <li> Program and automate information and delivery through data warehouse channels  </li> </ul> </div> </div>",Data Engineer,300000,700000,https://www.naukri.com/job-listings-data-engineer-cytiva-bangalore-bengaluru-1-to-4-years-071222501253,"['Google Analytics', 'Business analytics', 'Data collection', 'Workflow', 'Life sciences', 'Data mining', 'Business intelligence', 'Adobe', 'SQL', 'Salesforce']",['Bengaluru'],Data Engineer,2022-12-07 17:48:51
101222500380,"<div> <ul> <li> Use your in-depth understanding to architect and optimise databases and data ingestion pipelines  </li> </ul> <ul> <li> Develop HA strategies, including replica sets and sharding to for highly available clusters  </li> </ul> <ul> <li> Recommend and implement solutions to improve performance, resource consumption, and resiliency  </li> </ul> <ul> <li> On an ongoing basis, identify bottlenecks in databases in development and production environments and propose solutions  </li> </ul> <ul> <li> Help DevOps team with your deep knowledge in the area of database performance, scaling, tuning, migration & version upgrades  </li> </ul> <ul> <li> Provide verifiable technical solutions to support operations at scale and with high availability  </li> </ul> <ul> <li> Recommend appropriate data processing toolset and big data ecosystems to adopt  </li> </ul> <ul> <li> Design and scale databases and pipelines across multiple physical locations on cloud  </li> </ul> <ul> <li> Conduct Root-cause analysis of data issues  </li> </ul> <ul> <li> Be self-driven, constantly research and suggest latest technologies  </li> </ul> <strong> Requirements  </strong> <ul> <li> Engineering degree in Computer Science or related field  </li> </ul> <ul> <li> 10+ years of experience working with databases, most of which should have been around NoSql technologies  </li> </ul> <ul> <li> Expertise in implementing and maintaining distributed, Big data pipelines and ETL processes  </li> </ul> <ul> <li> Solid experience in one of the following cloud-native data platforms (AWS Redshift/ Google BigQuery/ SnowFlake)  </li> </ul> <ul> <li> Exposure to real time processing techniques like Apache Kafka and CDC tools (Debezium, Qlik Replicate)  </li> </ul> <ul> <li> Strong experience in Linux Operating System  </li> </ul> <ul> <li> Solid knowledge of database concepts MongoDB, SQL, and NoSql internals  </li> </ul> <ul> <li> Experience with backup and recovery for production and non-production environments  </li> </ul> <ul> <li> Experience in security principles and its implementation  </li> </ul> <ul> <li> Exceptionally passionate about always keeping the product quality bar at an extremely high level  </li> </ul> <ul> <li> Nice-to-haves  </li> </ul> <ul> <li> Proficient with one or more of Python/Node.Js/Java/similar languages  </li> </ul> </div>",Big Data Engineer,800000,1300000,https://www.naukri.com/job-listings-big-data-engineer-ushur-bangalore-bengaluru-5-to-12-years-101222500380,"['Linux', 'Machine learning', 'Workflow', 'Healthcare', 'Data processing', 'Apache', 'Financial services', 'SQL', 'Python']",['Bengaluru'],Data Engineer,2022-12-10 18:26:43
171122010479,<p><strong>Roles and Responsibilities</strong> </p><br /><br /><p><strong>Desired Candidate Profile</strong> </p><br /><br /><p><strong>Perks and Benefits</strong> </p><br /><br />,Senior Data Engineer,1500000,2500000,https://www.naukri.com/job-listings-senior-data-engineer-v-soft-consulting-corporation-private-limited-kolkata-hyderabad-secunderabad-pune-ahmedabad-chennai-bangalore-bengaluru-delhi-ncr-mumbai-all-areas-5-to-7-years-171122010479,"['python', 'CI & CD', 'Hadoop', 'Spark']","['Chennai', 'Pune', 'Delhi NCR', 'Ahmedabad', 'Bengaluru', 'Hyderabad', 'Kolkata', 'Mumbai (All Areas)']",Data Engineer,2022-12-07 15:01:41
111222001525,"<p> </p><p><strong><u>Bigdata (Python Spark)</u></strong> </p><p>Core Skills: </p><ul><li>3+ years      of professional development experience in <strong>Bigdata, Spark, Python</strong></li><li>Strong      development skills in Python</li><li>Experience      working in big data technologies Spark <strong>(preferably pyspark</strong>), Hive etc and a Good Understanding of Distributed Systems</li><li>Strong,      hands-on experience in solving & debugging complex technical problems</li><li>Good      Understanding of Datawarehouse/Data Lake, relational & non-relational databases technologies</li><li>Development      with high standards in software design, coding, code reviews, tests and      automation</li><li>Desire to      learn & adopt new technologies</li></ul><p>Good to Have Skills: </p><ul><li>Experience/Knowledge      about AI/ML MLOps or building Machine Learning Systems will be a big      advantage</li><li>Exposure      to Cloud technologies on AWS, GCP</li><li>Exposure      to any of these airflows, MLFlow, ETL, Webservices/Microservices</li></ul><br /><br />",Data Engineer,500000,1500000,https://www.naukri.com/job-listings-data-engineer-altimetrik-hyderabad-secunderabad-chennai-bangalore-bengaluru-3-to-8-years-111222001525,[],"['Chennai', 'Bengaluru', 'Hyderabad']",Data Engineer,2022-12-11 18:43:11
301122004247,"<p><strong>Dear Candidate, </strong></p><br /><p><strong>We are hiring Data Engineer - Azure and SQL for our Gurgaon office.</strong></p><br /><p><strong>Joining time : should be able to join in Dec Month </strong></p><br /><p>  </p><p><strong>Required Knowledge & Skills:</strong></p><p>Experience: 3-6 years would be preferable. Knowledge details are:</p><ul><li>Good knowledge in <strong>Azure Data Factory</strong> and <strong>SQL Queries</strong> to building and maintain data pipelines and flows.</li><li>Solid understanding of normalization and denormalization of data, database exception handling, transactions, profiling queries, performance counters, and debugging techniques</li><li>Experience with NoSQL database to migrate data into other type of databases with real time replication.</li><li>Basic Understanding of index design and performance-tuning techniques</li><li>Writing query performance and optimizing code</li><li>Writing queries used for front-end applications (websites, desktop applications, or cloud apps)</li><li>Familiarity with SQL security techniques such as data encryption at the column level, Transparent Data Encryption (TDE), signed stored procedures, and assignment of user permissions</li><li>Designing and coding database tables to store the applications data</li><li>Data modeling to visualize database structure</li><li>Working with application developers to create optimized queries</li><li>Creating database triggers for automation, e.g., automatic email notifications</li><li>Creating table indexes to improve database performance</li><li>Programming views, stored procedures, and functions</li><li>Adhere to standards for all database e.g., Data Models, Data Architecture and Naming Conventions</li><li>Exposure to Source control like GIT</li><li>Understanding of Agile methodologies</li><li>Competence in SQL Server (latest version experience preferred). Advance PL/SQL, Stored Procedures etc. </li></ul><br /><p><strong>Key Responsibilities</strong>: </p><ul><li>Create and maintain optimal data pipeline.</li><li>Assemble large, complex data sets that meet functional / non-functional business requirements.</li><li>Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.</li><li>Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and Azure ‘big data’ technologies.</li><li>Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.</li><li>Keep our data separated and secure across national boundaries through multiple data centres and Azure regions.</li><li>Work with data and analytics experts to strive for greater functionality in our data systems.</li><li>Test databases and perform bug fixes.</li><li>Develop best practices for database design and development activities.</li><li>Ability to quickly analyse existing T-SQL code and make improvements to enhance performance, take advantage of new SQL features, close security gaps, and increase robustness and maintainability of the code.</li><li>Take on technical leadership responsibilities of database projects across various scrum teams</li></ul><br /><p>Thanks </p><p>Recruitment team</p><br />",Data engineer - Azure,1000000,1600000,https://www.naukri.com/job-listings-data-engineer-azure-cbre-noida-gurgaon-gurugram-3-to-6-years-301122004247,"['data pipeline', 'big data']","['Gurgaon', 'Noida']",Data Engineer,2022-12-03 19:17:13
021222500270,"<div> <ul> <li> <span> <span> Business solution owner, process owners and solution users and information stewards, domain guides, subject matter experts! </span> </span> </li> </ul> <ul> <li> <span> <span> Business Analysts, Data Scientists, Data Architects, Automation Architects, Automation Engineers </span> </span> </li> </ul> <ul> <li> <span> <span> Project Steering Group, Operative Steering Group (OSG), Project Management Office (PMO) and enterprise/Regional IT representatives </span> </span> </li> </ul> <ul> <li> <span> <span> Partners and vendors Acuity for business flow understanding and expertise in data preparation and pre-processing </span> </span> </li> </ul> <ul> <li> <span> <span> SQL knowledge and experience working with relational databases, query authorising (SQL) as well as a variety of other databases/date-sources. </span> </span> </li> </ul> <ul> <li> <span> <span> Big data tools like Hadoop, Spark, Kafka, BigQuery,GCPetc.  </span> </span> </li> </ul> <ul> <li> <span> <span> Data and Model pipeline and workflow management tools like Azkaban, Luigi, Airflow, Dataiku, etc. </span> </span> </li> </ul> <ul> <li> <span> <span> Stream-processing systems like Storm, Spark-Streaming, etc. </span> </span> </li> </ul> <ul> <li> <span> <span> Object-oriented/object function scripting languages like Python, Java, C++, Scala, etc. </span> </span> </li> </ul> <ul> <li> <span> <span> SQL, Python/Java, GCP Big Query (Understanding of Cloud Env), Apache Kafka, Cassandra,Postgres, Hadoop, Airflow and hands on experience on Kubernetes </span> </span> </li> </ul> <b> <span> <span> <span> <b> You will bring  </b> </span> </span> </span> </b> <br /> <ul> <li> <span> <span> Manage data solution roadmap </span> </span> </li> </ul> <ul> <li> <span> <span> Translate business requirements to data solution features and create solution roadmap options proposals. </span> </span> </li> </ul> <ul> <li> <span> <span> Convert data solution roadmap to backlog items, sprint plans and release plans. </span> </span> </li> </ul> <ul> <li> <span> <span> Participate in backlog prioritization discussions. </span> </span> </li> </ul> <ul> <li> <span> <span> Perform risk analysis and build mitigation plans. </span> </span> </li> </ul> <ul> <li> <span> <span> Create and communicate status and progress reports. </span> </span> </li> </ul> <ul> <li> <span> <span> Create resource plans for realizing solution roadmap and handle resource skills by selection process and/or trainings. </span> </span> </li> </ul> <ul> <li> <span> <span> Design & lead data solution development </span> </span> </li> </ul> <ul> <li> <span> <span> Manage compliance with analytics reference architecture by creating complaint designs and/or transition plans. </span> </span> </li> </ul> <ul> <li> <span> <span> Write test plans and descriptions for component testing. Support system test specification. </span> </span> </li> </ul> <ul> <li> <span> <span> Participate in Design review, and document approval or exemptions from design guidelines. </span> </span> </li> </ul> <ul> <li> <span> <span> Provide feedback on Reference architecture and target architecture to Data Architecture and IT teams. </span> </span> </li> </ul> <ul> <li> <span> <span> Ensure compliance of data solutions within Data Architecture guidelines </span> </span> </li> </ul> <ul> <li> <span> <span> Develop and test data solution components and create detailed data solution design. </span> </span> </li> </ul> <ul> <li> <span> <span> Write technical specifications and create solution interface or Integration specifications. </span> </span> </li> </ul> <ul> <li> <span> <span> Provide guidance on suitable options, designing, and crafting data pipeline for the analytical solution s data lake / data warehouses to specific micro services </span> </span> </li> </ul> <ul> <li> <span> <span> Develop and improve Solution components: database, schemas, data dictionary, scripts, reports, graphic user interfaces, system roles. </span> </span> </li> </ul> <ul> <li> <span> <span> Perform configuration and/or code walkthroughs, peer reviews, track, and fix defects and perform unit testing of data solution components. ensure that solution component meet acceptance criteria. </span> </span> </li> </ul> <ul> <li> <span> <span> Support and participate in acceptance procedures and tests, Data solution verification after deployment. </span> </span> </li> </ul> </div>",Data Engineer,600000,1000000,https://www.naukri.com/job-listings-data-engineer-ericsson-india-global-services-pvt-ltd-noida-2-to-7-years-021222500270,"['C++', 'Automation', 'Data management', 'Project management', 'Analytical', 'PHP', 'HTML', 'microsoft', 'CRM', 'SQL']",['Noida'],Data Engineer,2022-12-02 17:53:51
301122006235,"<p>Share resume to akshara.raju@wipro.com</p><br /><p> </p><p>GCP Data Engineer:</p><ul><li>Hands-on experience      with Dataproc(spark)/Dataflow(beam)/Datafusion and/or cloud composer</li><li>Should have good knowledge      of  GCS, BQ, Pubsub & Cloud SQL. </li><li>Proficient with SQL scripts and/or      Shell scripts</li><li>Should have programming      experience in Java/Scala or Python</li><li>Experience with the Hadoop      ecosystem and/or any of the data warehouse      appliances(Teradata/Netezza,etc) is preferred.</li><li>Traditional data engineering expertise      on ETL tools or Data modeling or BI tools is a plus </li></ul><p>GCP Lead:</p><ul><li>Good experience with      technologies mentioned in the data engineer role</li><li>Strong solution architecture exposure      with data applications, streaming architectures. Exposure to Rest APIs and      Web Apps is a plus.</li><li>GCP Infrastructure experience      with VPCs, subnets, GCE VM (MIGs), GKE, Cloud function etc..</li><li>GCP Networking experience or      knowledge including Interconnect, VPN, Peer networking etc..</li><li>GCP Security experience or knowledge including IAM,      Encryption (GKE), DLP etc..</li><li>Knowledge on dataops or devops is a      plus</li></ul><br /><br />",Gcp Data Engineer,1300000,1900000,https://www.naukri.com/job-listings-gcp-data-engineer-wipro-india-5-to-10-years-301122006235,"['Pubsub', 'Bigquery', 'Dataproc', 'Gcp Cloud']",['India'],Data Engineer,2022-11-30 11:45:45
091222005165,"<p><strong>About Welspun</strong></p><p> </p><p>Welspun Group is one of Indias fastest growing global conglomerates with businesses in Line Pipes, Home Textiles, and Infrastructure, Steel, Advanced Textiles and Flooring solutions. As globally recognized leaders in Home Textiles and Line Pipes, we have presence in over 50 countries with a strong team of 26,000+ employees. At Welspun, we strongly believe in our purpose to delight customers through innovation and technology, achieve inclusive and sustainable growth to remain eminent in all our businesses. From Homes to Highways, Hi-tech to Heavy metals, we lead tomorrow together to create a smarter and a more sustainable world. </p><br /><p><strong>Role & Responsibility</strong></p><br /><ul><li>Experienced in IT industry with strong background in Data Warehousing, Business Intelligence and ETL process <strong>SSIS and MSBI</strong>.</li><li>Good working knowledge of data warehouse techniques and practices, experience including ETL processes, dimensional data modelling (Star Schema, Snowflake Schema, FACT & Dimension Tables), OLTP and OLAP.</li><li>Hand on in loading data, troubleshooting, debugging mappings, performance tuning of Jobs and Packages and fine-tuned transformations to make them more efficient in terms of session performance.</li><li>Worked on multiple domains. Strong technical experience in SQL Server, MY SQL, SSIS & Power BI.</li><li>Extensively worked on Data Models (Conceptual, logical & physical) and ETL solutions and executed complex Data migration 300 ETL jobs migration projects.</li><li>SQL Server Automation for SQL Cluster Failover and SQL Server Job Creation for Long Running Jobs. SQL Server Consolidated Backup Report Configuration. SQL Server CPU & Memory Utilization automatic alert setup and SQL Server Disk Space monitoring alert.</li><li>Database Offline status email alert and creating & Managing SQL Logins, Users, Roles & Authentication Modes.</li><li>Adherence to Tickets & Service Requests SLAs. Quality & Customer Service and Following Agile methodology to build and deliver the project using Jira and Strong experience using SQL, PL/SQL Procedures/Functions, Triggers and Packages.</li><li>Experienced in creating various SSRS reports like drill down, drill through, paginated etc. Experienced in slicing and dicing the data using SSAS.</li><li>Effective working relationships with client team to understand support requirements, and effectively manage client expectations</li></ul><br /><p><strong>Interview Venue:</strong> Office No. 812, Block A, Navratna Corporate Park, Iscon-Ambli Road, Bopal, Ahmedabad 380058, Gujarat, India.</p><br /><p><strong>Date:</strong> 17th December 2022</p><br /><p><strong>Timing:</strong> 10:00 AM onwards</p><br />",ETL Data Engineer  (Walk In Drive),300000,650000,https://www.naukri.com/job-listings-etl-data-engineer-walk-in-drive-welspun-transformation-services-limited-ahmedabad-3-to-5-years-091222005165,['ETL'],['Ahmedabad'],Data Engineer,2022-12-09 11:46:01
071222502482,"<div> <p> </p> <div> <ul> <li> <span> <span> <span> <span> <span> <span> Experience in manipulating, transforming, and analyzing data sets that are raw, large, and complex.  </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> experience in MSBI with relevant hands-on experience in Azure Data Platform (good to have) for a minimum of 2 years  </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Demonstrates ability to plan, gather, analyze, and document user and business information.  </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Incorporates, integrates, and interfaces technical knowledge with business / systems requirements.  </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Understanding of all aspects of an implementation project including, but not limited to planning, analysis and design, configuration, development, conversions, system testing, cutover and production support.  </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Produce written deliverables for requirement specifications and support documentation: process mapping, meeting minutes, glossaries, data dictionary, technical design, system testing and implementation activities.  </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Collect and organize data, data warehouse reports, spreadsheets, and databases for analytical reporting.  </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Knowledgeable on database concepts, data modelling, stored procedures, complex query writing, performance optimization of SQL queries.  </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Understanding in developing, maintaining, publishing, and supporting dashboards using Power BI.  </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Have good understanding in SSAS cubes (Tabular/Multi-Dimensional Models) using DAX,  </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Should have good understanding of programming languages like Python/R  </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> A problem solving, solution driven mindset - with the ability to innovate within the constraints of a project - time/cost/quality.  </span> </span> </span> </span> </span> </span> </li> </ul> <p> <strong> <span> <span> <span> <span> <span> <span> Other Key Duties Responsibilities  </span> </span> </span> </span> </span> </span> </strong> </p> <ul> <li> <span> <span> <span> <span> <span> <span> Collaborate with other team members and other disciplines to deliver project requirements.  </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Work independently/in team to complete allocated activities to meet budget, timeframe and quality objectives and meeting or exceeding client expectations.  </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Actively contribute to the Data Analytics community of practice, through development of integrated solutions that embed GEC capabilities into HK core advisory  </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Manages small projects as assigned by manager; provides training and leads others as a recognized business intelligence expert.  </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> Develop effective materials for clients, making sure that their messages are clearly conveyed through the appropriate channel, using the language that is suitable for the intended audience and readers, and would induce the desired response.  </span> </span> </li> </ul> <p> </p> <p> <span> <span> <span> <strong> <span> <span> <span> Key competencies / Values:  </span> </span> </span> </strong> </span> </span> </span> </p> <ul> <li> <span> <span> <span> <span> <span> <span> Client focus - differentiating through tailored skills and depth of client understanding  </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Excellence - striving for excellence, recognising that excellence is defined by our clients  </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Trust - building and retaining relationships of trust with our clients, colleagues and business partners  </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Teamwork - working together as a team for the Group, not just the individual  </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Responsibility - taking responsibility for our performance and our safety  </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> People - we create the environment for people to realize their potential  </span> </span> </span> </span> </span> </li> </ul> </div> </div>",Data Analyst/ Data Engineer,700000,1100000,https://www.naukri.com/job-listings-data-analyst-data-engineer-arcadis-consulting-india-pvt-ltd-noida-mumbai-2-to-4-years-071222502482,"['Data dictionary', 'System testing', 'Automation', 'Publishing', 'Production support', 'Analytical', 'Stored procedures', 'Business intelligence', 'Python', 'Data architecture']","['Mumbai', 'Noida']",Data Engineer,2022-12-07 21:03:12
071222010871,"<p><strong>Roles and Responsibilities</strong> </p><br /><ul><li>Research, develop, evaluate and optimize various computer vision and deep learning models for different problems. </li><li>Image processing and analysis</li><li>Developing computer vision models and optimizing them to meet customer requirements</li></ul><br /><br /><p><strong>Skills Required (Mandatory)</strong></p><br /><ul><li> Proficiency in Python and related packages like numpy, scikit-image, PIL, opencv, matplotlib, seaborn, etc </li><li> Experience with machine/deep learning frameworks like Tensorflow, Keras and PyTorch or <br /> similar</li><li>Development experience in Unix / Linux environment</li><li> Develop tools for algorithm development, algorithm testing, image quality validation </li><li> Experience in training models through GPU computing </li><li> Experience in using both basic and advanced image processing algorithms </li><li> Knowledge and experience with REST APIs, creating API centric microservices applications, working and connecting with API </li><li> Proficient in FastAPI and microservices standards and design patterns</li><li> Good Object-Oriented Programing & logical analysis skills in Python </li><li> Good written and verbal communication skills & proficiency in English</li></ul><br /><br /><p><strong>Preferred Skills (Good to have but not mandatory)</strong></p><p> </p><ul><li>Experience with AWS Cloud services like Lambda, EC2, RDS, S3 etc.</li><li>Experience with Docker/kubernetes and surrounding DevOps tools and related containerization technologies.</li><li>Knowledge in Redis, MongoDB and other JSON / NoSQL formats.</li><li>Proficiency with edge computing principles and architecture .</li><li>Experience in different model optimization techniques  to reduce memory usage without hindering the performance for deploying on edge devices.</li><li>Experience in Agile Application Development and Scrum methodologies to develop efficient, maintainable, readable and production-ready pipelines.</li></ul><br /><br /><p> </p><p><strong><u>Only Kolkata based candidates or candidates who are willing to relocate to Kolkata should apply. This is not a remote / WFH opportunity, it's a full time position and requires physical presence in office.  </u></strong></p><br /><p><strong><u>Candidates who can join immediately will be given preference.</u></strong></p><br />",Python Developer & Data Engineer,800000,950000,https://www.naukri.com/job-listings-python-developer-data-engineer-ispeck-digital-solutions-private-limited-kolkata-4-to-6-years-071222010871,"['tensorflow', 'Opencv', 'MySQL', 'Django Framework', 'nosql']",['Kolkata'],Data Engineer,2022-12-07 16:33:52
291122911719,"<p>Scope of Work (Objectives, Expectations, and Deliverables) :<br /><br />To support a number of 2022 initiatives, MobilityWare is looking to onboard the following roles :<br /><br />- 2 Sr. Data Engineer(s) with 7-10 years of software engineering experience and minimum 4 years working with Big data in a cloud environment<br /><br />- The engineer will work with the onshore team to design and implement big data pipelines, data warehouse solutions as well as perform code review among the team members.<br /><br />- The engineer should have proven experience on production ETL system monitoring and troubleshooting<br /><br />- Prior experience integrating with different types of data sources (API, batch, stream)<br /><br />- The engineer should have a good track on handling ambiguous requirements<br /><br />- The engineer should have experience on creating design specs, test cases and perform unit testing<br /><br />- The engineer will be required to provide test results before delivery of the product to the onshore teambusiness clients.<br /><br />- The engineer need to familiar with the github<br /><br />- Mobilityware would participate in resource selection by interviewing each proposed candidate.</p><p></p><p><br /></p><p></p><p>- Mobilityware will select candidates based on best fit for the role, technical abilities, and team fit.<br /><br />Technical Skills :<br /><br />- 7-10 years of experience in software development including Python, SQL, scala or Java<br /><br />- 5+ years on Data warehouse data model design, ETL process performance tuning<br /><br />- 5+ years experience on production system monitoring, troubleshooting using Python, SQL and airflow<br /><br />- 4+ years of experience with a public cloud AWS, Google, or Azure<br /><br />- 3+ year experience working on real-time data and streaming applications<br /><br />- 3+ years experience with Distributed datacomputing tools (Spark, PySpark)<br /><br />- 3+ years of experience with UNIXLinux including basic commands and shell scripting<br /><br />- 2+ year experience working on data job management software like Airflow, AWS Glue<br /><br />- Good to have production deployment experience on Machine Learning models training and integration with application systems.</p><p></p>",Data Engineer - AWS/Python/SQL,200000,600000,https://www.naukri.com/job-listings-data-engineer-aws-python-sql-datatobiz-kolkata-mumbai-hyderabad-secunderabad-lucknow-chennai-ahmedabad-bangalore-bengaluru-delhi-ncr-7-to-12-years-291122911719,"['Unix', 'Java', 'Azure', 'Shell Scripting', 'Scala', 'PySpark', 'data model design', 'Machine Learning', 'SQL', 'Linux', 'software engineering', 'Data Warehousing', 'Spark', 'ETL', 'Data warehouse', 'Python']","['Chennai', 'Delhi NCR', 'Lucknow', 'Mumbai', 'Ahmedabad', 'Bengaluru', 'Hyderabad', 'Kolkata']",Data Engineer,2022-11-29 17:41:24
211122003781,"<p><strong>Greeting form Infogain! </strong></p><br /><p>We are having Immediate requirement for Big Data Engineer (CloudEra)/Lead in Infogain India Pvt Ltd. Please Find the Job Description below & If you are interested please share your updated Resume with details:-</p><p>Mode of Hiring-Permanent</p><p>Experince-8-12 Yrs</p><p>Skills Required-  Cloud Era/Hortonworks, Hadoop, Hive, Hive Scripts, Talend pipeline</p><p>Notice Period- Immediate to 30 Days Max</p><p>Location- Noida/Bangalore/Pune/Mumbai (Currently work from home)</p><p><br /></p><p>Kindly share your update word formatted /pdf profile with the details below on arti.sharma@infogain.com </p><p>Total exp-</p><p>Exp in BigData</p><p>Exp in Hadoop/Hive</p><p>Exp in Cloud Era</p><p>Current CTC:</p><p>Exp CTC:</p><p>NP -</p><p>Current Location-</p><p>Preferred Location- </p><p><br /></p><p>About Infogain:</p><p>Infogain is a Silicon Valley headquartered company with expertise in software platform engineering and deep domain skills in travel, retail, insurance, automotive, and high technology. We accelerate the delivery of digital customer engagement systems using digital technologies such as cloud, microservices, robotic process automation, and artificial intelligence for our clients. </p><p>Our unique engagement approach of Listen-Curate-Deliver helps to accelerate the innovation journey of 5 of the worlds largest 50 companies and 24 of the Fortune 500, with several relationships of over 10 years. </p><p>We deliver positive business outcomes using rapid prototyping and a solid foundation of DevOps-based software platform engineering that ensure high-quality and on-time delivery. </p><p>Our 3,500 global employees across the US, UK, Singapore, Middle East and India focus on client value creation, delivery excellence and innovation. Our locations in India have 2700 employees spanning Noida, Pune, Mumbai and Bangalore.</p><p>Infogain maintains both strategic and technology partnerships with leading enterprise software providers to deliver value-added solutions. We engage with the world's largest, as well as mid-size, and startup, software providers for building product capability, product marketing, customization, professional services and post-implementation support</p><p><br /></p><br /><br /><br /><br /><br /><br /><br /><br /><br /><br />",Infogain India is hiring For Big Data Engineer (CloudEra)/Developer,2000000,3500000,https://www.naukri.com/job-listings-infogain-india-is-hiring-for-big-data-engineer-cloudera-developer-infogain-noida-pune-bangalore-bengaluru-mumbai-all-areas-8-to-10-years-211122003781,"['Bigdata', 'Hortonworks']","['Pune', 'Bengaluru', 'Noida', 'Mumbai (All Areas)']",Data Engineer,2022-12-06 18:18:25
210322003851,"<p> <strong>What will you do:</strong><br /></p><ul><li>Technical responsibility for data and data pipelines to ensure compliance with data standards, architectural standards, and achievement of documented requirements.</li><li>Develop and maintain current state documentation and deliverables for data solutions.</li><li>Maintain existing and new data solutions to ensure that they continue to meet user needs.</li><li>Create data tools for data scientist team members that assist them in building and optimizing our product into an innovative industry leader.</li><li>Work with data and analytics experts to strive for greater functionality in our data systems.</li><li>Collaborate with the Product, Engineering, and Data team to facilitate access to data.</li><li>Design, improve and manage the data warehouse and data structure in Zenius.</li></ul><br /><p><strong>Minimum Qualifications:</strong><br /></p><ul><li>Bachelor Degree in Computer Science, IT, Mathematics, or related field.</li><li>Minimum 3-5 years experience as Data Engineer, Data Warehouse, Business Intelligence.</li><li>Expert proficiency in Python, C++, Java, R, and SQL.</li><li>Advanced knowledge and experience working with complex ETL, unstructured datasets, and data warehousing tools.</li><li>Experience with cloud services such as GCP or AWS and Linux environment and storage architecture.</li><li>Experience with workflow scheduler (azkabar / airflow).</li><li>Familiar with big data and data engineering infrastructure.</li><li>Excellent analytical and problem-solving skills.</li><li>Good communication (both English & Bahasa Indonesia) and collaboration skills.</li><li>Passionate about startup, innovation, and technology.</li><li>Adaptable to dynamic, fast-paced environment and changing requirements.</li></ul><br /><br />",Senior Data Engineer,50000,50000,https://www.naukri.com/job-listings-senior-data-engineer-paidia-pte-ltd-bangalore-bengaluru-3-to-5-years-210322003851,"['c++', 'java', 'GCP', 'big data', 'business intelligence', 'sql']",['Bengaluru'],Data Engineer,2022-12-02 07:53:53
011222500248,"<div> <p> <span> <span> <b> Data Engineer/ DevOps - Enterprise Big Data Platform  </b> </span> </span> </p> <p> </p> <ul> <li> <span> <span> Proficiency in SQL / Java / Python (Python required; all 3 not necessary)  </span> </span> </li> <li> <span> <span> Proficiency in PySpark for distributed computation  </span> </span> </li> <li> <span> <span> Familiarity with Postgres and ElasticSearch  </span> </span> </li> <li> <span> <span> Familiarity with HTML, CSS, and JavaScript and basic design/visual competency  </span> </span> </li> <li> <span> <span> Familiarity with common databases (eg JDBC, mySQL, Microsoft SQL). Not all types required  </span> </span> </li> </ul> <p> <span> <span> </span> </span> </p> <p> <span> <span> This position will be project based and may work across multiple smaller projects or a single large project utilizing an agile project methodology.  </span> </span> </p> <p> <span> <span> </span> </span> </p> <p> <span> <span> <b> Roles Responsibilities:  </b> </span> </span> </p> <ul> <li> <span> <span> <span> Develop data pipelines by ingesting various data sources - structured and un-structured - into Palantir Foundry  </span> </span> </span> </li> <li> <span> <span> <span> Participate in end to end project lifecycle, from requirements analysis to go-live and operations of an application  </span> </span> </span> </li> <li> <span> <span> <span> Acts as business analyst for developing requirements for Foundry pipelines  </span> </span> </span> </li> <li> <span> <span> <span> Review code developed by other data engineers and check against platform-specific standards, cross-cutting concerns, coding and configuration standards and functional specification of the pipeline  </span> </span> </span> </li> <li> <span> <span> <span> Document technical work in a professional and transparent way. Create high quality technical documentation  </span> </span> </span> </li> <li> <span> <span> <span> Work out the best possible balance between technical feasibility and business requirements (the latter can be quite strict)  </span> </span> </span> </li> <li> <span> <span> <span> Deploy applications on Foundry platform infrastructure with clearly defined checks  </span> </span> </span> </li> <li> <span> <span> <span> Implementation of changes and bug fixes via Mercks change management framework and according to system engineering practices (additional training will be provided)  </span> </span> </span> </li> <li> <span> <span> <span> DevOps project setup following Agile principles (eg Scrum)  </span> </span> </span> </li> <li> <span> <span> <span> <span> Besides working on projects, act as third level support for critical applications; analyze and resolve complex incidents/problems. Debug problems across a full stack of Foundry and code based on Python, Pyspark, and Java  </span> </span> </span> </span> </li> <li> <span> <span> <span> Work closely with business users, data scientists/analysts to design physical data models  </span> </span> </span> </li> </ul> <p> <br /> <span> <span> </span> </span> </p> <p> <span> <span> <b> Education  </b> </span> </span> </p> <ul> <li> <span> <span> <span> <span> Bachelor (or higher) degree in Computer Science, Engineering, Mathematics, Physical Sciences or related fields  </span> </span> </span> </span> </li> </ul> <p> <span> <span> </span> </span> </p> <p> <span> <span> <span> <b> Professional Experience  </b> </span> </span> </span> </p> <ul> <li> <span> <span> <span> <span> 5 years of experience in system engineering or software development  </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> 3 years of experience in engineering with experience in ETL type work with databases and Hadoop platforms.  </span> </span> </span> </span> </li> </ul> <p> <span> <span> </span> </span> </p> <p> <span> <span> <b> Skills  </b> </span> </span> </p> <p> <span> <span> <b> Hadoop General  </b> </span> </span> </p> <p> <span> <span> Deep knowledge of distributed file system concepts, map-reduce principles and distributed computing. Knowledge of Spark and differences between Spark and Map-Reduce. Familiarity of encryption and security in a Hadoop cluster.  </span> </span> </p> <p> <span> <span> <b> Data management / data structures  </b> </span> </span> </p> <p> <span> <span> <span> Must be proficient in technical data management tasks, ie writing code to read, transform and store data  </span> </span> </span> </p> <p> <span> <span> <span> XML/JSON knowledge  </span> </span> </span> </p> <p> <span> <span> Experience working with REST APIs  </span> </span> </p> <p> <span> <span> <b> Spark  </b> </span> </span> </p> <p> <span> <span> Experience in launching spark jobs in client mode and cluster mode. Familiarity with the property settings of spark jobs and their implications to performance.  </span> </span> </p> <p> <span> <span> <b> Application Development  </b> </span> </span> </p> <p> <span> <span> Familiarity with HTML, CSS, and JavaScript and basic design/visual competency  </span> </span> </p> <p> <span> <span> <b> SCC/Git  </b> </span> </span> </p> <p> <span> <span> Must be experienced in the use of source code control systems such as Git  </span> </span> </p> <p> <span> <span> <b> ETL  </b> </span> </span> </p> <p> <span> <span> Experience with developing ELT/ETL processes with experience in loading data from enterprise sized RDBMS systems such as Oracle, DB2, MySQL, etc  </span> </span> </p> <p> <span> <span> <b> Authorization  </b> </span> </span> </p> <p> <span> <span> <span> Basic understanding of user authorization (Apache Ranger preferred)  </span> </span> </span> </p> <p> <span> <span> <b> Programming  </b> </span> </span> </p> <p> <span> <span> Must be at able to code in Python or expert in at least one high level language such as Java, C, Scala.  </span> </span> </p> <p> <span> <span> Must have experience in using REST APIs  </span> </span> </p> <p> <span> <span> <b> SQL  </b> </span> </span> </p> <p> <span> <span> Must be an expert in manipulating database data using SQL. Familiarity with views, functions, stored procedures and exception handling.  </span> </span> </p> <p> <span> <span> <b> AWS  </b> </span> </span> </p> <p> <span> <span> General knowledge of AWS Stack (EC2, S3, EBS, )  </span> </span> </p> <p> <span> <span> <b> IT Process Compliance  </b> </span> </span> </p> <p> <span> <span> <span> SDLC experience and formalized change controls  </span> </span> </span> </p> <p> <span> <span> <span> Working in DevOps teams, based on Agile principles (eg Scrum)  </span> </span> </span> </p> <p> <span> <span> ITIL knowledge (especially incident, problem and change management)  </span> </span> </p> <p> <span> <span> <b> Languages  </b> </span> </span> </p> <p> <span> <span> Fluent English skills  </span> </span> </p> <p> <span> <span> <b> Palantir Foundry  </b> </span> </span> </p> <p> <span> <span> Any additional experience knowledge is beneficial but optional  </span> </span> </p> <p> <span> <span> </span> </span> </p> <p> <span> <span> <b> Specific information related to the position:  </b> </span> </span> </p> <ul> <li> <span> <span> <span> <span> Physical presence in primary work location (Bangalore)  </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Flexible to work CEST and US EST time zones (according to team rotation plan)  </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> Willingness to travel to Germany, US and potentially other locations (as per project demand)  </span> </span> </span> </span> </li> </ul> </div>",Sr Data Engineer - PySpark,700000,1100000,https://www.naukri.com/job-listings-sr-data-engineer-pyspark-marlabs-software-pvt-ltd-bangalore-bengaluru-4-to-7-years-011222500248,"['Change management', 'Data management', 'Coding', 'Db2', 'XML', 'MySQL', 'Javascript', 'HTML', 'Oracle', 'SQL']",['Bengaluru'],Data Engineer,2022-12-01 18:21:35
301122910506,"<p>- You will be responsible for the organisation's data architecture design, maintenance and will lead GMP's data lakewarehousing efforts.<br /><br />Responsibilities :<br /><br />- Lead organisation's data lake and data warehousing efforts.<br /><br />- Design and maintain automated data pipelines working with developers.<br /><br />- Put process in place to monitor and audit data quality.<br /><br />- Evaluate various new tools, database services from time to time and recommend any upgrades to existing infrastructure.<br /><br />- Transform existing tables to increase efficiency and ease of use.<br /><br />- Work with multiple stakeholders to understand their requirement and make changes in data design to match the same.<br /><br />- Create new variables from existing data to improve data utilization.<br /><br />- Develop and grow the data engineering team.<br /><br />- Lead the data engineering by example. This a hands-on role.<br /><br />Qualifications :<br /><br />- 5+ years experience as a Data Engineer.<br /><br />- High ownership and ability to work independently.<br /><br />- Experience working with SQL, Hive, Spark, Scala and open source <br /><br />- NoSQL DBMS like MongoDB, DynamoDB.<br /><br />- Knowledge of pros and cons of various DBMS technologies and ability to identify best solutions based on need.<br /><br />- Experience working on data design and architecture problems.<br /><br />- Expertise in transforming and cleaning data.<br /><br />- Prior experience with cloud based database services and willingness to research and stay updated with new technologies.<br /><br />- Experience setting up DBMS.<br /><br />- Experience with AWS stack.<br /><br />- Comfortable with shell scripting.<br /><br />- Good communication skills.</p><p></p>",Senior Data Engineer - Spark/Hive/Scala,600000,1000000,https://www.naukri.com/job-listings-senior-data-engineer-spark-hive-scala-get-my-parking-bangalore-bengaluru-5-to-10-years-301122910506,"['Hive', 'Scala', 'DynamoDB', 'MongoDB', 'Python']",['Bengaluru'],Data Engineer,2022-11-30 17:17:38
011222500818,"<div> <ul> <li> <span> Create and maintain data pipelines and foundational datasets to support analytics and product/business needs  </span> </li> <li> <span> Collaborate closely with data science and engineering teams to improve the coverage, accuracy, and reliability of instrumentation  </span> </li> <li> <span> Develop audits for data quality at scale, implementing alerting and anomaly detection as necessary  </span> </li> <li> <span> Create scalable dashboards and reports to support business objectives and enable data-driven decision making  </span> </li> <li> <span> Contribute to shared Data Engineering tooling standards to improve the productivity and quality of output for Data Engineers across the company  </span> </li> <li> <span> Improve data quality by using improving internal tools to automatically detect issues  </span> </li> </ul> <p> <strong> Skills Experience:  </strong> </p> <p> <span> We are looking for teammates who share and practice our values: open communication, transparency, taking ownership, and high level of craftsmanship.  </span> </p> <p> <strong> Required:  </strong> </p> <ul> <li> <span> 4 years of experience building data pipelines in production with deep knowledge of performant scalable patterns  </span> </li> <li> <span> 4 years of experience in designing, developing, and maintaining robust data models from structured and unstructured sources  </span> </li> <li> <span> 4 years of experience writing accurate and effective code in SQL and Python  </span> </li> <li> <span> Experience proactively identifying opportunities to improve ETL dashboard performance and cost  </span> </li> <li> <span> Strong spoken English skills  </span> </li> <li> <span> Experience in Git/GitHub and branching methodologies, code review tools, CI tools, JIRA, Confluence, etc.  </span> </li> <li> <span> Bachelor s degree in computer science, applied mathematics, or another technical discipline from a top university  </span> </li> </ul> <p> <strong> Preferred  </strong> </p> <ul> <li> <span> Understanding and experience working in defi or centralized financial industry  </span> </li> <li> <span> Understanding and strong interest in cryptocurrencies and blockchain industry  </span> </li> <li> <span> Familiarity with Snowflake, DBT, Airflow, Looker  </span> </li> </ul> </div>",Senior Data Engineer,500000,900000,https://www.naukri.com/job-listings-senior-data-engineer-bitgo-bangalore-bengaluru-4-to-7-years-011222500818,"['Computer science', 'Financial markets', 'data science', 'Instrumentation', 'Data quality', 'JIRA', 'Analytics', 'Financial services', 'SQL', 'Python']",['Bengaluru'],Data Engineer,2022-12-01 19:44:23
011222501267,"<div> <ul> <li> A graduate with a Bachelor s Degree in Engineering Discipline </li> </ul> <ul> <li> Must have 4-8 yrs technical experience working with technologies related to various Data Analytics and Visualization tools </li> </ul> <ul> <li> Experience in development of Financial planning and reporting model in IBM Cognos TM1 or IBM PAX. </li> </ul> <ul> <li> Knowledge on TM1 architecture and components of the tool like rules, TI, TM1 reports, perspective, TM1 web PAX & PAW. </li> </ul> <ul> <li> Should be able write complex business logics in TM1 Rules and Turbointegrator (TI). </li> </ul> <ul> <li> Should have worked on various source database like SQL, flat files, Teradata and other relational databases </li> </ul> <ul> <li> Experience in Report designing in Cognos Analytics, Cognos BI and other reporting tools </li> </ul> <ul> <li> Knowledge on Cloud computing and storage is a must </li> </ul> <ul> <li> Knowledge and experience on development and support of project through Agile methodology </li> </ul> <ul> <li> Good Analytical skills and ready to take up challenge as independent resource. </li> </ul> <ul> <li> Fluency in communication </li> </ul> </div>",Senior Data Engineer,700000,1100000,https://www.naukri.com/job-listings-senior-data-engineer-concord-talent-hunt-mumbai-pune-chennai-bangalore-bengaluru-4-to-8-years-011222501267,"['Analytical skills', 'Cloud computing', 'Cognos', 'Financial planning', 'Agile methodology', 'Data analytics', 'Teradata', 'Analytics', 'Reporting tools', 'SQL']","['Chennai', 'Pune', 'Mumbai', 'Bengaluru']",Data Engineer,2022-12-01 20:20:37
290922501138,"<div> <div>   </div> <div> <div> <ul> <li> Play with and transform data. </li> <li> Work towards creating easy-to-digest analytical reports for US healthcare customers. </li> <li> Design and build interfaces that facilitate workflows between Data Activation Platform and client third party systems as scoped while complying with respective standards and industry best practices. </li> <li> Define and document best practices along with thorough message specifications. </li> <li> Monitor and tune the configuration of interfaces for high availability once deployed in production environments. </li> <li> Understand Innovaccer data warehousing concepts and implement best practices. </li> <li> Work on creating in-house predictive models to forecast medical events. </li> </ul> <strong> What You Need </strong> <ul> <li> 4 years in an analytics role in data services/product firm. </li> <li> Data modeling ability - knowledge of different data modeling concepts. </li> <li> Hands-on experience in creating statistical models; understanding when to use which model to fit the data better. </li> <li> Strong knowledge of SQL and able to write complex SQL queries. </li> <li> Exposure to Python Libraries - Numpy, Scipy, Scikit-Learn. </li> <li> Hands-on experience with BI tools such as Power BI / Tableau / Sisense. </li> <li> Self-starter, curious, accountable, enjoys a healthy level of autonomy, strong work ethic, and success in a fast-paced, high-intensity startup environment. </li> <li> Extensive experience relaying technical and non-technical information clearly and concisely. </li> <li> Demonstrated expert problem-solving and analytical skills. </li> <li> Excellent oral and written communication skills. </li> <li> Excellence in multitasking and managing multiple high-priority customer engagements at once. </li> <li> Ability to assess complex client environments and workflows and arrive at integration solutions to satisfy seamless experience between our platform and theirs. </li> <li> Ability to mentor junior team members and introduce industry expertise and best practices across integration development. </li> <li> Bachelor s degree in Engineering, Computer Science. An advanced degree in any of the areas above would be a plus. </li> </ul> Preferrefd Skills  <ul> <li> Knowledge of SQL, ETL, and PowerBI / Tableau / Sisense. </li> <li> Experience in US Healthcare  </li> </ul> </div> <div> </div> </div> </div>",Senior Data Engineer,900000,1400000,https://www.naukri.com/job-listings-senior-data-engineer-innovaccer-noida-hyderabad-secunderabad-chennai-bangalore-bengaluru-4-to-9-years-290922501138,"['Analytical skills', 'Health insurance', 'tableau', 'Data modeling', 'Subject Matter Expert', 'US healthcare', 'Analytics', 'SQL', 'Python']","['Chennai', 'Bengaluru', 'Hyderabad', 'Noida']",Data Engineer,2022-12-07 11:25:17
271122000879,"<br /><p>XZEDS is your career partner for life! We are into Career Counselling, Skill Assessment, Skilling and Placement.  </p><br /><p>We bring to you an excellent opportunity to work with a multinational company on their products/projects. T </p><br /><p>Role: Senior Data Engineer</p><br /><p>Experience:  8+ years  </p><br /><p>Work location: MG Road, Bangalore   </p><br /><p>Responsibilities:</p><ul><li>Design and build reusable      components, frameworks and libraries at scale to support analytics      products </li><li>Design and implement product      features in collaboration with business and Technology stakeholders </li><li>Anticipate, identify and solve      issues concerning data management to improve data quality</li><li>Clean, prepare and optimize      data at scale for ingestion and consumption</li><li>Drive the implementation of new      data management projects and re-structure of the current data architecture</li><li>Implement complex automated      workflows and routines using workflow scheduling tools </li><li>Build continuous integration,      test-driven development and production deployment frameworks</li><li>Drive collaborative reviews of      design, code, test plans and dataset implementation performed by other      data engineers in support of maintaining data engineering standards</li><li>Analyze and profile data for      the purpose of designing scalable solutions </li><li>Troubleshoot complex data      issues and perform root cause analysis to proactively resolve product and      operational issues</li><li>Mentor and develop other data      engineers in adopting best practices</li></ul><p>Qualifications:</p><ul><li>8 years experiencing developing      scalable Big Data applications or solutions on distributed platforms</li><li>Able to partner with others in      solving complex problems by taking a broad perspective to identify      innovative solutions</li><li>Strong skills building positive      relationships across Product and Engineering.</li><li>Able to influence and      communicate effectively, both verbally and written, with team members and      business stakeholders</li><li>Able to quickly pick up new      programming languages, technologies, and frameworks</li><li>Experience working in Agile and      Scrum development process</li><li>Experience working in a      fast-paced, results-oriented environment</li><li>Experience in Amazon Web      Services (AWS) or other cloud platform tools</li><li>Experience working with Data      warehousing tools, including Dynamo DB, SQL, Amazon Redshift, and      Snowflake</li><li>Experience architecting data      product in Streaming, Serverless and Microservices Architecture and      platform.</li><li>Experience working with Data      platforms, including EMR, Data Bricks etc</li><li>Experience working with      distributed technology tools, including Spark, Presto, Scala, Python,      Databricks, Airflow</li><li>Working knowledge of Data      warehousing, Data modelling, Governance and Data Architecture</li><li>Working knowledge of Reporting      & Analytical tools such as Tableau, Quicksite etc.</li><li>Demonstrated experience in      learning new technologies and skills</li><li>Bachelor's degree in Computer      Science, Information Systems, Business, or other relevant subject area</li></ul>",Senior Data Engineer,2000000,3000000,https://www.naukri.com/job-listings-senior-data-engineer-xzed-skills-bangalore-bengaluru-8-to-12-years-271122000879,"['Airflow', 'Github', 'Scala', 'Data Structures', 'SQL', 'Presto', 'Amazon Redshift', 'Dynamo DB', 'Snowflake', 'Python']",['Bengaluru'],Data Engineer,2022-11-27 10:56:04
291122015034,"<p> </p><p>This role will provide critical thinking and a high degree of technical development and attention to detail while being recognized as a trusted advisor to the team and stakeholders.  The characteristics include being versatile, displaying leadership qualities and enthusiasm to take on new problems across the full SDLC, ultimately assisting in moving technology forward.</p><p>This role will have varying degrees of analysis, design, development, documentation, testing and support responsibilities.</p><p><strong>Essential Experience</strong></p><ul><li>It is expected that the role holder will most likely have the following qualifications and experience</li><li>5+ years technical experience (within financial services industry preferred)</li><li>Technical Domain experience (Subject Matter Expertise in Technology or Tools)</li><li>Solid experience, knowledge and skills in Data Engineering, BI/software development such as ELT/ETL, data extraction and manipulation in Data Lake/Data Warehouse/Lake House environment.</li><li>Hands on programming experience in writing Python, SQL, Unix Shell scripts, Pyspark scripts, in a complex enterprise environment</li><li>Experience in configuration management using Ansible/Jenkins/GIT</li><li>Hands on cloud-based solution design, configuration and development experience with Azure and AWS</li><li>Hands on experience of using AWS Services - S3,EC2, EMR, SNS, SQS, Lambda functions, Redshift</li><li>Hands on experience Of building Data pipelines to ingest, transform on Databricks Delta Lake platform  from a range of data sources - Data bases, Flat files, Streaming etc..</li><li>Knowledge of Data Modelling techniques and practices used for a Data Warehouse/Data Mart application.</li><li>Quality engineering development experience (CI/CD  Jenkins, Docker)</li><li>Experience in Terraform, Kubernetes and Docker</li><li>Experience with Source Control Tools  Github or BitBucket</li><li>Exposure to relational Databases - Oracle or MS SQL or DB2 (SQL/PLSQL, Database design, Normalisation, Execution plan analysis, Index creation and maintenance, Stored Procedures) , PostGres/MySQL</li><li>Skilled in querying data from a range of data sources that store structured and unstructured data</li><li>Knowledge or understanding of Power BI (Recommended)</li></ul><p><strong>Essential capabilities</strong></p><ul><li>Enthusiasm for technology, keeping up with latest trends</li><li>Ability to articulate complex technical issues and desired outcomes of system enhancements</li><li>Proven analytical skills and evidence-based decision making</li><li>Excellent problem solving, troubleshooting & documentation skills</li><li>Strong written and verbal communication skills</li><li>Excellent collaboration and interpersonal skills</li><li>Strong delivery focus with an active approach to quality and auditability</li><li>Ability to work under pressure and excel within a fast-paced environment</li><li>Ability to self-manage tasks</li><li>Agile software development practices</li></ul><p><strong>Desired Experience</strong></p><ul><li>Hands on in SQL and its Big Data variants (Hive-QL, Snowflake ANSI, Redshift SQL)</li><li>Python and Spark and one or more of its API (PySpark, Spark SQL, Scala), Bash/Shell scripting</li><li>Experience with Source code control - GitHub, VSTS etc.</li><li>Knowledge and exposure to Big Data technologies Hadoop stack such as HDFS, Hive, Impala, Spark etc, and cloud Big Data warehouses - RedShift, Snowflake etc.</li><li>Experience with UNIX command-line tools.</li><li>Exposure to AWS technologies including EMR, Glue, Athena, Data Pipeline, Lambda, etc</li><li>Understanding and ability to translate/physicalise Data Models (Star Schema, Data Vault 2.0 etc)</li></ul><p><strong>Key Accountabilities</strong></p><ul><li>Design, develop, test, deploy, maintain and improve software</li><li>Develop flowcharts, layouts and documentation to identify requirements & solutions</li><li>Write well designed & high-quality testable code</li><li>Produce specifications and determine operational feasibility</li><li>Integrate software components into fully functional platform</li><li>Apply pro-actively & perform hands-on design and implementation of best practice CI/CD</li><li>Coaching & mentoring of other Service Team members</li><li>Develop/contribute to software verification plans and quality assurance procedures</li><li>Document and maintain software functionality</li><li>Troubleshoot, debug and upgrade existing systems, including participating in DR tests</li><li>Deploy programs and evaluate customer feedback</li><li>Contribute to team estimation for delivery and expectation management for scope.</li><li>Comply with industry standards and regulatory requirements</li></ul>",Senior Data Engineer | National Australia Bank,2500000,4000000,https://www.naukri.com/job-listings-senior-data-engineer-national-australia-bank-talent500-hyderabad-secunderabad-chennai-bangalore-bengaluru-5-to-10-years-291122015034,"['ETL', 'SQL']","['Chennai', 'Bengaluru', 'Hyderabad']",Data Engineer,2022-11-29 19:52:49
091222010799,"<p><strong>Roles and Responsibilities</strong> </p><ul><li>Deep and hands-on experience of designing, planning, productionizing, maintaining and documenting reliable and scalable data infrastructure and data products in complex environments</li><li>Development experience in one or more object-oriented programming languages (e.g.</li><li>Python, Go, Java, C++)</li><li>Advanced SQL knowledge</li><li>Experience in Bigdata technologies and Cloud.</li></ul><br /><br /><br />",Data Engineer,2000000,3500000,https://www.naukri.com/job-listings-data-engineer-master-mind-consultants-pune-8-to-13-years-091222010799,"['Hive', 'java', 'Hadoop', 'Spark', 'NiFi', 'Apache']",['Pune'],Data Engineer,2022-12-09 19:07:41
091222010500,"<p> </p><p><strong>Duties & Responsibilities</strong></p><ul><li>Building and operationalizing large scale enterprise data solutions and applications using one or more of AZURE data and analytics services in combination with custom solutions  Azure Data Factory, Azure Databricks, Azure Data Lake, Azure Blob etc</li><li>Experience in Datawarehouse implementation</li><li>Experience in migrating on-premise data warehouses to data platforms on AZURE cloud. </li><li>Designing and implementing data engineering, ingestion and transformation functions using SQL and PySpark </li><li>Capacity Planning and Performance Tuning on Azure Spark</li><li>Support data visualization development using Power BI</li><li>Exposure across all the SDLC process, including testing and deployment</li><li>Experience in relational and dimensional modelling, including big data technologies</li><li>Experience in Azure DevOps – Build CI/CD pipelines for ADF, ADLS, Databricks, Azure SQL  DB etc </li><li>Experience of working in secured Azure environments using Azure KeyVaults</li><li>Ability to interact with platform audience – Data Analysts. Data Scientists, Business Analysts etc to understand requirements and create technical design</li><li>Actively looks to stay up to date with the evolving Cloud landscape. Constantly developing technical skills using the latest cloud technology</li><li>Working on off shore office based development teams, collaborating within a team environment and participating in typical project lifecycle activities such as requirement analysis, testing and release.</li></ul><p><strong>General Responsibilities</strong></p><ul><li>Develop Azure Data skills within the team through knowledge sharing sessions, articles, etc.</li><li>Adherence to organisations Risk & Controls requirements</li><li>Participate in various initiatives/forums/groups and contribute effectively from idea creation to proto-type development. Support Strategic programs</li></ul><p><strong>Qualification</strong></p><ul><li>Graduate/Post-graduate. Preferably with specialisation in Computer Science, Statistics, Mathematics, Data Science, Engineering or related discipline</li><li>Microsoft Azure certification (good to have)</li></ul><p><strong>Experience</strong></p><ul><li>Overall 5-7 years of total experience and at least 3 years of relevant experience Azure Data Engineering</li></ul><p><strong>Required Competence / Knowledge / Skills (Mandatory)</strong></p><p><strong>Core Skillset</strong></p><ul><li>Azure Data Factory</li><li>Azure Databricks</li><li>Azure Datawarehouse</li><li>Azure Data Lake Store</li><li>Power BI (Power BI Desktop, Power BI Service)</li><li>Azure SQL DB</li><li>Azure DevOps</li><li>Understanding of Business Intelligence and Data Warehousing concepts and methods</li><li>Good communication skills</li></ul>",Azure Data Engineer,1000000,2000000,https://www.naukri.com/job-listings-azure-data-engineer-rekrut-india-mumbai-all-areas-5-to-8-years-091222010500,"['data science', 'spark', 'sql']",['Mumbai (All Areas)'],Data Engineer,2022-12-09 18:39:46
091222501179,"<ul> <li> Data analytics and reporting using BI tools like Power BI, Spotfire and develop/ automate new reports/ dashboards.  </li> <li> Propose best end to end automation solutions for developing reports and dashboards  </li> <li> Work with Global teams and Asset teams to find different dimensions of data and produce the analysis, reports insights accordingly.  </li> <li> Engagement with the Asset stakeholders prepare requirements, timelines, progress update and project closeout  </li> <li> Interpret and present data of maintenance, reliability and TA in an easy-to-understand way and build new insights from data.  </li> <li> Apply data analytics to find value added dimensions in maintenance, reliability and TA  </li> </ul> <p> <b> <u> <span> Key Challenges: -  </span> </u> </b> </p> <ul> <li> Business knowledge and operational skills on activities at the Upstream, Downstream and IG operating units. Includes specific methods, software and hardware.  </li> <li> Confidence to work in virtual environment and managing engagement across different levels of stakeholders. Includes strong proficiency in using English for both spoken and written communication.  </li> <li> Shift work following operating hours of the upstream, downstream and IG operation units of shell  </li> <li> Sound understanding of business/process workflow and having mature mindset to deal tough/challenging situations.  </li> <li> A self-starter leader and reliable deliverer, with very good verbal and written skills in English.  </li> <li> Exposes the individual to all cultural backgrounds and organizational levels across the globe, across diverse time-zones.  </li> </ul> <p> <strong> Requirements  </strong> </p> <p> <b> <span> Mandatory Education/Experience:  </span> </b> </p> <ul> <li> <b> <span> </span> </b> University Degree in Mechanical/ Industrial/  <span> Production/Petroleum/ </span> </li> <li> <span> </span> <span> Instrumentation/Electrical/  </span> Chemical Engineering.  </li> <li> Required 2+ yrs. of maintenance work experience in Oil Gas/ </li> <li> Petrochemicals/ Heavy Industries/Power Plants/Steel Plants/Fertilizer </li> <li> Plants or any equivalent industry.  </li> <li> Skill/Proficiency in data analytics tools Spotfire, Power BI, R, Python, and concepts of data analytics are must.  </li> <li> Skill/Proficiency in SAP PM module is must.  </li> <li> Skill/Proficiency in AMS processes ME, MEC, MTO and TA are must  </li> </ul> <p> <b> <span> Desirables/Added Value: -  </span> </b> </p> <ul> <li> <b> <span> </span> </b> Effective communication skill and stakeholder management is a necessary skill for the job.  </li> </ul> <p> <b> <span> Other Skills, Competence and Behavior:  </span> </b> </p> <ul> <li> <b> <span> </span> </b> Developed engagement and communication skills.  </li> <li> Knowledge or higher-level awareness of the context of use of technical data and documentation within technical data processes,  </li> <li> Ability to deal effectively with complexity and detail.  </li> <li> Ability to prioritize and ensure delivery of priorities for the area of responsibility.  </li> <li> Flexible and adaptable to change, with track record of demonstrating initiative, analytical capabilities and problem solving </li> </ul>",Process Data Engineer,1000000,1400000,https://www.naukri.com/job-listings-process-data-engineer-shell-pvt-ltd-chennai-2-to-5-years-091222501179,"['Business process', 'Automation', 'Analytical', 'Workflow', 'Instrumentation', 'Stakeholder management', 'Operations', 'Principal', 'Technical support', 'Recruitment']",['Chennai'],Data Engineer,2022-12-09 19:15:28
091222501178,"<ul> <li> Data analytics and reporting using BI tools like Power BI, Spotfire and develop/ automate new reports/ dashboards.  </li> <li> Propose best end to end automation solutions for developing reports and dashboards  </li> <li> Work with Global teams and Asset teams to find different dimensions of data and produce the analysis, reports insights accordingly.  </li> <li> Engagement with the Asset stakeholders prepare requirements, timelines, progress update and project closeout  </li> <li> Interpret and present data of maintenance, reliability and TA in an easy to understand way and build new insights from data.  </li> <li> Apply data analytics to find value added dimensions in maintenance, reliability and TA  </li> </ul> <p> <b> <u> <span> Key Challenges: -  </span> </u> </b> </p> <ul> <li> Business knowledge and operational skills on activities at the Upstream, Downstream and IG operating units. Includes specific methods, software and hardware.  </li> <li> Confidence to work in virtual environment and managing engagement across different levels of stakeholders. Includes strong proficiency in using English for both spoken and written communication.  </li> <li> Shift work following operating hours of the upstream, downstream and IG operation units of shell  </li> <li> Sound understanding of business/process workflow and having mature mindset to deal tough/challenging situations.  </li> <li> A self-starter leader and reliable deliverer, with very good verbal and written skills in English.  </li> <li> Exposes the individual to all cultural backgrounds and organizational levels across the globe, across diverse time-zones.  </li> </ul> <p> <strong> Requirements  </strong> </p> <p> <b> <span> Mandatory Education/Experience:  </span> </b> </p> <ul> <li> <b> <span> </span> </b> University Degree in Mechanical/ Industrial/  <span> Production/Petroleum/ </span> </li> <li> <span> </span> <span> Instrumentation/Electrical/  </span> Chemical Engineering.  </li> <li> Required 2 to 4+ yrs. of maintenance work experience in Oil Gas/ </li> <li> Petrochemicals/ Heavy Industries/Power Plants/Steel Plants/Fertilizer Plants or any equivalent industry.  </li> <li> Skill/Proficiency in data analytics tools Spotfire, Power BI , R, Python, SQL, Alteryx and concepts of data analytics are must.  </li> <li> Skill/Proficiency in SAP MM/PM module is must.  </li> <li> Skill/Proficiency in AMS processes ME,MEC,MTO and TA are must </li> </ul> <p> <b> <span> Desirables/Added Value: -  </span> </b> </p> <ul> <li> <b> <span> </span> </b> Effective communication skill and stakeholder management is a necessary skill for the job.  </li> </ul> <p> <b> <span> Other Skills, Competence and Behavior:  </span> </b> </p> <ul> <li> <b> <span> </span> </b> Developed engagement and communication skills.  </li> <li> Knowledge or higher-level awareness of the context of use of technical data and documentation within technical data processes,  </li> <li> Ability to deal effectively with complexity and detail.  </li> <li> Ability to prioritize and ensure delivery of priorities for the area of responsibility.  </li> <li> Flexible and adaptable to change, with track record of demonstrating initiative, analytical capabilities and problem solving </li> </ul>",Process Data Engineer,1000000,1400000,https://www.naukri.com/job-listings-process-data-engineer-shell-pvt-ltd-chennai-2-to-4-years-091222501178,"['Business process', 'Automation', 'Analytical', 'Workflow', 'Instrumentation', 'Operations', 'Principal', 'Technical support', 'Recruitment', 'SQL']",['Chennai'],Data Engineer,2022-12-09 19:15:28
091222501177,"<ul> <li> <span> The new SEAM organization integrates Safety, Environment Asset Management activities, with a broad geographical footprint, that will support Shell s business assets around the world.  </span> </li> <li> <span> </span> <span> The vision of SEAM is to provide capability across the spectrum of Safety, Environment and Asset Management with:  </span> </li> <li> <span> Shaping the future ways of working through introducing to the business new technology and news way of working, including eg digital, SBO, and real-time, data-driven, end-to-end optimization and risk management, but also global programmes like Human Performance Based Safety Philosophy.  </span> </li> <li> <span> Providing performance feedback driving disciplined execution to deliver reliable, predictable results  </span> </li> <li> <span> Helping the businesses build their improvement plans and provide support to execute them  </span> </li> <li> <span> Sustaining performance through strong core capabilities (internal, including SBO contingent) through building expertise and a company-wide consistent approach  </span> </li> <li> <span> Technology is critical for Shell s success. Almost everything we do for our customers and partners is enabled by technology in some way, and where we differentiate ourselves versus our competitors is often because of technology.  </span> </li> <li> <span> Technology opens doors to create business value: growing the top line by accessing new resources, products and customers, growing the bottom line by reducing cost of doing business, growing the return on investment by reducing capital intensity.  </span> </li> <li> <span> </span> <span> The Insights Improvement / Operations Support team plays an important role in driving Technology Deployment and Replication across RDS with accelerated deployment and replication of technologies developed in-house as well as uptake of 3rd party technologies.  </span> </li> <li> <span> This position sits within the Technical Asset Operations organization within SEAM but will provide direct support to the Insights Improvement / Operations Support team.  </span> </li> <li> <span> </span> This role will reside within the VP TAO organization that will be accountable for maximizing integrated business value across the organization.  </li> <li> Technical Asset Operations (TAO) is a key enabler for the accelerated delivery of Shell s Asset Management System and will help us to reach our ultimate potential in Downstream Manufacturing, Integrated Gas and Upstream.  </li> <li> TAO provides high quality and cost-competitive technical resources who are not physically present at site yet are an integral part of asset teams delivering value through end-to-end AMS work processes.  </li> </ul> <p> <strong> Purpose Accountabilities </strong> </p> <ul> <li> Support the TRT lead in managing and monitoring technology replications in SEAM  </li> <li> Apply data analytics on the Global Technology Register to identify trends, gaps and opportunities for replication of technologies  </li> <li> Custodian of the Global Performance Dashboard on the Technology Deployment side in visualizing the Technology Deployment space (aka the Quilt ) and generating dashboards for key business stakeholders  </li> <li> Support GM Ops staff and TRT bundle leads in keeping the GTR tracker up to date  </li> <li> Support SEAM in raising awareness of value adding technologies for further replication through preparation/support via regular highlight communications and or Yammer posts  </li> <li> Work closely with the GTR/GTC custodianship in PTX on GTR/GTC integration, Fit4 alignment and overall data quality control  </li> <li> Ensure deployment opportunities and benefits are captured within the Global Technology Register and work seamlessly with the TRT leads for Wells and Subsurface to drive the uptake of the appropriate technologies  </li> <li> Liaise with TAO OMs and ASMs to analyze / assess GTP closure - technology supported opportunities via pro-active advisory role  </li> </ul> <p> </p> <p> <strong> Dimensions  </strong> </p> <ul> <li> Working within the TAO MEC team - with close connection to all OMs within TAO, ASMs and the Operations Support team to support gap closure via robust deployment of technology.  </li> </ul> <p> <strong> Education  </strong> </p> <ul> <li> Technical background with engineering degree (Mechanical / Electrical / Instrumentation Control Automation/Computer Science/ IT )  </li> </ul> <p> </p> <p> <i> </i> <strong> Skills Requirements </strong> </p> <ul> <li> Strong aptitude for Learner Mindset  </li> <li> Minimum of 1 to 2 years experience in relevant development experience with visualization and automation tools like Power BI, Power App, Power Automate.  </li> <li> Experience of working on Salesforce Platform is desirable.  </li> <li> Strong data base management skillset and associated data analysis  </li> <li> Ability to engage and effectively communicate at all levels inside and outside Shell and within different cultural settings  </li> <li> Strong interpersonal skills, ability to challenge and ability to build internal and external relationships based on trust and to integrate across multiple functions.  </li> <li> This is a global role with significant elements of working through others, integrating across internal and external interfaces, delivering without direct authority. As such it requires pronounced skills in the areas of:  </li> <li> Working through others  </li> <li> Building trusted relationships  </li> <li> Integrating across internal and external interfaces  </li> <li> Competitive applicants will have a proven track record of delivery  </li> </ul>",Process Data Engineer,900000,1300000,https://www.naukri.com/job-listings-process-data-engineer-shell-pvt-ltd-chennai-1-to-2-years-091222501177,"['Data analysis', 'Data quality', 'Manager Quality Control', 'Asset management', 'Risk management', 'Technical support', 'Monitoring', 'Recruitment', 'Salesforce']",['Chennai'],Data Engineer,2022-12-09 19:15:28
091222501176,"<div> <ul> <li> Bachelor of Engineering or equivalent with minimum 2-5 years of experience in Project Controls in Oil and Gas Industry with sound knowledge on Project Control Processes and tools and Management Information System (MIS) reporting (Upstream, Downstream, Integrated gas etc).  </li> <li> Having hands on working experience in Project Controls on Oil and gas Capital projects of mid-size/mega projects /portfolio projects with sound knowledge on Processes, best practices and terminologies used in Oil and gas Project controls  </li> <li> Proficient in using Project control tools like Primavera P6, Ecosys, SAP etc  </li> <li> Proficient in MS Office - Microsoft Excel, Power Point and Microsoft Visualization tool - MS Power BI.  </li> <li> Flair of working with Tools and Data and showcase understanding of processes and apply in working ways of tools and Data management  </li> <li> Strong analytical and problem-solving skills  </li> <li> Strong Communication Skills both verbal and in writing and Strong Stakeholder management skills  </li> <li> Individual contributor role - Mature, proactive, resourceful, and willing to work in staggered hours to cater to global time zones and global customer base. Independent and able to work well with minimal supervision  </li> </ul> </div>",Process Data Engineer,1000000,1400000,https://www.naukri.com/job-listings-process-data-engineer-shell-pvt-ltd-chennai-2-to-5-years-091222501176,"['SAP', 'Data management', 'HP data protector', 'Project control', 'Analytical', 'MS Office', 'Stakeholder management', 'Technical support', 'Downstream', 'Recruitment']",['Chennai'],Data Engineer,2022-12-09 19:15:28
091222501175,"<div> <p> </p><ul> <li> The key accountability of the Process Engineer is providing support of the technical content for their respective technology or theme area </li> <li> Performs technical monitoring of the process technology against the safe operating envelope and limits for the process unit(s) that reside within their technology or theme area base </li> <li> Audits and supports technical content development and other unit performance KPI s to assure the operation is compliant </li> <li> The purpose of this activity is to understand unit and equipment performance, identify threats, troubleshoot performance and reliability gaps, and identify opportunities for improvement </li> <li> This activity feeds into multiple deliverables </li> <li> Provides inputs and guidance to the OSE s/CSE s assigned to the physical assets to ensure their assigned assets are operated in a safe, environmentally sound and reliable manner in accordance with the license to operate and the defined safe operating envelope </li> <li> Maintains up to date technology specific knowledge through LFI, technical networks etc so as to be able to proactively identify threats to the units or business coming from the wider Shell organization or industry knowledge and practices </li> <li> Identify opportunities to improve unit capacity, yield, availability and utilization in support of the site and business plans </li> <li> Participates in technology forums, conferences to share learning and maintain and improve competencies in their specialty </li> <li> Maintains and help s develop training materials and guidance manuals designed for Engineers in conjunction with PT and Learning </li> </ul> <p> </p> <p> </p> <p> <u> <strong> Specific activities can include (but not limited to): </strong> </u> </p> <ul> <li> Generate and review unit KPI dashboards, and to raise awareness on, create response plans and discuss risks as part of the continuous unit proactive monitoring.  </li> <li> Identifies and discusses Threats Opportunities  </li> <li> Collaborate to build monthly reports which capture short, medium and long term perspectives for future learning.  </li> <li> Actively participates in the ESP Process through leading Alarm Review meetings and engaging in Console Review meetings with Production PTMs and sharing unit status updates / escalating issues in daily unit PE Huddle meetings.  </li> <li> Assists Production and Maintenance with troubleshooting process issues and may escalate to UE for additional support.  </li> <li> Owns, develops, selects and implements solutions to operational and / or customer centric risks and opportunities for smaller scope using OPEX related MOCs.  </li> <li> Provides monthly, quarterly and annual input for production accounting and regulatory reporting (i.e. Solomon, yield reporting, flaring, chemical costs, etc).  </li> <li> Identifying violations of the Operating Window, IOW and DOW  </li> <li> Developing of proposals to address identified immediate threats  </li> <li> Detailed root-cause analysis of abnormalities  </li> <li> Developing of proposals to optimize units  </li> </ul> <p> </p> <p> <strong> <u> Skills Requirements: </u> </strong> </p> <ul> <li> Professional qualifications: Bachelor s or Master s degree in Chemical engineering  </li> <li> Minimum 2 years of experience in a chemical complex or a refining facility.  </li> <li> Knowledge of one or more of the following technologies: Hydrocracking, Hydrotreating, Gas processing, Distillation, Catalytic cracking, Alkylation, Platforming.  </li> <li> Ability to analyse, troubleshoot, and resolve asset problems with maintenance and operations.  </li> <li> Utilization of problem-solving techniques to resolve design engineering issues.  </li> <li> Knowledge of integration between process units  </li> <li> Experience using industry standard process simulation software  </li> <li> Problem solving skills - knowledge of engineering Standards and best practices.  </li> <li> Understanding fundamentals of flow measurement technology, basic mass and energy balance  </li> <li> Experience in data quality assurance checks, troubleshooting and statistics  </li> <li> FIM investigation, MOC knowledge.  </li> <li> Self-starter and an independent worker capable of collaborating in a team environment with an ability to build strong effective relationships with asset production teams while working remotely  </li> <li> Ability to articulate and defend a case for change or improvement.  </li> <li> Proven collaboration competencies  </li> <li> Ability to prioritize, manage multiple tasks, and demonstrate strong written and oral communication skills  </li> <li> Good organization, documentation, and follow-up.  </li> <li> Time management skills.  </li> </ul> </div>",Process Data Engineer,1000000,1400000,https://www.naukri.com/job-listings-process-data-engineer-shell-pvt-ltd-chennai-2-to-4-years-091222501175,"['OPEX', 'Manager Quality Assurance', 'Manager Technology', 'Process safety', 'Data quality', 'Operations', 'Technical support', 'Monitoring', 'Performance improvement', 'Recruitment']",['Chennai'],Data Engineer,2022-12-09 19:15:27
091222501174,"<div> <ul> <li> Collect estimating benchmarking data (CEC metrics) from approved estimates type 1, 2, 3 in the lookback database.  </li> <li> Analyze (via the lookback database) and determine the suitability of the estimate for a subsequent benchmarking with the IPA SPT web tool.  </li> <li> Create and enter projects cost and schedule data in the IPA SPT web tool to obtain benchmarking indicators for cost and schedule performance; for all stages of a project (i.e., DG3, DG4, Closeout)  </li> <li> Keep current the lookback database with the corresponding IPA SPT benchmarking results for those suitable projects; for all stages of a project (i.e., at DG3, DG4, Closeout)  </li> <li> Collect actual cost data for completed projects and run the IPA CEC metrics to determine performance indicators.  </li> <li> Ensure that the PowerBI dashboards are properly visualizing latest data from the lookback database.  </li> <li> For selected projects, complete the IPA detailed cost form and run the IPA CEC Detailed Cost Metrics Tool to obtain benchmarking metrics; and produce a report to the project team.  </li> <li> Facilitate the lookback analysis of completed projects by utilizing latest updated lookback database and conducting discussion meetings with cost engineering and estimating teams on a quarterly basis.  </li> <li> Support external IPA benchmarking evaluations on selected projects; facilitate the data collection process (including completion of detailed cost forms) and proper cost allocations (in conjunction with the cost engineers).  </li> <li> Support the Site IPA Benchmarking exercise (every 3 years) by ensuring adequate data collection from the portfolio of projects and facilitate the selection of the sample projects to be benchmarked.  </li> <li> Utilize analytics systems and tools such as Power BI for data presentation, aggregation, disaggregation, visualization, and analytics to identify patterns and interactions within benchmarking performance data across multiple portfolios of projects.  </li> <li> Create and maintain recurring reporting, supporting the interpretation, and troubleshooting of the underlying data, making recommendations to stakeholders.  </li> </ul> <p> <b> Competency Requirements  </b> </p> <ul> <li> Engineering degree and/or certification as a Data Analyst or equivalent experience.  </li> <li> Minimum years data analyst experience.  </li> <li> Experience in capital projects delivery an asset.  </li> <li> Works effectively in a team environment and builds consensus.  </li> <li> Working knowledge of a broad range of data analyzing systems, tools, and practices.  </li> <li> Computer skills -Microsoft Office (particularly Excel, Power BI), MS Teams  </li> <li> Strong leadership, communication, and presentation skills.  </li> <li> Excellent customer focus or stakeholder engagement.  </li> <li> Excellent analytical, oral and written communication skills.  </li> </ul> </div>",Process Data Engineer,1000000,1400000,https://www.naukri.com/job-listings-process-data-engineer-shell-pvt-ltd-chennai-2-to-4-years-091222501174,"['Analytical', 'Data collection', 'Database', 'power bi', 'Scheduling', 'Troubleshooting', 'Cost', 'Technical support', 'Analytics', 'Recruitment']",['Chennai'],Data Engineer,2022-12-09 19:15:27
091222501173,"<ul> <li> Develop comprehensive Plant Maintenance Schedules in Primavera P6 with look ahead and catch-up schedules (as applicable) with optimization and allocation of the resources (man, materials machines/tools).  </li> <li> Assure the Equipment Maintenance Activities are planned, scheduled, and executed within Latest Allowable Finish dates, Mange Maintenance overdues and backlogs with approved mitigation measures.  </li> <li> Integration of different schedules (vendor schedule, mobilization schedule, material availability schedules) to make a master schedule and periodic update of the same in Primavera P6.  </li> <li> Assign Schedule baselines, update the activity progress, prepare, and analyze the KPIs.  </li> <li> Enable the assigned operating units, and its on-site employees in producing and compiling high value maintenance information and data, as measured by improvement trends in data quality.  </li> <li> Maintenance, assurance, technical support and standardization of technical data and documents of the operating units. Includes site engagement leadership on technical tools forums.  </li> <li> Analyze technical data and documents and processes by working with engineering, maintenance, operations, and project teams partners of respective OUs.  </li> <li> Understanding the functions of the various equipment and instruments and thorough knowledge of reading/ interpreting the engineering drawings (PIDs, PEFS, PFDs).  </li> </ul> <p> </p> <p> <b> Dimensions  </b> </p> <ul> <li> No Annual budgets which you directly control but can have influence over department and production area budgets.  </li> <li> No direct staff, but the candidate is expected to coach/mentor less experienced staff  </li> </ul> <p> </p> <p> <b> Skills Requirements <em> </em> </b> </p> <ul> <li> University Degree in Mechanical/ Industrial/ Production/ Electrical/ Instrumentation Engineering.  </li> <li> Minimum 2+ years of overall working experience in Oil Gas / Refinery / Petrochemicals Industry  </li> <li> Experience in Scheduling day to day Preventive maintenance, Corrective / Predictive Maintenance/Major and Minor Plant Turnarounds/Shutdowns/  </li> <li> Resource allocation and leveling, Baselining, Progress updates, Reporting, Manpower planning and allocation. Understanding the safety requirement and Risk Assessment.  </li> <li> Having at least 1 year of field experience in Maintenance, troubleshooting knowledge of static and rotating industrial equipment.  </li> <li> Experience in Technical Data and Documents management including processes and tools.  </li> <li> Knowledge of Scheduling Tool - Primavera P6, and CMMS Tool - SAP (PM module) or MAXIMO is mandatory.  </li> <li> Skill in MS Office tools (like Word, excel, PowerPoint).  </li> <li> Strong aptitude for Learner Mindset  </li> <li> Flexibility to move quickly across changing priorities and manage multiple Projects/Programmes  </li> <li> Ability to independently, resourcefully, and creatively research and implement new solutions  </li> <li> A good understanding of the Upstream/DS/IG business and how it works.  </li> <li> Ability to engage and effectively communicate at all levels inside and outside Shell and within different cultural settings.  </li> <li> Strong interpersonal skills, ability to challenge and ability to build internal and external relationships based on trust and to integrate across multiple functions.  </li> <li> Continuous Improvement  </li> </ul>",Process Data Engineer,1000000,1400000,https://www.naukri.com/job-listings-process-data-engineer-shell-pvt-ltd-chennai-2-to-5-years-091222501173,"['Assurance', 'Risk assessment', 'Scheduling', 'Refinery', 'Troubleshooting', 'Continuous improvement', 'Risk management', 'Technical support', 'Downstream', 'Recruitment']",['Chennai'],Data Engineer,2022-12-09 19:15:27
071222012742,"<p><strong>Roles and Responsibilities</strong> </p><p> Experience in developing Big Data applications using Spark, Hive, Sqoop,  Kafka, and Map Reduce • Experience with stream-processing systems: Spark-Streaming, Strom etc. • Experience with object-oriented/object function scripting languages: Python,  Scala etc • Experience in designing and building dimensional data models to improve  accessibility, efficiency, and quality of data • Should be proficient in writing Advanced SQLs, Expertise in performance  tuning of SQLs. Experience with data science and machine learning tools and  technologies is a plus • Experience with relational SQL and NoSQL databases, including Postgres and  Cassandra. • Experience with Azure cloud services is a plus • Financial Services Knowledge is a plus</p><br /><p><strong>Desired Candidate Profile</strong> </p><br /><p> We are looking for a savvy Data Engineer to join our growing team of analytics experts. The  hire will be responsible for expanding and optimizing our data and data pipeline  architecture, as well as optimizing data flow and collection for cross functional teams. The  ideal candidate is an experienced data pipeline builder and data wrangler who enjoys  optimizing data systems and building them from the ground up. The Data Engineer will  support our software developers, database architects, data analysts and data scientists on  data initiatives and will ensure optimal data delivery architecture is consistent throughout  ongoing projects. They must be self-directed and comfortable supporting the data needs  2 www.nicesoftwaresolutions.com of multiple teams, systems and products. The right candidate will be excited by the  prospect of optimizing or even re-designing our companys data architecture to support  our next generation of products and data initiatives. </p><p><strong>Perks and Benefits</strong> </p><br /><p>NA</p>",Hiring Big Data Engineer,375000,875000,https://www.naukri.com/job-listings-hiring-big-data-engineer-talentahead-nagpur-pune-5-to-8-years-071222012742,"['Pyspark', 'scala', 'sqoop', 'big data', 'Python', 'SQL']","['Pune', 'Nagpur']",Data Engineer,2022-12-07 18:10:33
061222011918,"<p>  </p><p>Uplers is looking for Lead Data Engineer. This is an onsite opportunity at Hyderabad.</p><br /><p><strong>Position: Lead Data Engineer</strong></p><p><strong>Office Hours: 11:00 AM to 8 PM IST</strong></p><p><strong>Working Days: Monday to Friday</strong></p><p><strong>Location: Hitech City Main Road, Hyderabad, Telangana 500081</strong></p><p><strong>Notice period: Immediate to 30 days required.</strong></p><br /><p><strong>Experience & Skills Required:</strong> 6 to 8 years of exp and 2 years of leading the team = hand on experience of developing python-based solutions/applications.</p><p>Strong experience in Python, preferably with a good low level designing and programming concepts.</p><p>Strong experience in any cloud platform preferably AWS or GCP</p><p>Gain experience using different big data and Cloud based Data technologies.</p><p>Experience operating distributed systems of data extraction and processing of large data sets.</p><br /><p><strong>Job Description:</strong></p><p>In this role, youll work within our Data Science team as a Data Engineer. You will clean, transform, and analyze vast amounts of raw data from various systems to provide ready-to-use data to our Data Scientists. This involves both ad-hoc requests as well as data pipelines that are embedded in our production environment.</p><br /><p><strong>Additionally, you will:</strong></p><ul><li>Gain experience using different big data and Cloud based Data technologies.</li><li>Experience operating distributed systems of data extraction and processing of large data sets.</li><li>Identify, analyze, and interpret trends or patterns in complex data sets.</li><li>Design data processing pipelines.</li><li>Write good documentation with all code.</li><li>Develop strong communication skills with a proven success communicating with users, other tech teams.</li></ul><br /><p><strong>Educational Qualifications</strong></p><p>Bachelor/Master’s degree in Computer Science, Computer Engineering, quantitative studies, such as Statistics, Math, Operation Research, Economics and Advanced Analytics.</p><br /><p><strong>Requirements</strong></p><ul><li>4 - 8 years of hand on experience of developing python-based solutions/applications.</li><li>Strong experience in Python, preferably with a good low level designing and programming concepts.</li><li>Strong experience in any cloud platform preferably AWS or GCP.</li><li>Strong experience of working with API’s and integrating multiple applications together.</li><li>Experience with SQL and relational databases.</li><li>Solid architectural understanding of NOSQL databases.</li><li>Good understanding of cloud security best practices.</li><li>Excellent understanding of OOP concepts with experience applying it in enterprise programming.</li><li>Deep understanding of distributed systems (e.g. CAP theorem, partitioning, replication, consistency, and consensus)</li><li>Good Git version control practices</li><li>Proficient with Microsoft office suite (Excel, Word, PowerPoint)</li><li>Experience of working with Data Warehousing Technologies like BigQuery, Redshift, or snowflake is a plus.</li><li>Experience in using Business Intelligence tools such as: Tableau, Power BI for business applications is a plus.</li><li>Self-starter and have strong interpersonal skills</li><li>Strong analytical, problem solving, and troubleshooting abilities.</li><li>Flexibility to adapt to a variety of engagement types, working hours and work environments and locations.</li></ul><br /><p>Interested folks may send their updated CV to <strong>reshmi.g@uplers.in</strong></p><br /><p><strong>Folks staying in and around Hyderabad are preferred.</strong></p><br /><p>You may also refer someone that might be interested.</p>",Lead Data Engineer,1000000,2000000,https://www.naukri.com/job-listings-lead-data-engineer-uplers-hyderabad-secunderabad-5-to-8-years-061222011918,['GCP'],['Hyderabad'],Data Engineer,2022-12-06 19:47:16
021222004025,"<p><strong>Role: Lead Data Engineer</strong><br /> </p><p><strong>Job Description:</strong></p><p><strong>MUST: </strong>Sr. Data Engineer with hands on experience in Azure Cloud with knowledge of Python, SQL and pyspark having 8+ years of exp and has/is already leading a team of 4+ engineers. </p><ul><li><strong>Good Programming skills in Scala/Pyspark/python.</strong> </li><li><strong>Candidates having Azure cloud certificate is a plus. </strong></li><li>Strong SQL/PLSQL knowledge, Datamodelling </li><li>Gather and elicit requirements from business users, and translate them into technical <br /> requirements </li><li>Understand source systems and perform data profiling </li><li>Document source table details and appropriate transformation rules that needs to be <br /> applied on the target tables/reports </li><li>Work closely with the business, IT teams, project managers, Architects, developers, and <br /> Modelers. </li><li>Experience in Translating Business requirements into Code and deliver the required result </li><li>Experience working with Azure Cloud Data Platform solutions </li><li>Good communication skill and able to work on multiple projects </li></ul><br /><br /><p>Regards</p><p>Sundeep</p><p>+91-9391011773</p><p>sundeep.s@syrencloud.com</p>",Lead Data Engineer,2000000,3500000,https://www.naukri.com/job-listings-lead-data-engineer-syren-technologies-hyderabad-secunderabad-8-to-13-years-021222004025,['data modeling'],['Hyderabad'],Data Engineer,2022-12-06 15:40:45
091222501769,"<div> <ul> <li> Performing source data mapping and modelling aligned with the product strategy. </li> <li> Performing integration of multiple data source-formats using various ETL tools, scripts, and stored procedures. </li> <li> Implementing different data integration methods like integration via API endpoints. </li> <li> Performing a variety of data transformations to support analytics models. </li> <li> Writing unit tests and integration tests to validate quality of data ETL process. </li> <li> Improving and automating existing ETL process for performance and scalability. </li> </ul> <p> <b> <span> <span> </span> </span> </b> </p> <p> <b> <span> Fuel your passion </span> </b> </p> <p> <b> <span> </span> To be successful in this role you will: </b> </p> <ul> <li> Be a Graduate in Computer Engineering. A minimum 5 yrs of professional experience </li> <li> Have experience in with Python and data ETL process with hands on experience of data source identification, mapping, ingestion, modelling and transformation for analytical consumption purposes. </li> <li> Have good understanding of logical and physical data models and data lineage. </li> <li> Exposure to working in an Agile environment. </li> <li> Experience of working with cloud deployment and open-source languages like JavaScript/Java would be an added advantage. </li> <li> Experience of working with databases like PostgreSQL or MongoDB would be an added advantage. </li> <li> Strong oral and written communication skills. </li> <li> Strong interpersonal and leadership skills. </li> </ul> </div>",Senior Data Engineer,300000,600000,https://www.naukri.com/job-listings-senior-data-engineer-fabhotel-aay-kay-model-town-mumbai-bangalore-bengaluru-5-to-8-years-091222501769,"['Postgresql', 'Analytical', 'Javascript', 'Agile', 'MongoDB', 'Stored procedures', 'Open source', 'Analytics', 'Python', 'Data extraction']","['Mumbai', 'Bengaluru']",Data Engineer,2022-12-09 19:47:13
141122006680,"<p><strong>Roles and Responsibilities</strong> </p><p> </p><p>The Data Engineer works alongside with Analytics Team and Data Scientists to <br />develop scalable and production ready Advanced Analytics and AI software and <br />products. Additionally	develop different technical tools/services to enable large <br />scale machine learning solutions. </p><p><br />A Data Engineer believes in a non-hierarchical culture of collaboration, <br />transparency, safety, and trust. Working with a focus on value creation, growth <br />and serving customers with full ownership and accountability. Delivering <br />exceptional customer and business results. </p><p><br />• Design, develop and build real-time data pipelines from a variety of <br />sources (streaming data, file uploads, APIs, data warehouse, <br />messages etc.) </p><p><br />• Leverage the understanding of software architecture and <br />software design patterns to write scalable, maintainable, well- <br />designed and future-proof software </p><p><br />• Manage existing pipelines and create new pipelines from a variety <br />of sources (relational, XML, etc.) </p><p><br />• Develop programs to consume external API (using C# .Net, SSIS, <br />etc.) and load data into our data warehouse by understanding <br />correct relationships with our data warehouse and external API. </p><p><br />• Develop programs to consume complex HL7, FHIR, CCDA standard <br />files (using C# .Net, SSIS, etc.) and load data into our data <br />warehouse by understanding correct relationships with our data <br />warehouse and HL7 files </p><p><br />• Actively apply best practices within CI/CD </p><p><br />• Propose and implement solutions for data pipeline stabilization and <br />data quality checks </p><p><br />• Coordination with other teams to design optimal patterns for data <br />ingest and egress, as well as lead and coordinate data quality <br />initiatives and troubleshooting </p><p><br />• Design and build solutions to track data quality, stabilize data <br />pipeline, etc. to ensure reliable operations </p><p><br />• Ensure best practices are followed across architecture, codebase <br />and configuration </p><p><br />• Can design and develop dashboards using tools like Tableau or <br />PowerBI. </p><p><br />• Incorporate data security and HIPAA compliance, along with data <br />protection (transit and at-rest) to build a solid security model <br /></p><p> <strong>REQUIREMENTS :</strong></p><br /><ul><li>Ability to establish priorities with clear goals and responsibilities to <br />achieve a high level of performance. </li></ul><br /><ul><li>Ability to evaluate different options proactively and ability to solve <br />problems in an innovative way. Develop new solutions or combine <br />existing methods to create new approaches. <br /></li><li>Comfortable in working with external product teams to establish the <br />optimal data integration patterns/solutions <br /></li><li>Is ever looking to adopt new tools and technologies to improve the <br />architecture and accelerate development. <br /></li><li>Strong understanding of environment management, release <br />management, code versioning best practices, and deployment <br />methodologies <br /></li><li>Ability to communicate complex issues to diverse audiences in a <br />manner that is easily understood and actionable <br /></li><li> Strong analytical and problem-solving skills <br /></li><li>Ability to manage multiple projects and prioritize workload <br /></li><li>Self-starter, take ownership of projects <br /></li><li>Ability to work in a fast-paced, result-driven, and complex healthcare <br />setting.</li></ul><br /><br /><br /><br />",Data Engineer,500000,1500000,https://www.naukri.com/job-listings-data-engineer-gm-analytics-solutions-gurgaon-gurugram-5-to-8-years-141122006680,"['Data Ingestion', 'Power Bi', 'Powershell', 'Azure Databricks', 'docket', 'Devops', 'Soap And Rest Api', 'sql', 'spark', 'Azure Cloud', 'Web Services Api', 'ML', 'Predictive Modeling', 'Ccda', 'AI', 'Fhir', 'Bash', 'HL7', 'GIT', 'CI/CD', 'WCF', 'Data Analytics', 'ETL']",['Gurgaon'],Data Engineer,2022-12-08 20:34:15
081222011713,"<p>HI ALL,</p><p>Greetings from Teamware Solutions!!!</p><p><br />We are hiring for Azure Data Engineer,</p><p>Experience:  5+ Years</p><p>Location: Pune(WFO)</p><p>Notice Period: Immediate to 15 days</p><p>Client: MNC</p><p>Payroll: TeamWare Solutions</p><p><br /><strong>Technical Experience :</strong> </p><p> </p><br /><ul><li>PySpark, Python</li><li>Azure SQL , Cosmos Db , MongoDb</li><li>Azure Data Lake</li><li>Databricks , HDInsight</li><li>Event Hub , Azure Functions, Azure Data Lake</li></ul><br /><ul><li>Experience in building metadata-driven data pipelines to process batch and real time data.</li><li>Experience architecting , designing and optimizing big data pipelines, architectures and data sets</li><li>Build processes supporting data transformation, data structures, metadata, and data blueprints</li><li>Experience building data platforms with Databricks ( PySpark , Python  )</li><li>Experience in implementing data engineering and transformation modules using pyspark</li><li>Proficiency building data lakes and designing and maintaining data solutions using Spark and Azure PaaS services such as Azure Data Factory , Azure Data Lake , Cosmos Db and Azure Databricks</li><li>Knowledge of software engineering best practices across the development lifecycle, including<br />agile methodologies, coding standards, code reviews, source management, build processes,<br />testing, and operations.</li><li>Experience with data engineering and /distribution and manipulating large data sets leveraging tools and Data Wrangling tools</li><li>Working knowledge of Azure Platform and key components such as Azure Sql Server , Cosmos Db , Azure Data Lake etc.</li></ul><br /><p><strong>If you are interested, share your below details & updated resume to mohammed.s@twsol.com</strong><br /></p><p>Total Experience:</p><p>Relevant Experience:</p><p>CTC:</p><p>ECTC:</p><p>Notice Period:</p><p>Current Location:</p><p>Preferred Location:</p><p><br />Thanks & Regards,</p><p>Mohammed Sharief</p><p>Executive-Talent Acquisition</p><p>mohammed.s@twsol.com</p><p>Teamware Solutions<br /> </p><br /><br />",Immediate Openings For Azure Data Engineer/Pune/5+ years/WFO/MNC,1500000,3000000,https://www.naukri.com/job-listings-immediate-openings-for-azure-data-engineer-pune-5-years-wfo-mnc-teamware-solutions-pune-5-to-10-years-081222011713,['Azure Databricks'],['Pune'],Data Engineer,2022-12-08 18:48:05
061222008264,"<p>We are seeking a Data Engineer who is passionate about creating an excellent user experience and enjoy taking on new challenges. The Data Engineer will be responsible for the design, development, testing, deployment, and support of our Data Analytics and the Data warehouse platform. </p><br /><p><strong>Minimum Experience:</strong></p><ul><li>Masters/Bachelor's degree in Computer Science or a related field</li></ul><br /><p><strong>Minimum Experience:</strong></p><ul><li>2-5 years of experience as a Data Engineer with experience in Data Analysis, ingestion, cleansing, validation, verification, and presentation (Reports/Dashboards)</li><li>2-5 years of working knowledge/experience utilizing the following: Python, Spark/PySpark, Big Data Platforms (Data bricks/Delta Lake), REST services, MS SQL Server/MySQL, MongoDB, Unix/Linux Shell scripting, Azure Cloud and machine learning.</li><li>Experience with SQL, PL/SQL, and Relational Databases (MS SQL Server/MySQL/Oracle). Experience with Tableau/Power BI, NoSQL (MongoDB), and Kafka is a plus.</li><li>Experience with REST API, Web Services, JSON, Build and Deployment pipelines (Maven, Ansible, Git), and Cloud environments (Azure, AWS, GCS) is desirable.</li></ul><br /><p><strong>Job Responsibilities:</strong></p><ul><li>The software developer will perform the following duties:</li><li>Understand data services and analytics needs across the organization and work on the data warehouse and reporting infrastructure to empower them with accurate information for decision-making.</li><li>Develop and maintain a data warehouse that aggregates data from multiple content sources, including Salesforce, NoSQL DBs, RDBMS, social media, other 3rd party web services (RESTful, JSON), flat-file stores, and application databases (OLTPs).</li><li>Use Python, Spark/PySpark, Data Bricks, Delta Lake, SQL Server, Maria DB, Mongo DB, Jira, Git/Bit Bucket, Confluence, Data Bricks/Delta Lake, REST services, Tableau, Unix/Linux shell scripting, and Azure Cloud for data ingestion, processing, transformations, warehousing, and reporting.</li><li>Develop scalable data pipelines using Data connectors, distributed processing transformations, schedulers, and data warehouse</li><li>Understanding of data structures, analytics, data modeling, and software architecture</li><li>Develop, modify, and test algorithms that can be used in scripts to store, locate, cleanse, verify, validate, and retrieve specific documents, data, and information</li><li>Develop analytics to understand product sales, marketing impact, and application usage for UWorld products and applications</li><li>Employ best practices for code sharing and development to ensure common code base abstraction across all applications. Continuously be up to date on the industry standard practices on big data and analytics and adopt solutions to the UWorld data warehousing platform.</li><li>Work with QA engineers to ensure the quality and reliability of all reports, extracts, and dashboards by process of continuous improvement.</li><li>Collaborate with technical architects, developers, subject matter experts, QA team, and customer care team to drive new enhancements or fix bugs in a timely manner.</li><li>Work in an agile environment such as Scrum</li></ul><br /><p><strong>Soft Skills:</strong></p><ul><li>Working proficiency and communication skills in verbal and written English</li><li>Excellent attention to detail and organization skills and ability to articulate ideas clearly and concisely</li><li>Ability to work effectively within a changing environment that is going through high growth</li><li>Exceptional follow-through, personal drive, and ability to understand direction and feedback</li><li>Positive attitude with a willingness to put aside ego for the sake of what is best for the team</li></ul><br /><br />",Data Engineer,600000,1600000,https://www.naukri.com/job-listings-data-engineer-uworld-hyderabad-secunderabad-2-to-5-years-061222008264,"['delta lake', 'Data Bricks']",['Hyderabad'],Data Engineer,2022-12-06 15:16:14
200922008978,"<p><strong><em><u>As a Data Engineer for PreludeSys, you will:</u></em></strong></p><br /><ul><li> Data engineer with work experience in various big data technologies like Hadoop, Hive, Spark frameworks.</li><li> Also, should have a strong experience in Python, or Scala, or Java.</li><li> Build data source pipelines, ETL / ELT</li><li> Very good understanding in data model for enabling the reporting layer</li><li> Understand advanced concepts like delta lake databricks </li></ul><br /><p><strong><em><u>You Are Great At:</u></em></strong></p><br /><ul><li> Identify the most appropriate data sources to use for a given requirement and analyze the structures and contents, in collaboration with subject matter experts.</li><li> Strong knowledge of distributed systems, load balancing and networking, massive data storage, massively parallel processing and security.</li><li> Experience creating data pipelines /ETL/ELT development and processing structured and unstructured data.</li><li> Collect, clean, prepare and load the necessary data onto Big data ecosystem (Hadoop / Cloud based storage) for reporting purposes.</li><li> Strong knowledge in data aggregations, deduplication and linking to identities, containerization, data streaming etc., </li></ul><br /><br /><p><strong><em><u>MUST HAVE:</u></em></strong></p><br /><ul><li> IT, Azure, Kubernetes</li><li> Exploration, Cleaning, Normalizing, Feature Engineering and Scaling</li><li> Experience in development using Python, R or Scala, Spark / SQL / Hive / Synapse / Databricks / kafka</li><li> Proposals or Presales Support for all data and analytics initiatives.</li><li> Experience in data level security and strong knowledge in governance aspects of data life cycle.</li><li> Takes end to end responsibility for all stages in the data architecture development process of large or complex systems</li><li> Implement data quality controls and fix data quality issues detected, liaise with the data supplier for joint root cause analysis.</li><li> Very good understanding in Open Source tools like Apache frameworks - Hadoop, Yarn, Spark, Airflow,ELK, Neo4j,kafka streaming etc.,</li><li> Design and execute the build pipeline using Jenkins/Gitlab etc. to the release activities</li></ul><br />",Data Engineer,600000,1500000,https://www.naukri.com/job-listings-data-engineer-preludesys-chennai-3-to-8-years-200922008978,"['hive', 'scala', 'hadoop', 'big data']",['Chennai'],Data Engineer,2022-12-07 14:50:21
030621001413,"<p>We are looking for a skilled Data Engineer to join the experienced team, ideally with 3-6 years of relevant experience. Ability to establish priorities, work independently, and proceed with objectives without supervision. Must have excellent written and verbal communication skills. Ability to interact with colleagues, supervisors and customers face to face.</p><br /><p><strong>Required Skills:</strong></p><ul><li>Sound skills and hands on experience with Azure Data Lake, Azure Data Factory, Databricks using Pyspark and Spark SQL, SQL Data Warehouse Azure Blob, Azure Storage Explorer.</li><li>Experience SQL,Python,or R.</li><li>Data wrangling using Trifacta, Fivetran, Tableau Dataprep, Snowflake.</li><li>Experience structuring data to support analytics.</li><li>Proficient in creating On-premises Linux GPU machine using python and shell script, Data Factory pipelines for on-cloud ETL processing; copy activity, custom Azure development etc.</li><li>Knowledge of Azure Data Catalog, Event Grid, Service Bus, SQL and Synapse.</li><li>Experience in SQL Server BI suite (ETL, Reporting, Analytics, Dashboards) using SSIS, SSAS, SSRS, Power BI.</li><li>Doing backend program like creating procedure, functions in SQL server.</li><li>General administration on SQL server.</li><li>NoSQL (i.e. MongoDB)</li><li>Performance Optimization, Database Optimization and data ingestion techniques, </li><li>Stream Processing, Maintaining ELT processes and maintaining Big Data Pipeline</li></ul><p>Good to have Skills:</p><ul><li>Oracle Core DBA.</li><li>Creating backend script in Informix DB.</li><li>Middleware administration (i.e.,JBoss, Tomcat etc.).</li><li>Web-service integration using Java restful webservice.</li><li>SQL Server .NET CLR Integration.</li></ul>",Data Engineer,400000,900000,https://www.naukri.com/job-listings-data-engineer-data-core-systems-kolkata-3-to-8-years-030621001413,"['Power Bi', 'SSRS', 'MS SQL Server', 'Data Warehousing', 'ETL']",['Kolkata'],Data Engineer,2022-12-09 17:02:50
071222007804,"<p><strong>Roles and Responsibilities</strong> </p><p> </p><ul><li>Work in collaboration with engineers and stakeholders to build a platform for enabling data-driven decisions.</li><li>Build reliable, scalable, CI/CD driven streaming and batch data engineering pipelines.</li><li>Oversee and govern the expansion of the current data architecture and the optimization of query and data warehouse.</li><li>Create a conceptual data model to identify key business entities and visualize their relationships</li><li>Create detailed logical models using business intelligence logic by identifying all the entities, attributes, and relationships</li><li>Storage (cloud data warehouse, S3 data lake), orchestration (Airflow), processing (Spark, Flink), streaming services (Kafka), BI tools, graph database, and real-time large scale event aggregation store are all examples of data architecture to design and maintain.</li><li>Work on cloud data warehouses, data as a service, business intelligence, and machine learning solutions.</li><li>Data wrangling in a diverse environment.</li><li>Ability to provide data and analytics solutions that are cutting-edge.</li><li>Identify strategic and Operational KPIs for the team and drive the team to deliver the committed targets.</li></ul><br /><br /><p><strong>Desired Candidate Profile</strong> </p><br /><ul><li>SQL knowledge, as well as programming skills in Scala or Python.</li><li>5+ years of applicable data warehousing, data engineering, or data architecture experience</li><li>Experience with the GCP stack (BigQuery, GCP Databricks) is a plus.</li><li>Ability to design data analytics solutions to meet performance and scaling requirements.</li><li>Demonstrated analytical and problem-solving abilities, particularly in the context of large data.</li><li>Data warehousing concepts and modern data warehouse/Lambda architecture are well-understood.</li><li>Good understanding of the Machine Learning and Artificial Intelligence (AI) solution space.</li><li>Communication and interpersonal skills at all levels of management</li><li>You are a detail-oriented person with excellent communication skills and a strong sense of teamwork.</li></ul><br /><br /><p><strong>Perks and Benefits</strong> </p><br /><p> </p><p>Competitive salary that puts us in the top 10% in terms of compensation.</p><p>A 25K yearly budget dedicated to your upskilling.</p><p>A great health insurance coverage for you and your family + awesome Loop health benefits (Duh!)</p><p>An inclusive place to learn and grow that values data and data-driven decisions.</p><br /><br /><br />",Data Engineer 3,4000000,5500000,https://www.naukri.com/job-listings-data-engineer-3-invoq-healthcare-pune-6-to-10-years-071222007804,"['Airflow', 'Cloud Storage', 'Data Engineering', 'SCALA', 'Kafka', 'Python', 'Data Lake Storage']",['Pune'],Data Engineer,2022-12-07 13:18:31
061222004600,"<p><strong>PREFFERED IMMEDIATE JOINERS </strong></p><br /><p><strong>WORK LOCATION - CHENNAI (WORK FROM OFFICE)</strong></p><br /><p><strong>Programming Side:</strong><br /></p><ul><li> Understanding of Spark architecture.<br /></li><li> Programming experience in Python/Scala <br /></li></ul><p><strong>Other Responsibilities:</strong><br /></p><ul><li> Experience working in Azure Cloud Platform.<br /></li><li> Experience working with REST APIs and services, messaging and event technologies.<br /></li><li> Experience with ETL or building Data Pipeline tools using Azure Data Factory, Azure Databricks and Azure Data Lake Gen 2<br /></li><li> Experience with streaming platforms such as Kafka/Azure Event Hubs.<br /></li><li> Experience with relational databases (such as SQL Server, Postgres) and analytics databases (SSAS, Apache Pinot, Elastic Search).<br /></li><li> Experience with business intelligence tools (Power BI)<br /></li><li> Demonstrated experience working with large and complex data sets.<br /></li><li> Ability to document data pipeline architecture and design<br /></li></ul><p><strong>Primary Skills:</strong><br /></p><ul><li> Hadoop Ecosystem(HDP/Cloudera/MapR/EMR etc)<br /></li><li> Azure data factory/Airflow/control-M/Luigi<br /></li><li> PL/SQL<br /></li><li> Spark Core, Spark SQL<br /></li><li> SQL/Hive/Impala<br /></li><li> Git/SVN/Any other VCS and Data warehousing<br /></li><li> Exposure to NOSQL(Hbase/Cassandra/GraphDB(Neo4J)/MongoDB)<br /></li><li> File formats (Parquet/ORC/AVRO/Delta/Hudi etc.)<br /></li><li> Kafka/Kinesis/Eventhub<br /></li></ul><br /><br />",Data Engineer,300000,800000,https://www.naukri.com/job-listings-data-engineer-agilisium-chennai-3-to-8-years-061222004600,"['hive', 'python', 'SCALA', 'databricks', 'power bi', 'Spark', 'hadoop', 'SQL']",['Chennai'],Data Engineer,2022-12-06 11:08:35
051222501738,"<div> <p> <span> Data engineer with strong experience in Data Profiling, data cleansing. </span> </p> <p> <span> Experience with large-scale data warehousing architecture and data modelling </span> </p> <p> <span> De-duplication and data quality assessment. </span> </p> <p> <span> Data governance data quality management roles with hands on experience in transforming data delivery architecture. </span> </p> <p> <span> Experience with Data Management. Injection, storage, wrangling and rendering. </span> </p> <p> <span> Experience with ETL or ELT with any of the Cloud Platform like , Azure (Function Apps, Transformation ..) </span> </p> <p> <span> Experience more than one data warehousing platforms </span> </p> <p> <span> Experience with object-oriented/object function scripting languages: Python, Java, Scala, etc., a plus. </span> </p> <p> <span> Agile/Scrum is valuable. </span> </p> </div>",Data Engineer,300000,700000,https://www.naukri.com/job-listings-data-engineer-quisitive-hyderabad-secunderabad-1-to-4-years-051222501738,"['data cleansing', 'Data management', 'Agile scrum', 'Architecture', 'SCALA', 'data governance', 'Data quality', 'Data warehousing', 'Python', 'Quality management']",['Hyderabad'],Data Engineer,2022-12-05 21:56:25
031222500197,"<span> <span> <span> <span> <span> <span> <span> <span> <ul> <li> <span> <span> <span> The Knowledge Team is developing the enterprise semantic capabilities of Medidata to provide semantic-enablement to new and existing products and services. The semantic  </span></span></span></li> <li> <span> <span> <span> projects include services to support data governance, master data management, data fabric, linked data, AI/ML, data science, inferencing/reasoning, ontologies, and semantic store/analytics/query.  </span></span></span></li> <li> <span> <span> <span> The duties of the backend Services Software Engineer include:  </span></span></span></li> <li> <span> <span> <span> Design and develop new enterprise semantic-based services  </span></span></span></li> <li> <span> <span> <span> Develop service domain logic  </span></span></span></li> <li> <span> <span> <span> Design, document, and develop new APIs  </span></span></span></li> <li> <span> <span> <span> Extend, document, and enhance existing APIs  </span></span></span></li> <li> <span> <span> <span> Commit backend functional and unit testing  </span></span></span></li> <li> <span> <span> <span> Follow standard operating procedures (SOPs) to ensure all software meets regulatory and Medidata requirements  </span></span></span></li> <li> <span> <span> <span> Participate in agile development meetings  </span></span></span></li> <li> <span> <span> <span> Document technical and process flows  </span></span></span></li> <li> <span> <span> <span> Share knowledge across the Medidata enterprise with different teams  </span></span></span></li> </ul> <ul> <li> <span> <span> <span> <span> <span> The Challenges ahead  </span></span></span></span></span></li> </ul> <p> <span> <span> <span> Provide technical competency regarding feature design, development, enhancement, and implementation of backend services  </span></span></span></p> <p> <span> <span> <span> Solve complex problems with pragmatic and maintainable solutions in the Medidata enterprise environment  </span></span></span></p> <p> <span> <span> <span> Communicate effectively technical and other information to a variety of audiences (engineering, product, management, customer)  </span></span></span></p> <p> <span> <span> <span> Execute effectively processes associated with the complete software development lifecycle (SDLC)  </span></span></span></p> <p> <span> <span> <span> <span> <span> Your key success factors  </span></span></span></span></span></p><ul> <li> <span> <span> <span> Graduate (Computer Science or other Science/Engineering)  </span></span></span></li><li> <span> <span> <span> Three to five (5-10) years of professional backend service development experience. Graduate degree / work may be substituted for professional experience.  </span></span></span></li><li> <span> <span> <span> Experience in the following development technologies/languages:  <ul> <li> <span> <span> <span> JavaScript / Node.js  </span></span></span></li><li> <span> <span> <span> Python  </span></span></span></li><li> <span> <span> <span> Erlang / Elixir  </span></span></span></li><li> <span> <span> <span> Haskell  </span></span></span></li><li> <span> <span> <span> Scala  </span></span></span></li><li> <span> <span> <span> other functional language(s)  </span></span></span></li><li> <span> <span> <span> Experience with git revision control system (preferably Github)  </span></span></span></li><li> <span> <span> <span> Experience writing unit tests  <p> </p> <p> <span> <span> <span> Preferred Qualifications:  </span></span></span></p><p> </p> <ul> <li> <span> <span> <span> GraphQL experience (Apollo a big plus)  </span></span></span></li><li> <span> <span> <span> Functional service development (at least one (1))  <ul> <li> <span> <span> <span> Clojure/Compojure  </span></span></span></li><li> <span> <span> <span> Node.js/Express  </span></span></span></li><li> <span> <span> <span> Python/(Django/Flash)  </span></span></span></li><li> <span> <span> <span> Elixir/Phoenix  </span></span></span></li><li> <span> <span> <span> Semantic web interest/experience (RDF , SPARQL , RDF / S , OWL , RDF/ SPARQL, inference / reasoner)  </span></span></span></li><li> <span> <span> <span> Graph database interest/experience  <p> </p> </span> </span> </span> </li>    </ul> </span></span></span></li>    </ul> </span></span></span></li>    </ul> </span></span></span></li>    </ul> <ul>            </ul> </span> </span> </span> <p> </p> </span> </span> </span>  </span> </span>                           <ul> </ul>             <ul> </ul>      <p> </p>                 <ul> </ul>      <p> </p>                                     <ul> </ul>    <p> </p>    <p> </p>    <p> </p>      <p> </p>    <p> </p>       <p> </p> <div> </div>",Data Engineer,800000,1200000,https://www.naukri.com/job-listings-data-engineer-3d-plm-software-solutions-ltd-pune-5-to-10-years-031222500197,"['Backend', 'GIT', 'data science', 'Master data management', 'Standard operating procedures', 'Software development life cycle', 'Javascript', 'data governance', 'SDLC', 'Python']",['Pune'],Data Engineer,2022-12-03 19:26:15
200922001701,"<p>Job Tittle: Data Engineer</p><p>Shift Timings: 2pm - 11 pm  </p><p>Required Skills:</p><br /><p>These are the skillsets needed for Data Engineer</p><ul><li>Azure Synapse</li><li>Azure Data Factory</li><li>PowerBI</li><li>SSIS/SSRS</li><li>T-SQL</li></ul><p>Nice to have skills:</p><ul><li>Snowflake</li><li>Third Party Integration experience (API, Web Services)</li></ul><br />",Data Engineer,700000,1700000,https://www.naukri.com/job-listings-data-engineer-epiq-systems-hyderabad-secunderabad-5-to-8-years-200922001701,"['Azure Data Factory', 'T-SQL', 'Data Engineering', 'PowerBI', 'SSRS', 'SSIS']",['Hyderabad'],Data Engineer,2022-12-07 12:59:46
191022010561,"<br /><p> </p><p><strong>Job Title</strong>  </p><p>Data Engineer  </p><p><strong>Department</strong>  </p><p>Information Technology  </p><p><strong>Location</strong>  </p><p>India  </p><p><strong>Area</strong>  </p><p>Data  </p><p><strong>Summary</strong>  </p><p>Reporting to the Manager of Data Engineering, the Data Engineer is someone who enjoys challenging work in a fast-paced environment and solving complex business problems with data and analytics, enabling our customers in their pursuit of better healthcare outcomes for their members.  </p><p><strong>Essential Functions</strong>  </p><ul><li>Design, develop and implement data pipelines to ingest, store, and publish customer data  </li><li>Orchestrate and schedule data pipeline jobs to optimize run times and reduce compute utilization thresholds  </li><li>Significant participation in analysis, interpretation, and translation of complex health plan data, issues, trends, and relationships to provide our customers detailed insights into their data  </li><li>Analyze and interpret complex data on source and target systems, identify the gaps, and provide solutions  </li><li>Ability to development full end-to-end data pipelines that consist of complex SQL queries, stored procedures, triggers, and detailed logging  </li><li>Optimizes SSIS package execution and SQL execution to minimize load times.  <br />Ensures data integrity throughout the ETL process and appropriately handle errors  </li><li>Work closely with Scrum team members using Agile processes to iteratively develop and improve ETL pipelines.  </li><li>Proactively provide feedback and process improvement recommendations to the team and management  </li><li>Participate in cloud migration development initiatives to move data and technologies to Azure  </li><li>Integrate with and implement detailed logging and status alerts to improve troubleshooting and investigation of data pipelines  </li><li>Build, implement and design data intelligence layouts for reports, dashboards, and other data visualizations  </li><li>Create, modify and integrate reports with SSRS  </li><li>Create, modify and integrate reports with Power BI  </li><li>Leverage DAX expressions to optimize performance and interactions with the data  </li><li>Participate in the migration from legacy reports and dashboards (SSRS) to Power BI  </li><li>Work with data engineers to develop and define data pipelines, data models and data sets  </li><li>Apply data warehousing best practices to define, design, and develop data transformation rules.  </li><li>Perform ETL/ELT design, development, and support for moving large data volumes from various sources into the various destinations after significant cleansing, transformation, and processing.  </li><li>Setup and manage Power BI Gateways for onprem data interactions  </li><li>Design and implement real-time and near real-time dashboards and reports, the associated data pipelines and the necessary DataMart  </li><li> </li><li> </li></ul><p><strong>Skills</strong>  </p><ul><li>Full mastery of entire MSBI platform stack (SSIS, SSRS, SSAS, T-SQL, MSSQL Server)  </li><li>Knowledge of ETL/ELT technologies (Python, Scala, dbt)  </li><li>Knowledge of cloud orchestration tools (Airflow, ADF)  </li><li>Knowledge of cloud data visualization and report technologies (PowerBI, Tableau, Looker)  </li><li>Knowledge of cloud compute technologies (Spark, Databricks, Snowpark)  </li><li>Knowledge of federated MPP sql query engine like Trino or Starburst  </li><li>Knowledge of US healthcare  </li><li>Knowledge of Building/Maintaining Reporting Dashboard tools.  </li></ul><p><strong>Experience</strong>  </p><ul><li>3+ years of experience with data engineering pipelines in the cloud or with MSBI  </li><li>3+ years of experience designing and developing end-to-end data pipelines with SSIS, SSRS and T-SQL  </li><li>2+ years of experience in data migration or platform transformation.  </li><li>Azure or AWS Cloud certifications  </li></ul><p><strong>Education</strong>  </p><ul><li>Bachelors degree in Computer Science, Engineering, Mathematics or related discipline  </li></ul>",Hiring Data Engineer,200000,700000,https://www.naukri.com/job-listings-hiring-data-engineer-altiux-innovations-hyderabad-secunderabad-3-to-5-years-191022010561,"['cloud', 'T-SQL', 'Data migration', 'Cloud Services', 'ssrs', 'ssis', 'aws', 'azure']",['Hyderabad'],Data Engineer,2022-12-08 17:23:51
071222501089,"<div> <p> <span> <span> <span> The successful candidate will play a big role in the success of the new Data Engineering program, the incumbent would work closely with Data Engineering team to build automated testing frameworks for the Data Lake and Data Warehouse environments of Jet2 to ensure high quality certified data available in the platform for consumption. The incumbent will be working under the supervision of Senior Test Engineer and Data Engineering team to deliver testing projects. </span> </span> </span> </p> <p> <span> <span> <span> <b> Key Responsibilities: </b> </span> </span> </span> </p> <p> <span> <span> <span> The successful candidate will work independently on test engineering projects with zero or minimal guidance and mentor/guide junior members in the team, the incumbent is expected to operate out of Pune location and collaborate with various stakeholders in Pune, Leeds and Sheffield. </span> </span> </span> </p> <ul> <li> <span> <span> <span> <span> <span> <span> Partner with Data Engineering team to define quality and ensure that data product meets or exceeds the quality standards </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Review requirements, specifications and technical design documents to provide timely and meaningful feedback </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Develop software quality assurance (SQA) test plans, write test cases for the given functional requirement, and determine product quality or release readiness </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Develop and maintain testing framework and automation for Data Engineering pipeline testing and Data Warehouse model testing </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Develop, prioritize, plan and coordinate testing activities </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Design, develop and execute automation scripts </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Lead and drive Quality Engineering initiatives </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Identify, record, document bugs and maintain issue tracking system </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Perform functional and integration testing to assure data quality </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Design and apply data quality assurance practices, processes and standards </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Design and develop actionable reports and dashboards for continuous improvements </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Ensure tracking, reporting and resolution of issues in a timely manner </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Coordinate and collaborate with broader Data team  </span> <span> including Data Architects, Data Scientists, Other Data Engineering Teams, Business Intelligence and Visualisation Teams </span> </span> </span> </span> </span> </span> </li> </ul> <p> </p> <p> </p> <p> <span> <span> <span> <b> Technical Skills Knowledge  </b> </span> </span> </span> </p> <ul> <li> <span> <span> <span> <span> <span> <span> Strong understanding of testing methodologies </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Work experience with testing RDBMS databases </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Expert knowledge of SQL is desired </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Hands on experience in working with ETL Testing </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Experience of automated testing tools and frameworks </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Hands-on experience of test engineering work in Cloud environment (GCP - preferred, AWS, Azure) </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Good understanding of DevOps/CICD processes and tools </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Test cases creation and management experience </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Strong understanding of APIs and working knowledge on testing APIs. </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Experience of non-functional testing including security and performance testing processes and tools (desirable) </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Experience with source control tools such as Git\TFS </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Testing and Quality Assurance experience of Data Warehouse environments (Snowflake Cloud Data Warehouse desirable) </span> </span> </span> </span> </span> </span> </li> </ul> <p> </p> <p> <span> <span> <span> <b> Soft Skills </b> </span> </span> </span> </p> <ul> <li> <span> <span> <span> <span> <span> <span> Good communication skill - Written Verbal </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Ability build strong relationship with people across teams </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Experience of working with people from different geographies particularly UK US </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Ability to negotiate on project timelines, efforts and resources for win-win situation </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Exceptional presentation skill to an ability to do storytelling to create maximum impact of the proposal or final solution </span> </span> </span> </span> </span> </span> </li> </ul> <p> </p> <p> <span> <span> <span> <b> Leadership Organizational Skill </b> </span> </span> </span> </p> <ul> <li> <span> <span> <span> <span> <span> <span> Collaborate with broader Data team including Data Architects, Data Scientists, Other Data Engineering Teams, Business Intelligence and Visualisation Teams </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Work closely with Senior Test Engineer and Data Engineering Team Lead to help them build Test Engineering Centre of Excellence </span> </span> </span> </span> </span> </span> </li> </ul> <p> </p> <p> <span> <span> <span> <b> Qualifications Certification </b> </span> </span> </span> </p> <ul> <li> <span> <span> <span> <span> <span> <span> <span> <span> <span> B.E./B.Tech/ MTech in IT or Computer Science </span> </span> </span> </span> <span> from reputed institute (preferred) or Master Degree in Quantitative Subjects e.g. Mathematics, Statistics Economics </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> ISTQB Certification (Highly Desirable) </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Cloud Certification in GCP/AWS/Azure (Desirable) </span> </span> </span> </span> </span> </span> </li> <li> <span> <span> <span> <span> <span> <span> Data Engineering Testing Certification/Training (Desirable) </span> </span> </span> </span> </span> </span> </li> </ul> </div>",Data Engineering - Test Engineer,500000,900000,https://www.naukri.com/job-listings-data-engineering-test-engineer-jet2-travel-technologies-pvt-ltd-pune-3-to-7-years-071222501089,"['Automation', 'SQA', 'Manager Quality Assurance', 'RDBMS', 'Testing tools', 'Performance testing', 'Business intelligence', 'Data warehousing', 'Testing', 'SQL']",['Pune'],Data Engineer,2022-12-07 17:48:38
071222006235,"<p>  </p><p><strong><u>Position : Azure Data Engineer ( UK shift)</u></strong></p><p><br /> </p><p> <strong><u>The Role :</u></strong></p><p>Being a senior in the offshore Analytics team, overseeing the work of Data engineers and being a SPOC in technical questions.<br /> Work involves Solution definition, technical implementation.<br /> Must be able to participate in technical work in Azure Data Platform projects/maintenance.<br /> Ability to work with the customer/ business team Responsible, Goal oriented, communicative</p><p>The Candidate would be required to work on Solution definition, work planning and technical implementation.</p><p><strong><u>Required Skills</u></strong></p><ul><li>Minimum experience 3 to 5years</li><li>Priority Skillset 1 - Azure Synapse and Data Vault 2.0 modelling, Technologies required for Microsoft Azure Data Engineer certification, 1+ years</li><li>Priority Skillset 2  Snowflake, 1+ year</li><li>Good To have skillset – Reporting technologies like Power BI etc. ( 1+ year)</li><li>Should have knowledge on Data Warehousing, Analytics and Reporting for 2+ years</li></ul><p>Softskills:</p><ul><li>Must have good communication skills</li><li>Must be a self-motivated and team player</li></ul><br /><br />",Azure Data Engineer,50000,300000,https://www.naukri.com/job-listings-azure-data-engineer-fulcrum-worldwide-software-pune-3-to-6-years-071222006235,"['Azure Synapse', 'Power BI']",['Pune'],Data Engineer,2022-12-07 12:09:23
061222910274,"<p></p><p>REPORTS TO: Delivery Manager<br /><br />EDUCATION: Bachelor's and above<br /><br />SUMMARY - <br /><br />The Data Engineer and Project Manager position is part of the core Virtue team.- The- person in this position is responsible for identifying and solving important complex- business problems while utilizing- their- advanced analytics, data management and modeling skills. On a day to day basis this role involves working closely with- US and UK delivery team, managing the data, building and executing ML models, documenting approach and results and helping with the weekly client deliverables, which will include creating presentations in power point,- conducting- market research and- performing ad-hoc- analysis.<br /><br />ROLE REQUIREMENTS - <br /><br />We need someone with exceptional- problem-solving skills. At least 3+ years working in data analytics. Strong skillset in R, Comfort with advanced data mining and predictive modeling skills. A keen ability to story board data and good communication skills. We are looking for a hard worker, fast learner and person with ability to deliver quality work. Strong attention to detail and a positive mindset.<br /><br />SALARY - Negotiable and based on experience and expertise.<br /><br />START DATE - Immediate<br /><br />HOW TO APPLY<br /><br />If interested kindly apply along with your updated resume and following details:<br /><br />Short motivation (100-150 words)<br /><br />- Current CTC<br />- Expected CTC<br />- Notice period<br /><br /></p>",Data Engineer,800000,1400000,https://www.naukri.com/job-listings-data-engineer-virtue-analytics-lucknow-3-to-5-years-061222910274,"['Predictive Modeling', 'Data Management', 'Data Engineer', 'Data Mining', 'ML models', 'Data Modeling', 'Analytics']",['Lucknow'],Data Engineer,2022-12-06 17:28:34
071222502205,"<div> <ul> <li> <span> <span> Scheduling and troubleshooting of automated jobs/ backups/PubSub (asynchronous event listening) - example BorgCron  </span> </span> </li> <li> <span> <span> Defining data schemas/ metadata/ data slices  </span> </span> </li> <li> <span> <span> ETL jobs/pipelines in core Java and Python (similar to Clarinet, Replicator)  </span> </span> </li> <li> <span> <span> Monitoring of sync services/ events and APIs across multiple systems/  </span> </span> </li> <li> <span> <span> Data Security enforcement, provisioningand monitoring services  </span> </span> </li> </ul> <div> <span> <span> <strong> Good to have (Not Mandatory): </strong> </span> </span> </div> <ul> <li> <span> <span> Functional knowledge of Corporate Finance Structure, HR Functional Structure to start with. </span> </span> </li> <li> <span> <span> Cloud identity and access management  </span> </span> </li> <li> <span> <span> Defining fine grained granular/row/column level access policies  </span> </span> </li> <li> <span> <span> Monitoring cloud quota policies  </span> </span> </li> </ul> <div> <span> <span> <strong> Detailed Job Description :  </strong> </span> </span> </div> <ul> <li> <span> <span> 6 years of hands-on experience in managing data warehouse/data mart/data lakes preferably in cloud Knowledge. RTH-Y </span> </span> </li> </ul> </div>",Data Engineer,1000000,1400000,https://www.naukri.com/job-listings-data-engineer-varite-hyderabad-secunderabad-6-to-8-years-071222502205,"['Core Java', 'metadata', 'Access management', 'data security', 'Cloud', 'Scheduling', 'Corporate finance', 'Troubleshooting', 'Monitoring', 'Python']",['Hyderabad'],Data Engineer,2022-12-07 19:48:33
291122014537,"<p> </p><p>Dun & Bradstreet is looking for a Senior Data Developer/Engineer to join our Technology team. You will be part of a group responsible for designing, implementing, maintaining and supporting Data & Analytics Platform applications.  The role will leverage modern processes and tools in ensuring highest quality data in the Dun & Bradstreet Data Supply Chains.</p><br /><p><strong>Job Description</strong></p><ul><li> Passionate Hands-on programmer in developing data pipelines in Spark , Scala and Python. Experience in working in cloud platforms like AWS/GCP.</li><li>Focus in more on the programming skills. </li><li>Collaborate with project teams (solution architects, business, QA and project management) to ensure solutions meet business objectives and fall within timelines and acceptance criteria</li><li>Participate in testing of prototypes & validate test procedures to ensure that they are applicable to the design</li><li>Application support/ bug fixes / QA</li><li>Perform root-cause analysis (RCA) of complex issues ranging from hardware, operating system, application, network, and information security platforms while working closely with various infrastructure teams and business users to quickly arrive at creative, tactical and long-term solutions. </li></ul><br /><br />",D&B is hiring Data Engineer,1200000,2200000,https://www.naukri.com/job-listings-d-b-is-hiring-data-engineer-dun-bradstreet-hyderabad-secunderabad-4-to-8-years-291122014537,"['gcp', 'aws']",['Hyderabad'],Data Engineer,2022-11-29 18:57:30
281122907377,"<p>- Define, implement and validate solution frameworks and architecture patterns for data modeling, data integration, processing, reporting, analytics and visualization using leading cloud, big data, open source and other enterprise technologies<br /><br />- Develop scalable data and analytics solutions leveraging standard platforms, frameworks, patterns and full stack development skills<br /><br />- Analyze, characterize and understand data sources, participate in design discussions and provide guidance related to database technology best practices<br /><br />- Write tested, robust code that can be quickly moved into production<br /><br />Responsibilities :<br /><br />- Experience with distributed data processing and management systems<br /><br />- Familiarity with leveraging and modifying open source libraries to build custom frameworks<br /><br />- Good knowledge on Azure Cloud systems and Back End frame work<br /><br />- Experienceknowledge on Kafka BrokerConsumer<br /><br />- ExperienceKnowledge on the frame work design for logs using Spark Scala and Hadoop management<br /><br />- Good knowledge on JAR file creations, automation and CICD process<br /><br />- Strong knowledge of Development Support projects and good understanding of support processes<br /><br />- Should have very good understanding of OffshoreOnsite model of project execution and support<br /><br />- Provide Production Support i.e on call support<br /><br /></p><p>Desired Candidate profile :<br /><br />- Should have experience working with Agile methodology<br /><br />- Demonstrated ability to learn and apply new technologies and frameworks quickly<br /><br />- Should be self-motivated and Smart working<br /><br />- Candidate should have excellent communication skills both oral and written<br /><br />- Should be willing to learn<br /><br />- Should be B.E, B.Tech, MCA, M.Sc. qualified<br /><br />Primary Technical Skills : Spark Scala ,SbtMavenGradle, HDFS, Hive<br /><br />Secondary Technical Skills : Azure HD Insights , Azure Cloud Native Services, Kafka, Livy, Hadoop & CICD, Postgress SQL</p>",Big Data Engineer- Distributed Systems,1900000,2500000,https://www.naukri.com/job-listings-big-data-engineer-distributed-systems-avap-agile-kolkata-mumbai-hyderabad-secunderabad-lucknow-chennai-ahmedabad-delhi-ncr-bangalore-bengaluru-5-to-9-years-281122907377,"['Hive', 'Scala', 'Kafka', 'CI/CD', 'HDFS', 'Agile methodology', 'Spark', 'Data Modeling']","['Chennai', 'Delhi NCR', 'Lucknow', 'Mumbai', 'Ahmedabad', 'Bengaluru', 'Hyderabad', 'Kolkata']",Data Engineer,2022-11-28 17:57:27
301122911443,"<p>Primary Responsibilities :<br /><br />- Candidates should possess strong knowledge and interest across big data technologies and have a strong background in data engineering.<br /><br />- Build data pipeline frameworks to automate high-volume batch and real-time data delivery <br /><br />- Continuously integrate and ship code into our cloud production environments<br /><br />- Work directly with Product Owners and customers to deliver data products in a collaborative and agile environment<br /><br />- Assist in creating architectures using cloud-native technologies <br /><br />Your Responsibilities Will Include :</p><p></p><p><br />- Developing sustainable data driven solutions with current new generation data technologies to drive our business and technology strategies<br /><br />- Build robust, scalable, production-ready data pipelines<br /><br />- Unit test pipelines to ensure high quality<br /><br />- Design AWS data ingestion frameworks and pipelines based on the specific needs driven by the Product Owners and user stories.<br /><br />- Experience building Data Lake using AWS and Hands-on experience in S3, EKS, ECS, AWS Glue, AWS KMS, AWS Firehose, EMR<br /><br />- Experience Apache Spark Programming with Databricks<br /><br />- Leverage capabilities of Databricks Delta Lake functionality as needed<br /><br />- Leverage capabilities of Databricks Lakehouse functionality as needed to build CommonConformed layers within the data lake<br /><br />- Building data APIs and data delivery services to support critical operational and analytical applications<br /><br />- Contributing to the design of robust systems with an eye on the long-term maintenance and support of the application<br /><br />- Leveraging reusable code modules to solve problems across the team and organization<br /><br />- Handling multiple functions and roles for the projects and Agile teams<br /><br />- Defining, executing and continuously improving our internal software architecture processes<br /><br />- Experience working on NoSQL Databases such as Cassandra, HBase, and Elastic Search<br /><br />- Hands on experience with leveraging CICD to rapidly build & test application code<br /><br />- Expertise in Data governance and Data Quality<br /><br />- Experience working with PCI Data and working with data scientists is a plus<br /><br />- hands-on experience with any of the following programming languages: PySpark, Python, R, Scala<br /><br />Knowledge, Skills & Experience :<br /><br />Education :<br /><br />- BS degree in Computer Science, Data Engineering or similar<br /><br />- Intermediate to senior level experience in a Data Engineering role. Demonstrated strong execution capabilities<br /><br />- Any certifications on AWS, Databricks is a plus preferred<br /><br /></p><p></p><p>Required :<br /><br />- Overall, 5 to 7 years of must have prior data engineering and ETL experience<br /><br />- Demonstrated experience with best Agile Scrum SDLC practices: coding standards, reviews, code management, build processes, and testing.<br /><br />- History of successfully developing software following an Agile methodology<br /><br />- Search engine integration and data catalogmetadata store experience is preferred<br /><br />- 5+ years of experience on designing and developing Data Pipelines for Data Ingestion or Transformation using AWS technologies<br /><br />- At least 4+ years of experience in the following Big Data frameworks: File Format (Parquet, AVRO, ORC), Resource Management, Distributed Processing and RDBMS<br /><br />- 5+ years of developing applications with Monitoring, Build Tools, Version Control, Unit Test, TDD, Change Management to support DevOps<br /><br />- Deployment of advanced services on the cloud and working with Data Architects to deploy AIML and cutting-edge data lakes, warehouses, and pipelines is a plus (especially using Amazon Sage maker)<br /><br />- Familiarity with machine learning implementation using PySpark.<br /><br /></p><p></p><p>Preferred :</p><p></p><p><br />- Experience working with a combined in-house and outsourced team<br /><br />- Experience working in a geographically separated team including offshore resources<br /><br />- 2+ years' experience with other cloud services like Microsoft Azure, Google Compute, or others<br /><br />- 3+ years of experience working with Streaming using Spark or Flink or Kafka<br /><br />- 4+ years of experience working with Dimensional Data Model and pipelines<br /><br />- Intermediate level experienceknowledge in at least one scripting language (Python, Perl, JavaScript)<br /><br />- Hands on design experience with data pipelines, joining data between structured and unstructured data<br /><br />- Experience implementing opensource frameworks & exposure to various opensource & package software architectures (Elastic Search, Spark, Scala, Splunk, Apigee, and Jenkins etc.)<br /><br />- Experience with various NoSQL databases (Hive, MongoDB, Couchbase, Cassandra, and Neo4j) will be a plus<br /><br />Other :<br /><br />- Ability to work independently<br /><br />- Excellent oral and written communication skills<br /><br />- Ability to present new ideas, approaches and information clearly<br /><br />- Outstanding attention to detail and organizational skills<br /><br />- Diligent work ethic and insatiable desire to learn and develop skills<br /><br />- Ability to acquire new knowledge quickly<br /><br />- Strong interpersonal skills<br /><br />- Self-starter, highly motivated<br /><br />- Excellent Time management skills<br /><br />- Cultural sensitivityawareness<br /><br />- Successfully complete assessment tests offered in Pluralsight, Udemy, etc. or complete certifications to demonstrate technical expertise on more than one development platform.</p>",Data Engineer - AWS/Azure/Big Data,1600000,2500000,https://www.naukri.com/job-listings-data-engineer-aws-azure-big-data-rohini-it-consulting-llp-mumbai-bangalore-bengaluru-7-to-12-years-301122911443,"['DataLake', 'Azure', 'NoSQL', 'Data Pipeline', 'PySpark', 'Big Data', 'Azure Databricks', 'Spark']","['Mumbai', 'Bengaluru']",Data Engineer,2022-11-30 17:36:30
140322005893,"<p> </p><p><strong>Lead Data Engineer / Sr. Data Engineer </strong></p><p><strong>Experience: 6+ Years</strong></p><p><strong>Summary:</strong></p><p>The Lead Data Engineer will design and implement data solutions that support the organizations </p><p>strategic business goals.</p><p><strong>Responsibilities:</strong></p><ul><li> Focuses on data modelling, database design, performance optimization and ETL pipeline.</li><li> Tests, monitors, manage and validate data warehouse activity including data extraction, </li></ul><p>transformation, movement, loading, cleansing, and updating processes.</p><ul><li> Design and building simple (non - complex) reusable components of larger process or </li></ul><p>framework to support analytics products with guidance from experienced peers</p><ul><li> Participate in design, code, test plans and dataset implementation performed by other data </li></ul><p>engineers in support of maintaining data engineering standards</p><ul><li> Implement automated workflows and routines using workflow scheduling tools</li></ul><p><strong>Skills Needed:</strong></p><ul><li> 5+ years’ experience developing Data & Analytic solutions</li><li> Experience with data warehousing, Data Lake, analytic processes, and methodologies.</li><li> Proficient experience with SQL queries functionality, Power BI, Tableau, Business Intelligence </li></ul><p>software, and ETL tools.</p><ul><li> Strong knowledge of data integration, data quality and multi-dimensional design.</li><li> Experience in Data Migration from on-premises databases to Cloud.</li></ul><p>Knowledge with AWS/Azure/GCP data technologies is a plus.</p><ul><li> Knowledge of web services (e.g., SOAP and REST)</li><li> Experience with scripting languages such as Shell, Python</li><li> Experience of Big Data platforms, enabling non-technical users to gain insights </li><li> Experience in big data tools like spark, pyspark, </li><li> Hands on experience with modern data storage systems (e.g. S3, Snowflake, Athena, Glue)</li><li> Experience with workflow scheduling tools such as Airflow</li></ul><p><strong>Added Advantage:</strong></p><ul><li> Certifications on Snowflake, AWS /Azure/GCP<strong>Roles and Responsibilities</strong> </li></ul><br /><br /><br />",Lead Data Engineer / Senior Data Engineer,1500000,2500000,https://www.naukri.com/job-listings-lead-data-engineer-senior-data-engineer-zenon-analytics-noida-5-to-9-years-140322005893,"['python', 'Azure', 'sql queries', 'airflow', 'pyspark', 'ETL Tool', 'GCP', 'Data Lake', 'data integration', 'AWS']",['Noida'],Data Engineer,2022-12-01 12:02:31
031222500859,"<div> <span> <p> <span> Should have 4 years experience in data engineering. </span> <br /> </p> <p> <span> Should have 2 years relevant experience in AWS Stack. </span> <br /> </p> <p> <span> Should have 3 years relevant experience in Python and python libraries. </span> <br /> </p> <p> <span> Should have good experience in data pipeline. </span> <br /> </p> <p> <span> Team player, Motivated, able to grasp things quickly with analytical and problem-solving skills. </span> <br /> </p> <p> <span> <span> <span> </span> </span> </span> <br /> </p> <p> <span> <b> <u> <span> Good to have: </span> </u> </b> </span> <br /> </p> <p> <span> </span> <br /> </p> <p> <span> Experience in Redshift, Postgres. </span> <br /> </p> <p> <span> Experience in PySpark. </span> <br /> </p> <p> <span> Experience in database (e.g., SQL, MYSQL etc.). </span> <br /> </p> <div> <br /> </div> </span> <br /> </div>",Sr/Lead Data Engineer,1000000,1500000,https://www.naukri.com/job-listings-sr-lead-data-engineer-wavelabs-gurgaon-gurugram-4-to-5-years-031222500859,"['Managed services', 'Analytical', 'MySQL', 'Machine learning', 'Agile', 'Video conferencing', 'Life sciences', 'MSP', 'SQL', 'Python']",['Gurgaon'],Data Engineer,2022-12-03 19:47:33
281122002313,"<p><strong>Roles and Responsibilities</strong>  Unix, Abinito, SQL </p><br /><br /><p><strong>Desired Candidate Profile</strong></p><p> </p><br /><p>Contract to Hire position.</p><p>Only Immediate to 15 days joiners.</p><p>Work from Home.</p><br /><p>Please Share Updated Resume.</p><p>pkakkireni@vsoftconsulting.com</p><br /><p>Kindly ignore if not relevant to you.</p><p>Please let me know below details</p><br /><p>Present Payroll company name:</p><p>Notice Period(Please mention as immediate if not working):</p><br /><br /><br /><p><strong>Perks and Benefits</strong> </p><br /><br />",Data Engineer,800000,1500000,https://www.naukri.com/job-listings-data-engineer-v-soft-consulting-kolkata-hyderabad-secunderabad-pune-ahmedabad-chennai-bangalore-bengaluru-mumbai-all-areas-2-to-6-years-281122002313,[],"['Chennai', 'Pune', 'Ahmedabad', 'Bengaluru', 'Hyderabad', 'Kolkata', 'Mumbai (All Areas)']",Data Engineer,2022-12-07 15:01:42
221122004933,<p><strong>Roles and Responsibilities</strong> </p><br /><br /><p><strong>Desired Candidate Profile</strong> </p><br /><br /><p><strong>Perks and Benefits</strong> </p><br /><br />,Data Engineer,1500000,2500000,https://www.naukri.com/job-listings-data-engineer-v-soft-consulting-kolkata-hyderabad-secunderabad-pune-ahmedabad-chennai-bangalore-bengaluru-delhi-ncr-mumbai-all-areas-5-to-7-years-221122004933,[],"['Chennai', 'Pune', 'Delhi NCR', 'Ahmedabad', 'Bengaluru', 'Hyderabad', 'Kolkata', 'Mumbai (All Areas)']",Data Engineer,2022-12-07 15:01:43
081222910308,"Technologies you will get to work with: -<br />1.Azure Data-bricks<br />2.Azure Data factory<br />3.Azure DevOps<br />4.Spark with Python & Scala and Airflow scheduling.<br /><br />What You will Do: -<br />* Build large-scale batch and real-time data pipelines with data processing frameworks like spark, Scala on Azure platform.<br />* Collaborate with other software engineers, ML engineers and stakeholders, taking learning and leadership opportunities that will arise every single day.<br />* Use best practices in continuous integration and delivery.<br />* Sharing technical knowledge with other members of the Data Engineering Team.<br />* Work in multi-functional agile teams to continuously experiment, iterate and deliver on new product objectives.<br />* You will get to work with massive data sets and learn to apply the latest big data technologies on a leading-edge platform.",Data Engineer,1500000,1800000,https://www.naukri.com/job-listings-data-engineer-cliqhr-recruitment-services-bangalore-bengaluru-3-to-5-years-081222910308,"['Scala', 'Data Engineer', 'Azure Data factory', 'Azure Data-bricks', 'Azure DevOps', 'Python']",['Bengaluru'],Data Engineer,2022-12-08 17:08:12
051222007980,"<p> </p><ul><li><strong>At least 7+ years</strong> <strong>within relevant domain of Data Engineering across industries and work experience providing analytics solutions in a commercial setting</strong></li><li><strong>Consulting experience will be considered a plus</strong></li><li><strong>Proficient understanding of distributed computing principles</strong></li><li><strong>Management of Spark clusters, with all included services  various implementations of Spark preferred</strong></li><li><strong>Ability to solve ongoing issues with operating the cluster and optimize for efficiency</strong></li><li><strong>Understanding of prevalent cloud ecosystems and their associated services AWS, Azure, Google Cloud, IBM Cloud. Primary</strong> <strong>expertise In more than one and awareness about multiple would be a plus.</strong></li><li><strong>Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming</strong></li><li><strong>Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala</strong></li><li><strong>Experience with integration of data from multiple data sources</strong></li><li><strong>Experience with NoSQL databases, such as HBase, Cassandra, MongoDB</strong></li><li><strong>Knowledge of various ETL techniques and frameworks, such as Flume</strong></li><li><strong>Experience with various messaging systems, such as Kafka or RabbitMQ</strong></li><li><strong>Experience with Big Data ML toolkits, such as Mahout, SparkML, or H2O</strong></li><li><strong>Good understanding of Lambda Architecture, along with its advantages and drawbacks </strong></li></ul><br />",Lead Data Engineer,950000,1900000,https://www.naukri.com/job-listings-lead-data-engineer-mount-talent-consulting-bangalore-bengaluru-delhi-ncr-6-to-10-years-051222007980,"['cloud', 'Senior Data Engineer', 'Lead Data engineer', 'AWS']","['Delhi NCR', 'Bengaluru']",Data Engineer,2022-12-05 19:31:15
051222906904,"<p>• 5-7 years of in-depth hands-on experience in data warehousing Redshift or any OLAP to support business/data analytics, business intelligence (BI) </p><p>• Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases and Cloud Data warehouse like Redshift </p><p>• Data Model development, additional Dims and Facts creation and creating views and procedures, enable programmability to facilitate Automation </p><p>• Prior Data Modelling and Power BI experience </p><p>• Experience with Redshift and OLAP systems is must. GLUE pipeline skill is must </p><p>• Data compression into PARQUET to improve processing and finetuning SQL programming skills required </p><p>• Experience building and optimizing “big data” data pipelines, architectures and data sets </p><p>• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement </p><p>• Strong analytic skills related to working with structured and unstructured datasets</p>",ETL Data Engineer - AWS and SQL,1000000,2000000,https://www.naukri.com/job-listings-etl-data-engineer-aws-and-sql-edge-bangalore-bengaluru-karnataka-4-to-9-years-051222906904,"['S3', 'Data Modelling', 'Power BI', 'data warehousing', 'Redshift', 'SQL', 'NoSQL', 'Glue', 'SSAS', 'Data Engineer', 'AWS', 'Lambda']","['Bengaluru', 'Karnataka']",Data Engineer,2022-12-05 17:45:57
021222500057,"<ul> <li> <p> Youre passionate about everything Crypto and Web3.0 </p> </li> <li> <p> You take ownership and have a thirst for excellence with an impact driven and result oriented mindset. </p> </li> <li> <p> You grow while helping others grow with you  </p> </li> <li> <p> You thrive on change, have attention to detail and passion for quality </p> </li> <li> <p> You love exploring new ideas to build something useful and are always curious to learn. </p> </li> </ul> <p> <br /> </p> <p> <strong> <br /> </strong> </p> <p> <strong> <span> What youll do </span> </strong> </p> <p> <br /> </p> <ul> <li> <p> Design Build reliable, scalable, CICD driven streaming and batch data engineering pipelines. </p> </li> <li> <p> Work in collaboration with Data scientists, ML engineers, Stakeholders to build a platform for enabling data-driven decisions. </p> </li> <li> <p> Oversee and govern the expansion of the current data architecture and the optimization of query and data warehouse. </p> </li> <li> <p> Create a conceptual data model to identify key business entities and visualize their relationships </p> </li> <li> <p> Create detailed logical models using business intelligence logic by identifying all the entities, attributes, and their relationships </p> </li> <li> <p> Create a taxonomy/data dictionary to communicate data requirements that are important to business stakeholders work on acquiring external data sets through APIs and/or Websockets and prepare physical data models on top of that </p> </li> <li> <p> Acts as team lead stay current with new and evolving tech stack. Guide and mentor team of Data Engineers. </p> </li> </ul> <p> <br /> </p> <br /> <br /> <p> <strong> <span> What youll bring </span> </strong> <br /> </p> <ul> <li> <p> 7+ years of Experience in building Data Engineering pipelines Data Governance using modern Cloud Architecture. </p> </li> <li> <p> Proficient in Databricks, Spark, Data Lake, Kaka/Kinesis  </p> </li> <li> <p> Experience in Any Cloud DW Redshift / Snowflake / BigQuery / Synapse </p> </li> <li> <p> Experts Programming in Any one - Python/Scala/Java (Python preferred) </p> </li> <li> <p> Design, Test-driven development, code review and implement CICD using Github/Gitlab/Docker </p> </li> <li> <p> Good understanding of ETL/ELT technology and processes </p> </li> <li> <p> Basic knowledge of Apache Airflow would be a plus </p> </li> <li> <p> Basic knowledge of Data Modeling tools (dbt, dataform, Alteryx, Informatic,etc) would be a plus </p> </li> </ul> <p> <strong> <br /> </strong> </p> <p> <strong> <span> Big Plus </span> </strong> </p> <p> </p> <p> Experience in ML ops and tools for Model Reproducibility, Deployment, packaging, </p> <p> monitoring and Model retraining. </p> <p> Experience in Lakehouse Architecture using Databricks. </p> <p> </p> <p>   </p>",Lead Data Engineer,800000,1200000,https://www.naukri.com/job-listings-lead-data-engineer-coindcx-mumbai-bangalore-bengaluru-4-to-8-years-021222500057,"['Data dictionary', 'SCALA', 'Packaging', 'data governance', 'test driven development', 'Apache', 'Business intelligence', 'Monitoring', 'Python', 'Data architecture']","['Mumbai', 'Bengaluru']",Data Engineer,2022-12-02 13:45:25
061222009096,"<p>At Remote Office, we connect talents with global businesses. We are  an equal opportunity employer and committed to creating an inclusive  environment. We hire, develop, and retain the most talented individuals  by celebrating our diverse cultures, perspectives, skills, and  experiences.</p><br /><p>We are looking for a <strong>Senior Data Engineer (Snowflake) </strong>who  will work directly with one of our Australian clients. This is a  full-time job where you will provide senior-level contribution to the  team that is responsible for the design, deployment, and maintenance of  the businesss data platforms by ensuring proper execution of duties and  alignment with business vision and objectives. You will also implement  strategies directed at acquiring data and promoting the development of  new insights across the business.</p><br /><p><strong>Responsibilities</strong></p><ul><li>Develop and extend a recently started data platform to support big data pipelines in the consumer data space</li><li>Working with Datavault & ETL to design and develop data models to create new databases or update existing ones</li><li>Continue helping to load data into Snowflake and build out the raw and business vaults for the existing core platform.</li><li>Facilitate in loading data into Snowflake by building out the raw and business vaults.</li><li>Providing consultation on data management issues to other members of the organisations staff</li><li>Working with data architects to design enterprise data warehouses for storing large quantities of data for later retrieval</li><li>Performing  thorough testing of data pipelines and data models to ensure data is  accurate and pipelines are resilient and self-healing</li><li>Developing reports that present data in a feasible format for non-technical users such as managers</li><li>Reviewing and analysing data sets to identify patterns or trends that may have business implications</li><li>Design data infrastructure with privacy and security being cross-cutting concerns</li><li>Take charge of maintaining data quality and integrity</li><li>Contribute to project discussions, collaborate directly with architect team and present results to key stakeholders</li><li>Analyse and visualise data to identify untapped trends and business opportunities</li><li>Stay updated about the latest developments in the field of data science</li></ul><br /><p><strong>Must Have Requirements:</strong></p><ul><li>Bachelor's degree or higher in IT or related field or combination of relevant education, experience and training</li><li>A minimum of 5 years of experience in the data engineering or relevant field</li><li>Experience with enterprise data warehouse</li><li>In-depth skills in developing and maintaining in & ETL data pipeline with exposure to Snowflake</li><li>In-depth skills in developing and maintaining ETL/ELT data pipelines</li><li>Experience in building and maintaining Data Pipelines</li><li>Experience in Data bricks development processes</li><li>Strong understanding of the challenges in building end-to-end big data pipelines for a large variety of use cases at scale</li><li>Experience with cloud platform and services (either AWS or Azure DevOps is preferred)</li><li>Expertise in data modelling techniques such as Data vault</li><li>Builds, maintains, and troubleshoots CICD pipelines through cloud services</li><li>Proficiency in relational SQL/NoSQL databases</li><li>Familiar with alerting and self-recovery methods concerning data accuracy</li><li>Analytical skills with the ability to transform data into optimal business decisions</li><li>Expertise in peer reviewing pipeline codes and suggesting improvements when required</li><li>Experience in helping teams make informed business decisions with data</li><li>Strong communication and presentation skills</li><li>Excellent English speaking and writing skills</li></ul><br /><p><strong>Additional requirements(Optional):</strong></p><ul><li>Familiarity with tools like Tableau and Power BI will add value</li><li>Competence in object-oriented or object function scripting languages such as Python</li><li>Familiarity with Azure Databricks and/or Data Factory will be beneficial</li><li>Prior experience in handling junior engineers in completing high-quality projects within the timeline</li></ul><br /><p><strong>Employment Status:</strong></p><ul><li>Full-time (6 months contractual and possibility of extension)</li></ul><br /><p><strong>Experience Requirements:</strong></p><ul><li>5+ years</li></ul><br /><p><strong>Details:</strong></p><ul><li>Work hour: 10AM-6PM AEST </li><li>Weekdays: Monday to Friday</li><li>Type: Remotely (WFH)</li><li>Needed ASAP</li></ul><br /><p><strong>Job Location:</strong></p><ul><li>Remote</li></ul><br /><p><strong>Salary:</strong></p><ul><li>Negotiable</li></ul>",Senior Data Engineer (Snowflake),10000,20000,https://www.naukri.com/job-listings-senior-data-engineer-snowflake-remote-office-australia-5-to-10-years-061222009096,"['Team Management', 'Data Pipelines', 'Power BI', 'Data Vault', 'Data Warehouse', 'Agile Leadership', 'Tableau', 'ETL data', 'data science', 'CICD', 'Database', 'AWS', 'Azure DevOps']",['Australia'],Data Engineer,2022-12-06 16:11:24
040622000016,"<p><br /><strong>Location:</strong> Chennai, India. Work from home is flexible but individuals must be eligible to work in Chennai, India without sponsorship</p><br /><p>The Data Engineer position combines advanced knowledge in database systems and programming languages with strong interpersonal and project management skills. We are looking for someone with a very broad interest in transforming patient-authored text into powerful new tools.</p><br /><br /><p><strong><u>WHAT YOU WILL BE DOING:</u></strong></p><br /><ul><li>Research and create effective methods to collect, analyze, and store data efficiently that meets technical and business needs</li><li>Build strong working relationships with the engineering team to understand technical dependencies and architectural impact</li><li>Identify new data sources and assist with the development of new tables in the data warehouse</li><li>Build new and maintain existing ETL processing pipelines such as Airflow and improve its efficiency</li><li>Build data pipelines and ETL processes in AWS assisting the Machine Learning efforts</li><li>Follow the latest developments in ETL processing and data engineering methods for analytics and machine learning</li><li>Drive to understand organizational goals to chart out and maintain a data architecture plan</li><li>Work with Data Scientists and Engineers to research and develop a CI/CD Process for machine learning projects</li><li>Have strong ethical integrity about the use of data and privacy</li></ul><br /><p><strong><u>REQUIREMENTS/WHAT WE NEED:</u></strong></p><br /><ul><li>5+ years of experience programming with Python</li><li>5+ years of experience with SQL and relational databases</li><li>4+ years of experience with Git and version control</li><li>3+ years of experience working in a Unix environment</li><li>Experience with AWS products and services supporting data engineering and ETL applications</li><li>Ability to understand technical needs and convert them into a feasible architectural plan</li><li>Experience working with Jupyter Notebooks and AWS Sagemaker</li><li>Experience performing large-scale data transformation and data cleansing processes</li><li>Solid analytical skills and demonstrated problem-solving ability</li><li>Bachelor or Master's degree in computer science or a related field with analytical training OR an equivalent combination of education/experience in technology and operations</li></ul><br /><p><strong><u>Full-time employees are eligible for</u>:</strong></p><br /><ul><li>Competitive Compensation</li><li>Health reimbursement</li><li>Provident Fund contribution</li><li>Holidays</li><li>Flexible Time Off </li></ul><br /><p><strong>At Inspire, no citizen shall be discriminated against based on race, caste, religion, creed, descent, or place of birth in respect of any employment or office under the State.</strong></p><br />",Senior Data Engineer,1500000,2500000,https://www.naukri.com/job-listings-senior-data-engineer-inspire-analytics-chennai-3-to-5-years-040622000016,"['GIT', 'Relational Databases', 'Aws Sagemaker', 'Data Cleansing', 'Version Control', 'Jupyter Notebook', 'AWS']",['Chennai'],Data Engineer,2022-12-02 19:54:24
011222906867,"<p><strong>Description</strong></p><p>Data platform engineering team is looking for a flexible, skilled, highly driven Senior Data Engineer with strong technical abilities, outstanding communication skills and ability to mentor the team to achieve outstanding results. The individual will help lead development and operations for our real time data analytic and visualization platform that will be used for our next generation console, PlayStation 5!</p><p><strong>Responsibilities</strong></p><ul><li> You will play a senior role in design, build, code review, deploy, and support software for our Real Time Data Analytics and Visualization Platform.</li><li> You will be part of a team building streaming pipeline solutions to reshape data and ingest into the platform in near real time.</li><li> This position requires extensive hands-on technical domain expertise in data infrastructure and visualization within the cloud (preferably AWS) ecosystem.</li><li> Collaborate with the team to lead Technology product evaluations, POC execution and platform implementation for the Visualization platform.</li><li> You will participate in product road-map discussions and identify key areas for improvements for the analytics and visualization platform.</li><li> Take on technical ownership of a work stream, be the subject matter expert and managing risks and issues</li></ul><p><strong>Qualifications</strong></p><ul><li> BS Degree in Engineering, Computer Science or equivalent experience.</li><li> 5+ years experience in software development, design and analysis using Scala, Java, or Python</li><li> Proficient in using visualization technology such as Domo, Tableau, Quicksight, imply etc.( Quicksight preferred) from data discovery, dashboard prototyping, to productionize large scale visual analytics platform implementation, performance tuning and optimization.</li><li> Strong foundation in data engineering, data structures and software design.</li><li> Experience with big data analytics and visualization. </li><li> Experience with real time data streaming technologies such as Kafka, Kinesis - Prior experience in creating kafka producer/consumer code in scala/java to move/transform data in real time</li><li> Experience with OLAP/MOLAP database technologies such as Apache Druid, Pinot, Clickhouse</li><li> Experience delivering high performance, active-active, scalable services.</li><li> Experience with container technologies, such as Docker, Kubernetes, AWS EKS</li><li> Hands-on experience in cloud-based web services, at enterprise scale, AWS is preferable.</li><li> Possess the drive and passion for quality with the ability to inspire, excite and motivate other team members.</li><li> Strong verbal and written communication skills and be able to work with others at all levels, effective at working with geographically remote and culturally diverse teams.</li><li> Strong sense of ownership and passion to provide world class customer experience.</li><li> Keen eye for detail and handling multiple simultaneous dependencies.</li></ul>",Senior Data Engineer - Analytics & Visualization,700000,1200000,https://www.naukri.com/job-listings-senior-data-engineer-analytics-visualization-applied-cloud-computing-mumbai-5-to-10-years-011222906867,"['AWS EKS', 'python', 'software development', 'big data analytics', 'Scala', 'Tableau', 'java', 'Docker', 'kafka', 'Data Visualization', 'data structures', 'Kubernetes']",['Mumbai'],Data Engineer,2022-12-01 13:48:37
261122500192,"<div> <ul> <li> <p> <span> You will be a part of the team responsible for technical design building a nonstandard, customer dedicated data management and reporting solutions. </span> <span> </span> </p> </li> <li> <p> <span> Modelling the data from <span> </span> </span> <span> various sou </span> <span> rces and technologies. </span> </p> </li> <li> <p> <span> Troubleshooting and support of the most complex and high impact problems to deliver new features and functionality with the latest cloud technologies. </span> </p> </li> <li> <p> <span> Close cooperation with other Teams across the organization. </span> </p> </li> </ul> <p> </p> <span> </span> </div> <div> Requirements:  <p> Must have: </p> <ul> <li> <p> <span> 5-8 years of experience as a Data Engineer. </span> <span> </span> </p> </li> <li> <p> <span> Experience with  </span> GCP. </p> </li> <li> <p> <span> Programming skills (SQL, other scripting). </span> <span> </span> </p> </li> <li> <p> <span> Experienced in data warehouse customer dedicated solutions. </span> </p> </li> <li> <p> <span> Experienced with relational databases, data models/diagrams. </span> </p> </li> <li> <p> <span> Understanding Data Mining, Data Modeling, Data Provisioning (acquisition, transformation and sharing). </span> </p> </li> <li> <p> <span> Tools knowledge: Git, Jira, Confluence, etc. </span> </p> </li> <li> <p> <span> Strong written and verbal communication skills to influence others. </span> </p> </li> <li> <p> Problem-solving skills. </p> </li> <li> <p> <span> Ability to manage multiple priorities. </span> </p> </li> <li> <p> <span> Open for new technologies and solutions. </span> </p> </li> <li> <p> <span> Growth mindset - you do not need to know it all if you can learn fast. </span> </p> </li> <li> <p> <span> Fluent English (B2 or higher). </span> </p> </li> </ul> <p> <span> </span> Good to have: </p> <ul> <li> <p> <span> Experience with cloud-based infrastructure systems, </span> </p> </li> <li> <p> <span> Experience with GCP BI solutions, </span> </p> </li> <li> <p> <span> Experience working in distributed teams, </span> </p> </li> <li> <p> Experience in multinational environment, </p> </li> <li> <p> Linux, Shell Scripting, </p> </li> <li> <p> Basic Python. </p> </li> </ul> </div>",Senior Data Engineer,600000,900000,https://www.naukri.com/job-listings-senior-data-engineer-lingaro-group-remote-5-to-8-years-261122500192,"['Linux', 'Data management', 'Data modeling', 'GCP', 'Shell scripting', 'Troubleshooting', 'Data mining', 'JIRA', 'SQL', 'Python']",['remote'],Data Engineer,2022-11-26 18:53:45
221122010161,<p><strong>Roles and Responsibilities</strong> </p><br /><br /><p><strong>Desired Candidate Profile</strong> </p><br /><br /><p><strong>Perks and Benefits</strong> </p><br /><br />,Sr. Data Engineer,650000,1400000,https://www.naukri.com/job-listings-sr-data-engineer-mouri-tech-gurgaon-gurugram-4-to-9-years-221122010161,"['Senior Data Engineer', 'SQL']",['Gurgaon'],Data Engineer,2022-11-29 09:54:05
021222007261,"<p><strong> Job Role:</strong></p><ul><li>Building database solutions and validating their stability and efficiency</li><li>Creating program views, functions and stored procedures</li><li>Writing optimized SQL queries for integration with other applications</li><li>Developing scripts, procedures and triggers for application development</li><li>Maintaining data quality and backups and overseeing database security</li><li>Build the infrastructure required for optimal extraction, transformation, and loading of data</li><li>Analyze existing SQL queries for performance improvements and suggest new queries</li></ul><br /><p><strong>Required Skills:</strong></p><ul><li>3+ years of experience as SQL Developer</li><li>Experience with some of the relational databases like SQL Server, Oracle, PostgresSQL, MYSQL</li><li>Skilled at optimizing large complicated SQL statements</li><li>Capable of troubleshooting common database issues</li><li>Familiar with tools that can aid with profiling server resource usage and optimizing it</li><li>Knowledge of data modelling principles</li><li>Knowledge of at least one ETL tool (SSIS, Informatica etc.)</li><li>Good Communication skills</li><li>Experience with some of the modern relational databases like Azure SQL, Azure Synapse, AWS RedShift, AWS RDS is a plus </li></ul><br /><br /><p><strong>Perks and Benefits:</strong></p><ul><li>We follow five days a week culture</li><li>Flexible work hours</li><li>Medical Insurance Cover</li><li>Maternity/Paternity Benefits</li><li>Opportunities to work on the latest technologies, challenging enterprise-level applications</li><li>Follows flat hierarchy</li><li>Get rewarded for excellent performances</li></ul><br /><br /><br />",Sr. Data Engineer ( MS SQL ),700000,1300000,https://www.naukri.com/job-listings-sr-data-engineer-ms-sql-classic-informatics-gurgaon-gurugram-3-to-6-years-021222007261,"['Postgresql', 'Informatica', 'SSIS', 'Azure SQL', 'Azure Synapse', 'AWS RDS', 'MySQL', 'AWS RedShift', 'Oracle', 'ETL', 'SQL Developer']",['Gurgaon'],Data Engineer,2022-12-02 14:58:36
171122911192,"<b>The Role</b><br /><div><p><span>The new SEAM organization integrates Safety, Environment & Asset Management activities, with a broad geographical footprint, that will support Shell’s business & assets around the world<strong>.</strong></span></p><p><b><span>The vision of SEAM is to provide capability across the spectrum of Safety, Environment and Asset Management with:</span></b></p><ul><li>shaping the future ways of working through introducing to the business new technology and news way of working, including e.g., digital, SBO, and real-time, data-driven, end-to-end optimization and risk management, but also global programs like Human Performance Based Safety Philosophy.</li><li>providing performance feedback driving disciplined execution to deliver reliable, predictable results</li><li>helping the businesses build their improvement plans and provide support to execute them</li><li>sustaining performance through strong core capabilities (internal, including SBO & contingent) through building expertise and a company-wide consistent approach</li></ul><p>The VP TAO will be accountable for maximizing integrated business value across the organization. Technical Asset Operations (TAO) is a key enabler for the accelerated delivery of Shell’s Asset Management System and will help us to reach our ultimate potential in Downstream Manufacturing, Integrated Gas and Upstream. TAO provides high quality and cost-competitive technical resources who are not physically present at site yet are an integral part of asset teams delivering value through end-to-end AMS work processes.</p><p>The role of Sr. Process Data Engineer in Turnaround Planning is to actively contribute to the development of work packages and execution plans for all disciplines, including project and operational activities which are integrated into a single multi-disciplinary schedule. The overall aim is to maximize the re-use of proven work-packs for recurring or similar activities, and in doing so avoid unnecessary re-work for each event, introducing waste and loss of corporate memory.</p><p>The purpose of the Turnaround planning activities is to identify the steps and resources necessary for the safe and efficient execution of maintenance activities according to defined quality standards. A Work Package defines in detail the activities to complete a task, relationships between tasks, identifies all information, materials and services for efficient execution and scheduling for UPS/IS/DSM assets of Shell.</p><p>Effective T/A planning offers the reward of a safe, high quality, predictable, repeatable, and effective execution of an Event which will in turn lead to cost competitiveness.</p><p><b><u>Purpose & Accountabilities</u></b></p><p><b><u>Technical Expertise & Active Contribution</u></b>:</p><ul><li>Primary responsibility in preparing/developing work plan with comprehensive work packs from Calibration reports, maintenance reports, general task list, equipment specific task lists, spare part list, equipment drawings, circuit diagrams, P&IDs, instrument logics, cause & effects, Equipment data sheets, operation, and maintenance manuals of OEMs etc., for Field Instruments, Analyzer instruments, Control system instruments etc., for various operating units as per Shell Standards</li><li>Lead Turnaround work scope collection process and significant contributor to the scope challenge and selection process.</li><li>Develop comprehensive plans in accordance with site Safe Work Practices and HSSE guidelines, and include all materials, manpower, work sequencing, rental equipment & tools, and entering requisitions for materials and services.</li><li>Building work packs include detailed description of the tasks that need to be performed for maintenance of each equipment, parts required, tools required, time required and relationship between tasks including the pre-work preparation steps.</li><li>Planning Work orders in SAP/Primavera and building library work packs that include detailed description of the tasks that need to be performed for time & condition-based maintenance, breakdown maintenance, ordering parts required, service or tools required, estimating time required, Estimating cost of work order, network mapping and quality check points for minimum assurance tasks.</li><li>Contribute to quality and productivity of work preparation so that Instrumentation work is timely, cost-efficient and practically feasible. Reviewing all Instrumentation work packages. Register, distribute, calculate, handle additional/less work for the Instrumentation work. Can actively contribute to integration within services department and optimizing the work processes.  Job hazard assessment, setting out risks anticipated required while performing tasks and creating Permit to work.</li><li>Review/approves all request for deviations from Planning Premises document proposed and seeks approval from TA Manager or Event Manager.</li><li>Responsible for Addenda Process implementation after the scope is frozen.</li><li>Accountable for the application of Scheduling Premises document to ensure optimization of schedule and efficient execution of the turnaround scope –leveled resource plan, critical path, floating understanding, minimization of schedule complexity.</li><li>Is mindful of and continues to look for opportunities to reduce turnaround scope, and plan and execute with low-cost mindset.</li><li>LEAN mindset that proposes and implements improvement ideas to both the planning and field execution process to improve performance and reduce cost.</li><li>Coordination with numerous other Shell disciplines – operations, engineering, procurement, finance.</li><li>Demonstrate Process Safety Management.</li><li>Understand and apply Management of Change, MOC principle and process to manage changes and deviation from norms or standards.</li><li>Use the TA Planning & Scheduling tools viz. Primavera P6, for efficient TA Planning.</li><li>Create subfolders based on Work Pack ID (Correspondence, Materials-Services, Work pack etc.)</li><li>Extract technical data for the scope items out of the following systems: GSAP, Digital Twin, Inview, Cintellate etc.</li><li>If data is exported from Digital Twin/Inview to SharePoint, check for revision updates.</li><li>Save the extracted data in the right subfolder according to the correct method.</li><li>If able to find documents in Digital Twin when collecting scope specific data, for example: Isometrics, PEFS, EHT, P&ID etc., attached to some other FLOC collect/ save this data to make them linked to correct Floc position.</li><li>Keep track of the progress in a dashboard that monitors progress on FLOCS to be processed.</li><li>A self-starter leader and reliable deliverer, with very good verbal and written skills in English, able to negotiate with people, and able to resist undue influence that might otherwise compromise integrity of data quality.</li><li>Sound understanding of business/process workflow and having mature mindset to deal and behave under tough/challenging situations.</li></ul><p><b><u>Ensure compliance</u></b>:</p><p>Ensure that activities are executed in line with Shell’s policies and standards. (Data privacy/protection, HSSE&SP, DEP, AMS processes & others).</p><ul><li>Prepare Initial response, carry out assessment, ensure correct prioritization of work, troubleshoot & propose technical solutions virtually, co-ordinate discussion with stakeholders, conduct Risk Assessment, ensure good QA/QC, record keeping, & final job closure to enable high equipment reliability & plant technical availability.</li><li>Constantly engage with site maintenance leads, original equipment manufacturers, vendors and modify the work packs as recommended/required by the site maintenance personnel.</li><li>Understanding the functions of the various equipment and instruments and thorough knowledge of reading/ interpreting the engineering drawings (P&IDs, PEFS, PFDs, EHT, Data Sheets etc.).</li><li>Identifying & resolving the bottlenecks for data analysis, maintaining issue logs and use the same in implementing changes/improvements in the processes.</li><li>Should be able to draft permits and demonstrate understanding of JHA, HRA, HAZID and similar risk assessment and their control mechanisms.</li><li>Clear understanding of maintenance/TA strategies for preventive maintenance and their implementation in CMMS.</li><li>Able to initiate a purchase requisition, track supply chain, find the right material masters and verify the correctness of bills of materials.</li><li>Understand the concept of manhour estimation for maintenance activities by incorporating HOTT, referring Global norms and best practices etc.</li><li>The candidate should have worked in the capacity of and now be able to work closely with Engineering, Supply Chain, scheduling to develop a plan for executing the work on field. This also involves coordination knowledge with Electrical, Instrumentation and Service Support Teams and basic idea about their work planning mechanism.</li><li>Should be able to coordinate with Third Party Vendors, OEMs, and Service Providers for execution of specific targeted maintenance/TA scopes. Understand the process of mobilization of manpower to the asset.</li><li>Awareness of the statutory, safety and regulatory compliances for asset operations to enable work planning for these scopes cater to the specific requirement.</li><li>Demonstrate sound knowledge in contracts management, document management, service entries and EFAs.</li><li>Understand the concept of work clustering for complex multi-discipline scopes using scheduling tools like Primavera to be able to optimize timeline for a shutdown or campaign.</li></ul><p><b><i>Dimensions</i></b></p><ul><li>No Annual budgets which you directly control but can have influence over department and production area budgets.</li><li>No direct staff, but the candidate is expected to coach/mentor less experienced staff.</li><li>Exposes the individual to all cultural backgrounds and organizational levels, across diverse time zones.</li><li>The role will involve working in shifts (Afternoon/Evening/Late Evening Shifts) to have enough interface with the asset/plant to be supported remotely/virtually.</li><li>Ability to work under pressure and perform multitask simultaneously.</li><li>May involve travelling to various operating units/sites across the globe as per requirements.</li></ul><p><b><i>Skills & Requirements</i></b></p><ul><li>University Degree in Electronics & Instrumentation / Instrumentation & control/ Electronics & communication/ Mechatronics Engineering</li><li>Minimum 5 plus years’ direct work experience in supporting planning, execution & supervision of Major Turnarounds/ Shutdowns in Upstream Oil & Gas/ Refineries/ Petrochemicals industries with experience in One or more of following areas:</li><li>Maintenance & troubleshooting of instruments for heavy rotating equipment’s like turbines, engines, compressors, centrifugal pumps, etc., & maintenance of instruments in static equipment like columns, safety valves, pressure vessels, heat exchangers, heaters, boilers, and relevant field instrumentation etc. Experience in Installation & commissioning/operations & design would be a n added advantage</li><li>Should have sound knowledge in Shutdown / turnaround Milestones i.e., Concept Phase, Definition Phase, Detailed Planning Phase, Pre-shutdown Phase, Turnaround Execution Phase & Post shutdown / turnaround Execution Phase.</li><li>Installation & commissioning/operations, design, engineering & selection of instruments would be an added advantage.</li><li>Experience in preparing the maintenance work instructions, task lists, work packs, work, and manpower estimation for major maintenance/overhauling of instruments available in oil and gas industry.</li><li>Experience in materials or resources to be ordered, in advance of the work order being issued, such that the scheduler can immediately bring the work order into the schedule without any intervention, Participated and supported ISO/safety-MS audits, as well as other related external audits, as part of Company Safety Control framework and management system</li><li>Has good understanding towards Turnaround strategy and can align with the Strategic asset management plan to ensure that strategic elements such as projects maintenance and inspection work, other critical activities, as well as external constraints and opportunities are aligned to drive optimal business outcomes.</li><li>Knowledge in managing the turnaround scope development process which includes gathering multi-disciplinary (including Engineering, Operations and Projects) scope requiring a T/A, validation against scope entry criteria and freezing of the scope for the T/A to deliver within the premises of the T/A to achieve overall top quartile Asset performance.</li><li><b>Knowledge in working with Primavera -P6 is Required</b></li><li>Skills in MS Office tools like Excel are highly desirable</li><li>Hands on Experience in Plant Maintenance Area as an engineering/end user of Maximo EAM / SAP PM Module/ Computerized Maintenance Management Systems (CMMS)/Document Management System etc.</li><li>Strong aptitude for Learner Mindset.</li><li>Flexibility to move quickly across changing priorities and manage multiple Projects/Programs.</li><li>Ability to independently, resourcefully, and creatively research and implement new solutions.</li><li>Effective communication skills and stakeholder management are necessary for the job</li><li>A good understanding of the Upstream/DS/IG business and how it works.</li><li>Ability to engage and effectively communicate at all levels inside and outside Shell and within different cultural settings.</li><li>Strong interpersonal skills, ability to challenge and ability to build internal and external relationships based on trust and to integrate across multiple functions.</li><li>Leadership qualities & ability to lead the activities & projects.</li><li>Able to multi-task, prioritize and ensure delivery of priorities as promised, work without close supervision, and work through others to deliver results.</li><li>Learner mindset-Ability to recognize & learn from mistakes.</li><li>Resilience & ability to adopt to changes.</li><li>Ability to resolve conflict and solve problems.</li><li>Team spirit & ability to help others.</li><li>Continuous Improvement and Knowledge of Lean CI methodology is an added value.</li></ul></div>",Senior Process Data Engineer-1,700000,1200000,https://www.naukri.com/job-listings-senior-process-data-engineer-1-shell-india-markets-private-limited-chennai-5-to-10-years-171122911192,"['SAP PM Module', 'plant maintenance', 'Maximo EAM', 'maintenance management', 'heat exchangers', 'design engineering', 'centrifugal pumps']",['Chennai'],Data Engineer,2022-12-01 12:02:53
171122909655,"The Role<br /><div><p><span>The new SEAM organization integrates Safety, Environment & Asset Management activities, with a broad geographical footprint, that will support Shell’s business & assets around the world.</span></p><p><span>The vision of SEAM is to provide capability across the spectrum of Safety, Environment and Asset Management with:</span></p><ul><li>shaping the future ways of working through introducing to the business new technology and news way of working, including e.g., digital, SBO, and real-time, data-driven, end-to-end optimization and risk management, but also global programs like Human Performance Based Safety Philosophy.</li><li>providing performance feedback driving disciplined execution to deliver reliable, predictable results</li><li>helping the businesses build their improvement plans and provide support to execute them</li><li>sustaining performance through strong core capabilities (internal, including SBO & contingent) through building expertise and a company-wide consistent approach</li></ul><p><span></span></p><p><span>The role of Sr. Process Data Engineer in Turnaround Planning is to actively contribute to the development of work packages and execution plans for all disciplines, including project and operational activities which are integrated into a single multi-disciplinary schedule. The overall aim is to maximize the re-use of proven work-packs for recurring or similar activities, and in doing so avoid unnecessary re-work for each event, introducing waste and loss of corporate memory. </span></p><p><span>The purpose of the Turnaround planning activities is to identify the steps and resources necessary for the safe and efficient execution of maintenance activities according to defined quality standards. A Work Package defines in detail the activities to complete a task, relationships between tasks, identifies all information, materials and services for efficient execution and scheduling for UPS/IS/DSM assets of Shell.</span></p><p><span>Effective T/A planning offers the reward of a safe, high quality, predictable, repeatable, and effective execution of an Event which will in turn lead to cost competitiveness.</span></p><p><u><span>Purpose & Accountabilities</span></u></p><p><u><span>Technical Expertise & Active Contribution</span></u><span>:</span></p><ul><li>Primary responsibility in preparing/developing work plan with comprehensive work packs from Calibration reports, maintenance reports, general task list, equipment specific task lists, spare part list, equipment’s drawings, circuit diagrams, P&IDs, instrument logics, cause & effects , Equipment data sheets, operation and maintenance manuals of OEMs etc., for Electrical equipment in field and in substation for operating units/plants as per Shell Standards</li><li>Lead Turnaround work scope collection process and significant contributor to the scope challenge and selection process.</li><li>Develop comprehensive plans in accordance with site Safe Work Practices and HSSE guidelines, and include all materials, manpower, work sequencing, rental equipment & tools, and entering requisitions for materials and services.</li><li>Building work packs include detailed description of the tasks that need to be performed for maintenance of each equipment, parts required, tools required, time required and relationship between tasks including the pre-work preparation steps. </li><li>Planning Work orders in SAP/Primavera and building library work packs that include detailed description of the tasks that need to be performed for time & condition-based maintenance, breakdown maintenance, ordering parts required, service or tools required, estimating time required, Estimating cost of work order, network mapping and quality check points for minimum assurance tasks. </li><li>Contribute to quality and productivity of work preparation so that Electrical work is timely, cost-efficient and practically feasible. Reviewing all Electrical work packages. Register, distribute, calculate, handle additional/less work for the Electrical work. Can actively contribute to integration within services department and optimizing the work processes.  Job hazard assessment, setting out risks anticipated required while performing tasks and creating Permit to work. </li><li>Review/approves all request for deviations from Planning Premises document proposed and seeks approval from TA Manager or Event Manager.</li><li>Responsible for Addenda Process implementation after the scope is frozen.</li><li>Accountable for the application of Scheduling Premises document to ensure optimization of schedule and efficient execution of the turnaround scope –leveled resource plan, critical path, floating understanding, minimization of schedule complexity.</li><li>Is mindful of and continues to look for opportunities to reduce turnaround scope, and plan and execute with low-cost mindset.</li><li>LEAN mindset that proposes and implements improvement ideas to both the planning and field execution process to improve performance and reduce cost</li><li>Coordination with numerous other Shell disciplines – operations, engineering, procurement, finance.</li><li>Demonstrate Process Safety Management.</li><li>Understand and apply Management of Change, MOC principle and process to manage changes and deviation from norms or standards.</li><li>Use the TA Planning & Scheduling tools viz. Primavera P6, for efficient TA Planning. </li><li>Create subfolders based on Work Pack ID (Correspondence, Materials-Services, Workpack etc.)</li><li>Extract technical data for the scope items out of the following systems: GSAP, Digital Twin, Inview, Cintellate etc.</li><li>If data is exported from Digital Twin/Inview to SharePoint, check for revision updates.</li><li>Save the extracted data in the right subfolder according to the correct method.</li><li>If able to find documents in Digital Twin when collecting scope specific data, for example: Isometrics, PEFS, EHT, P&ID etc., attached to some other FLOC collect/ save this data to make them linked to correct Floc position.</li><li>Keep track of the progress in a dashboard that monitors progress on FLOCS to be processed.</li><li>A self-starter leader and reliable deliverer, with very good verbal and written skills in English, able to negotiate with people, and able to resist undue influence that might otherwise compromise integrity of data quality.</li><li>Sound understanding of business/process workflow and having mature mindset to deal and behave under tough/challenging situations.</li></ul><p><u><span>Ensure compliance</span></u>:</p><p><span>Ensure that activities are executed in line with Shell’s policies and standards. (Data privacy/protection, HSSE&SP, DEP, AMS processes & others).</span></p><ul><li>Prepare Initial response, carry out assessment, ensure correct prioritization of work, troubleshoot & propose technical solutions virtually, co-ordinate discussion with stakeholders, conduct Risk Assessment, ensure good QA/QC, record keeping, & final job closure to enable high equipment reliability & plant technical availability.</li><li>Constantly engage with site maintenance leads, original equipment manufacturers, vendors and modify the work packs as recommended/required by the site maintenance personnel.</li><li>Understanding the functions of the various equipment and instruments and thorough knowledge of reading/ interpreting the engineering drawings (P&IDs, PEFS, PFDs, EHT, Data Sheets etc.).</li><li>Identifying & resolving the bottlenecks for data analysis, maintaining issue logs and use the same in implementing changes/improvements in the processes.</li><li>Should be able to draft permits and demonstrate understanding of JHA, HRA, HAZID and similar risk assessment and their control mechanisms.</li><li>Clear understanding of maintenance/TA strategies for preventive maintenance and their implementation in CMMS. </li><li>Able to initiate a purchase requisition, track supply chain, find the right material masters and verify the correctness of bills of materials.</li><li>Understand the concept of manhour estimation for maintenance activities by incorporating HOTT, referring Global norms and best practices etc.</li><li>The candidate should have worked in the capacity of and now be able to work closely with Engineering, Supply Chain, scheduling to develop a plan for executing the work on field. This also involves coordination knowledge with Electrical, Instrumentation and Service Support Teams and basic idea about their work planning mechanism.</li><li>Should be able to coordinate with Third Party Vendors, OEMs, and Service Providers for execution of specific targeted maintenance/TA scopes. Understand the process of mobilization of manpower to the asset.</li><li>Awareness of the statutory, safety and regulatory compliances for asset operations to enable work planning for these scopes cater to the specific requirement.</li><li>Demonstrate sound knowledge in contracts management, document management, service entries and EFAs.</li><li>Understand the concept of work clustering for complex multi-discipline scopes using scheduling tools like Primavera to be able to optimize timeline for a shutdown or campaign.</li></ul><p><i><span>Dimensions</span></i></p><ul><li>No Annual budgets which you directly control but can have influence over department and production area budgets.</li><li>No direct staff, but the candidate is expected to coach/mentor less experienced staff.</li><li>Exposes the individual to all cultural backgrounds and organizational levels, across diverse time zones.</li><li>The role will involve working in shifts (Afternoon/Evening/Late Evening Shifts) to have enough interface with the asset/plant to be supported remotely/virtually.</li><li>Ability to work under pressure and perform multitask simultaneously.</li><li>May involve travelling to various operating units/sites across the globe as per requirements.</li></ul><p><i><span>Skills & Requirements</span></i></p><ul><li>University Degree in Electrical/ Electrical & Electronics / Electronics & Communication Engineering</li><li>Minimum 5 plus years’ direct work experience in supporting planning, execution & supervision of Major Turnarounds/ Shutdowns in Upstream Oil & Gas/ Refineries/ Petrochemicals industries with experience in One or more of following areas:</li><li>Field electrical equipment viz motors, transformers, switchgears, VCB, ACB, Contactors, batteries, battery charger, VFDs, areas at HV/LV voltage levels etc.</li><li>Should have sound knowledge in Shutdown / turnaround Milestones i.e., Concept Phase, Definition Phase, Detailed Planning Phase, Pre-shutdown Phase, Turnaround Execution Phase & Post shutdown / turnaround Execution Phase.</li><li>Installation & commissioning/operations, design, engineering & selection of electrical equipment’s would be an added advantage. </li><li>Experience in preparing the maintenance work instructions, task lists, work packs, work, and manpower estimation for major maintenance/overhauling of electrical equipment available in oil and gas industry. </li><li>Experience in materials or resources to be ordered, in advance of the work order being issued, such that the scheduler can immediately bring the work order into the schedule without any intervention, Participated and supported ISO/safety-MS audits, as well as other related external audits, as part of Company Safety Control framework and management system</li><li>Has good understanding towards Turnaround strategy and can align with the Strategic asset management plan to ensure that strategic elements such as projects maintenance and inspection work, other critical activities, as well as external constraints and opportunities are aligned to drive optimal business outcomes. </li><li>Knowledge in managing the turnaround scope development process which includes gathering multi-disciplinary (including Engineering, Operations and Projects) scope requiring a T/A, validation against scope entry criteria and freezing of the scope for the T/A to deliver within the premises of the T/A to achieve overall top quartile Asset performance.</li><li>Knowledge in working with Primavera -P6 is Required</li><li>Skills in MS Office tools like Excel are highly desirable</li><li>Hands on Experience in Plant Maintenance Area as an engineering/end user of Maximo EAM / SAP PM Module/ Computerized Maintenance Management Systems (CMMS)/Document Management System etc.</li><li>Strong aptitude for Learner Mindset.</li><li>Flexibility to move quickly across changing priorities and manage multiple Projects/Programs.</li><li>Ability to independently, resourcefully, and creatively research and implement new solutions.</li><li>Effective communication skills and stakeholder management are necessary for the job</li><li>A good understanding of the Upstream/DS/IG business and how it works.</li><li>Ability to engage and effectively communicate at all levels inside and outside Shell and within different cultural settings.</li><li>Strong interpersonal skills, ability to challenge and ability to build internal and external relationships based on trust and to integrate across multiple functions.</li><li>Leadership qualities & ability to lead the activities & projects.</li><li>Able to multi-task, prioritize and ensure delivery of priorities as promised, work without close supervision, and work through others to deliver results.</li><li>Learner mindset-Ability to recognize & learn from mistakes.</li><li>Resilience & ability to adopt to changes.</li><li>Ability to resolve conflict and solve problems.</li><li>Team spirit & ability to help others.</li><li>Continuous Improvement and Knowledge of Lean CI methodology is an added value.</li></ul></div>",Senior Process Data Engineer-2,700000,900000,https://www.naukri.com/job-listings-senior-process-data-engineer-2-shell-india-markets-private-limited-chennai-5-to-7-years-171122909655,"['SAP PM Module', 'Excel', 'MS Office', 'Primavera', 'Lean CI']",['Chennai'],Data Engineer,2022-12-01 12:01:00
171122908712,"The Role<br /><div><p><span>The new SEAM organization integrates Safety, Environment & Asset Management activities, with a broad geographical footprint, that will support Shell’s business & assets around the world.</span></p><p><span>The vision of SEAM is to provide capability across the spectrum of Safety, Environment and Asset Management with:</span></p><ul><li>shaping the future ways of working through introducing to the business new technology and news way of working, including e.g., digital, SBO, and real-time, data-driven, end-to-end optimization and risk management, but also global programmes like Human Performance Based Safety Philosophy.</li><li>providing performance feedback driving disciplined execution to deliver reliable, predictable results</li><li>helping the businesses build their improvement plans and provide support to execute them</li><li>sustaining performance through strong core capabilities (internal, including SBO & contingent) through building expertise and a company-wide consistent approach</li></ul><p>The VP TAO will be accountable for maximizing integrated business value across the organization. Technical Asset Operations (TAO) is a key enabler for the accelerated delivery of Shell’s Asset Management System and will help us to reach our ultimate potential in Downstream Manufacturing, Integrated Gas and Upstream. TAO provides high quality and cost-competitive technical resources who are not physically present at site yet are an integral part of asset teams delivering value through end-to-end AMS work processes.</p><p>The role of Sr. Process Data Engineer in Turnaround Planning is to actively contribute to the development of work packages and execution plans for all disciplines, including project and operational activities which are integrated into a single multi-disciplinary schedule. The overall aim is to maximize the re-use of proven work-packs for recurring or similar activities, and in doing so avoid unnecessary re-work for each event, introducing waste and loss of corporate memory.</p><p>The purpose of the Turnaround planning activities is to identify the steps and resources necessary for the safe and efficient execution of maintenance activities according to defined quality standards. A Work Package defines in detail the activities to complete a task, relationships between tasks, identifies all information, materials and services for efficient execution and scheduling for UPS/IS/DSM assets of Shell.</p><p>Effective T/A planning offers the reward of a safe, high quality, predictable, repeatable, and effective execution of an Event which will in turn lead to cost competitiveness.</p><p><b><u>Purpose & Accountabilities</u></b></p><p><b><u>Technical Expertise & Active Contribution</u></b>:</p><ul><li>Responsible for preparing comprehensive work packs for major maintenance/ Overhauling of heavy-duty rotating equipment like engines, compressors, pumps, turbines etc., Also responsible for preparing E&I Instruments as per requirements.</li><li>Lead Turnaround work scope collection process and significant contributor to the scope challenge and selection process.</li><li>Will be responsible to develop Workpacks which shall include but not limited to,Shutdown and blowdown sequence (Operations)</li><li>Boundary Isolations</li><li>Equipment isolations (based on the work scope of the T/A)</li><li>Workpacks for all mechanical works, electrical and PACO (with inputs from the Maintenance, turnaround, and Projects Engineers)</li><li>Quality AssuranceTIP (Test Inspect Plan)</li><li>Flange management</li><li>Mechanical Completion Cert</li><li>Punchlist template</li></ul><li>Leak Testing</li><li>Facility walkdown and normalization in prep to start up</li><li>SURU (Start Up Ramp Up)</li><ul></ul><li>Review/approves all request for deviations from Planning Premises document proposed and seeks approval from TA Manager or Event Manager.</li><li>Responsible for Addenda Process implementation after the scope is frozen.</li><li>Accountable for the application of Scheduling Premises document to ensure optimization of schedule and efficient execution of the turnaround scope –leveled resource plan, critical path, floating understanding, minimization of schedule complexity.</li><li>Is mindful of and continues to look for opportunities to reduce turnaround scope, and plan and execute with low-cost mindset.</li><li>LEAN mindset that proposes and implements improvement ideas to both the planning and field execution process to improve performance and reduce cost.</li><li>Coordination with numerous other Shell disciplines – operations, engineering, procurement, finance</li><li>Demonstrate Process Safety Management.</li><li>Understand and apply Management of Change, MOC principle and process to manage changes and deviation from norms or standards.</li><li>Use the TA Planning & Scheduling tools viz. Primavera P6, Roser for efficient TA Planning.</li><li>Create subfolders based on equipment tag per Unit (RDU, RVC, etc.)</li><li>Extract technical data for the scope items out of the following systems: GSAP, AIM, Prima Vera P6, Roser, IN tools, SIFpro.</li><li>If data is exported from Prima Vera P6 / Roser to SharePoint, check for revision updates.</li><li>Save the extract data in the right subfolder according to the correct method.</li><li>If find useful information for the next TA in Primavera P6 /Roser when collecting scope specific data, for example: extradentary scaffolding information, work method statements, etc., collect/ save this data as well in the subfolder.</li><li>Keep track of the progress in a dashboard that monitors progress on FLOCS to be processed.</li><ul></ul><p><b><u>Ensure compliance</u></b>:</p><p>Ensure that activities are executed in line with Shell’s policies and standards. (Data privacy/protection, HSSE&SP, DEP, AMS processes & others).</p><ul><li>Prepare Initial response, carry out assessment, ensure correct prioritization of work, troubleshoot & propose technical solutions virtually, co-ordinate discussion with stakeholders, conduct Risk Assessment, ensure good QA/QC, record keeping, & final job closure to enable high equipment reliability & plant technical availability.</li><li>Constantly engage with site maintenance leads, original equipment manufacturers, vendors and modify the work packs as recommended/required by the site maintenance personnel.</li><li>Anticipate & plan for mandatory spares, tools and tackles and manpower required for the heavy-duty equipment based on the information extracted from maintenance manuals, collate any additional information about equipment & its spares, if required from the vendors and accommodate the information in the workpack.</li><li>Understanding the functions of the various equipment and instruments and thorough knowledge of reading/ interpreting the engineering drawings (P&IDs, PEFS, PFDs).</li><li>Identifying & resolving the bottlenecks for data analysis, maintaining issue logs and use the same in implementing changes/improvements in the processes.</li><li>Should be able to draft permits and demonstrate understanding of JHA, HRA, HAZID and similar risk assessment and their control mechanisms</li><li>Clear understanding of maintenance/TA strategies for preventive maintenance and their implementation in CMMS.</li><li>Able to initiate a purchase requisition, track supply chain, find the right material masters and verify the correctness of bills of materials.</li><li>Understand the concept of manhour estimation for maintenance activities by incorporating HOTT, referring Global norms and best practices etc.,</li><li>The candidate should be have worked in the capacity of and now be able to work closely with Engineering, Supply Chain, Scheduling to develop a plan for executing the work on field. This also involves coordination knowledge with Electrical, Instrumentation and Service Support Teams and basic idea about their work planning mechanism</li><li>Should be able to coordinate with Third Party Vendors, OEMs, and Service Providers for execution of specific targeted maintenance/TA scopes. Understand the process of mobilization of manpower to offshore asset.</li><li>Awareness of the statutory, safety and regulatory compliances for asset operations to enable work planning for these scopes cater to the specific requirement</li><li>Demonstrate sound knowledge in contracts management, document management, service entries and EFAs.</li><li>Understand the concept of work clustering for complex multi-discipline scopes using scheduling tools like Primavera/ Roser to be able to optimize timeline for a shutdown or campaign.</li></ul><p><b><i>Dimensions</i></b></p><ul><li>No Annual budgets which you directly control but can have influence over department and production area budgets.</li><li>No direct staff, but the candidate is expected to coach/mentor less experienced staff</li><li>Exposes the individual to all cultural backgrounds and organizational levels, across diverse time zones.</li><li>The role will involve working in shifts (Afternoon/Evening/Late Evening Shifts) to have enough interface with the asset/plant to be supported remotely/virtually.</li><li>Ability to work under pressure and perform multitask simultaneously.</li><li>May involve travelling to various operating units/sites across the globe as per requirements.</li></ul><p><b><i>Skills & Requirements</i></b></p><ul><li>Degree in Engineering in Mechanical/Industrial discipline.</li><li>Minimum 5 plus years’ direct work experience in supporting planning, execution & supervision of Major Turnarounds/ Shutdowns in Upstream Oil & Gas/ Refineries/ Petrochemicals industries.</li><li>Should have sound knowledge in Shutdown / turnaround Milestones i.e., Concept Phase, Definition Phase, Detailed Planning Phase, Pre-shutdown Phase, Turnaround Execution Phase & Post shutdown / turnaround Execution Phase.</li><li>Proficiency in working with Primavera -P6/Roser will be preferred.</li><li>Erection & commissioning/ operations and maintenance of heavy rotating equipment like turbines, compressors (Centrifugal/ reciprocating compressors), centrifugal pumps etc., inspection & maintenance of static equipment like columns, safety valves, valves, pressure vessels, heat exchangers etc., Also have experience in turnaround planning E&I equipment’s which is associated with main equipment’s.</li><li>Experience in preparing the maintenance work instructions, task lists, work packs, work, and manpower estimation for major maintenance/overhauling of the rotary/static equipment.</li><li>Has good understanding towards Turnaround strategy and can align with the Strategic asset management plan to ensure that strategic elements such as projects maintenance and inspection work, other critical activities, as well as external constraints and opportunities are aligned to drive optimal business outcomes.</li><li>Knowledge in managing the turnaround scope development process which includes gathering multi-disciplinary (including Engineering, Operations and Projects) scope requiring a T/A, validation against scope entry criteria and freezing of the scope for the T/A to deliver within the premises of the T/A in order to achieve overall top quartile Asset performance.</li><li>Hands on Experience in Plant Maintenance Area as an engineering/end user of Maximo EAM / SAP PM Module/ Computerized Maintenance Management Systems (CMMS)/Document Management System etc.,</li><li>Strong aptitude for Learner Mindset</li><li>Flexibility to move quickly across changing priorities and manage multiple Projects/Programmes</li><li>Ability to independently, resourcefully, and creatively research and implement new solutions</li><li>A good understanding of the Upstream/DS/IG business and how it works.</li><li>Ability to engage and effectively communicate at all levels inside and outside Shell and within different cultural settings.</li><li>Strong interpersonal skills, ability to challenge and ability to build internal and external relationships based on trust and to integrate across multiple functions.</li><li>Continuous Improvement and Knowledge of Lean CI methodology is an added value.</li></ul></div>",Senior Process Data Engineer-4,700000,1000000,https://www.naukri.com/job-listings-senior-process-data-engineer-4-shell-india-markets-private-limited-chennai-5-to-8-years-171122908712,"['SAP PM', 'RDU', 'data analysis', 'Flange management', 'contracts management', 'document management', 'Operations', 'RVC']",['Chennai'],Data Engineer,2022-12-01 12:03:03
171122908457,"<p>The VP TAO will be accountable for maximizing integrated business value across the organization. Technical Asset Operations (TAO) is a key enabler for the accelerated delivery of Shell’s Asset Management System and will help us to reach our ultimate potential in Downstream Manufacturing, Integrated Gas and Upstream. TAO provides high quality and cost-competitive technical resources who are not physically present at site yet are an integral part of asset teams delivering value through end-to-end AMS work processes.</p><p>This role is expected to help Electrical Maintenance engineers in work planning, implementing the standard work packs for preventive/corrective maintenance of various Field Electrical equipment</p><p><b><i>Purpose & Accountabilities</i></b></p><ul><li>Primary responsibility in preparing/developing work plan with comprehensive work packs from Calibration reports, maintenance reports, general task list, equipment specific task lists, spare part list, equipment’s drawings, circuit diagrams, single line diagrams and control circuit logics, cause & effects, Equipment data sheets, operation, and maintenance manuals of OEMs etc., for Electrical equipment in field and in substation for operating units/plants as per Shell Standards.</li><li>Planning PM & CM work orders in Maximo/SAP and building library work packs that include detailed description of the tasks that need to be performed for time & condition-based maintenance, breakdown maintenance, ordering parts required, service or tools required, estimating time required, Estimating cost of work order, network mapping and quality check points for minimum assurance tasks. Following up for services & materials with venders for mobilization and smooth execution of the activity. Customize the equipment make and model specific work packs into site specific work pack by adding site conditions and parameters. Accommodate the PM library changes by converting the turnaround documents to the global work packs. Planning of end-to-end work orders to enable efficient scheduling and ensure compliance of executable work orders. Improving productivity by ensuring the resource, special tool, material, and service requirements availability before the job begins.</li><li>Building work packs that include detailed description of the tasks that need to be performed for maintenance of each equipment, parts required, tools required, time required and relationship between tasks including the pre-work preparation steps.</li><li>Job hazard assessment, setting out risks anticipated required while performing tasks and creating Permit to work.</li><li>Identifying the system condition for each operation activity tasks which would require long term un-availability of equipment & capturing these opportunities in the upcoming pit stop/ shutdown windows or suitable opportunities for maintenance activities.</li><li>First level diagnosis of damage history, determining the failure mode based on anomaly reports and information given in corrective maintenance work request/ notification.</li><li>Investigating and analysis of CM work request to identify the true cause of the problem and prepare work orders with the detailed tasks to repair the damage with as much as detailed information required to complete the job effectively including procedures, sketches, specifications, or drawings deemed necessary.</li><li>Constantly engage with site maintenance leads, original equipment manufacturers, vendors and modify the work packs as recommended/required by the site maintenance personnel.</li><li>Will be assigned as trainer for area of development of comprehensive work packs from equipment specific task lists and work instructions. May share the responsibility with supervisor in staff development of 2 to 10 members.</li><li>Confidence to work in virtual environment and managing engagement across different levels of stakeholders, including senior management for day-to-day operations. Includes initiative to identify and help resolve business and technical issues within areas of responsibility.</li><li>Sound understanding of business/process workflow and having mature mindset to deal and behave under tough/challenging situations.</li><li>Strong proficiency in using English for both spoken and written communication, as well as using Skype for Business, MS Teams, Instant Messaging, Video Conferencing, Outlook, etc.</li><li>Shift work following operating hours of OU’s.</li><li>Resilient under pressure and able to work with people different culture and working style.</li><li>A self-starter leader and reliable deliverer, with very good verbal and written skills in English, able to negotiate with people, and able to resist undue influence that might otherwise compromise integrity of data quality.</li><li>Exposes the individual to all cultural backgrounds and organizational levels</li><li>Sound understanding of business/process workflow and having mature mindset to deal and behave under tough/challenging situations.</li></ul><p><b><i>Dimensions</i></b></p><ul><li>No direct staff, but the candidate is expected to coach/mentor less experienced staff</li></ul><ul><li>The role will involve working in shifts (Afternoon/Evening/Late Evening Shifts) to have enough interface with the asset/plant to be supported remotely/virtually.</li><li>May involve travelling to various operating units/sites across the globe as per requirements.</li></ul><p><b><i>Skills & Requirements</i></b></p><ul><li>University Degree in Electrical/ Electrical & Electronics / Electronics & Communication Engineering.</li><li>Minimum 5 plus years of overall working experience in in Oil & Gas/ Petrochemicals with experience in One or more of following areas:</li><li>Field electrical equipment viz motors, transformers, switchgears, VCB, ACB, Contactors, batteries, battery charger, VFDs, areas at HV/LV voltage levels etc.</li><li>Experience in operation and maintaining electrical equipment, procurement, and equipment selection.</li><li>Installation & commissioning/operations, design, engineering & selection of instruments for electrical equipment’s would be an added advantage.</li><li>Experience in preparing the maintenance work instructions, task lists, work packs, work, and manpower estimation for major maintenance/overhauling of electrical equipment available in oil and gas industry.</li><li>Experience in materials or resources to be ordered, in advance of the work order being issued, such that the scheduler can immediately bring the work order into the schedule without any intervention, Participated and supported ISO/safety-MS audits, as well as other related external audits, as part of Company Safety Control framework and management system.</li><li>Experience in Diagnosing the cause of the problem in field equipment, troubleshooting and determines the scope of activity required to return the equipment back into service by packaging the work to the tasks, disciplines, materials and services to an effective completion, utilizes feedback from past work plan implementation to improve future work plans, Carry out root cause analysis on repeated failures and collaborate with operations and technologist to identify process related degradation/failure mechanism.</li><li>Experience in Contribution to site reliability improvements would be an added advantage. Identify, propose, and execute improvement initiatives to eliminate plant reliability threats. Identify bad actors and make improvements plans to minimize or eliminate bad actors. Troubleshoot equipment failures to address and eliminate the root cause. Provide feedback on effectiveness of preventive maintenance, corrective generated from maintenance execution process.</li><li>Exposure to Electrical equipment condition monitoring whenever required & liaise with relevant subject matter experts/specialist/OEM to address any arising issues, problems & concerns. Provide support to other maintenance discipline, provide support, and allocate required resources for Turnaround execution in related Process Units e.g., scaffold erecting, lifting, excavation, etc.</li><li>Experience in leading team/process/projects is preferred</li><li>Skills in MS Office tools like Excel are highly desirable</li><li>Hands on Experience in Plant Maintenance Area as an engineering/end user of SAP PM Module/ Computerized Maintenance Management Systems (CMMS)/Document Management System etc.</li><li>A good understanding of the Upstream/DS/IG business and how it works.</li><li>Effective communication skills and stakeholder management are necessary for the job.</li><li>Knowledge of Lean CI methodology is an added value.</li><li>Able to multi-task, prioritize and ensure delivery of priorities as promised, work without close supervision, and work through others to deliver results.</li><li>Virtual working experience is highly desirable.</li><li>Professional Engineering certification is an added value.</li></ul>",Senior Process Data Engineer,700000,900000,https://www.naukri.com/job-listings-senior-process-data-engineer-shell-india-markets-private-limited-chennai-5-to-7-years-171122908457,"['electrical maintenance', 'plant maintenance', 'Excel', 'maintenance management', 'MS Office', 'Lean CI', 'design engineering']",['Chennai'],Data Engineer,2022-12-01 12:02:43
291122914722,"<p> </p><p><strong><u>Data Engineer </u></strong><br /> <br /><strong>Experience </strong></p><ul><li>Bachelors degree from Tier I/II colleges (Post Graduation Good to have, not mandatory) </li><li>5-7 years of experience in Data Engineering </li></ul><br /><p><strong>Responsibilities</strong> </p><ul><li>Requirements gathering, system analysis, design, development/configuration of new data sets/marts. </li><li>Build ETL pipelines in Spark / Python / Scala and standardize data fields across various data sources </li><li>Create data dictionaries, setup/monitor data validation alerts and execute periodic jobs </li><li>Define and build technical/data documentation and experience with code version control systems (e.g. git). Ensure data accuracy, integrity and consistency </li></ul><br /><p><strong>Necessary knowledge </strong> </p><ul><li>Proficiency in: </li><li><strong>Programming languages</strong>: Python, SQL </li><li>Apache Spark, Apache Airflow </li><li>Docker, Bash </li><li><strong>Cloud</strong>: Either of below: </li><li>Azure Data Factory </li><li>Google Cloud Dataflow / Data Fusion </li><li>AWS Data Pipeline / Glue </li><li>Stitch / Snowflake </li><li><strong>Data Streaming</strong>: Kafka/Kinesis </li><li><strong>Good to have</strong>: Tableau/PowerBI Other Visualization tools </li><li>Strong experience in creating large scale data engineering pipelines, data marts and warehouses </li><li>Experience with SQL for extracting, aggregating and processing big data Pipelines using Python </li><li>Good working knowledge of using and developing Data APIs </li><li>Experience with complex, high volume, multi-dimensional data based on unstructured, structured, and streaming datasets </li></ul><br /><p><strong>Other Desirable competencies</strong> </p><ul><li>Excellent interpersonal skills and the ability to successfully interact with stakeholders from senior management to junior staff </li><li>Proactively solves problems and seeks to own the resolution of issues throughout the lifecycle </li><li>Takes critical feedback in stride, and adjusts approaches accordingly </li><li>Digitally savvy, able to navigate easily in a virtual world </li><li>Team player and not afraid to challenge the status quo </li><li>Good Logical Thinking and Problem-Solving Skills </li></ul><br /><p><strong>Note: Preferred immediate joiners or 15-30 days notice</strong> </p><br />",Senior Data Engineer || Gurgaon Location,1500000,2500000,https://www.naukri.com/job-listings-senior-data-engineer-gurgaon-location-neoris-consulting-services-india-pvt-ltd-gurgaon-gurugram-5-to-7-years-291122914722,"['SCALA', 'Snowflake', 'Kafka', 'Big Data', 'Spark', 'Bash', 'SQL']",['Gurgaon'],Data Engineer,2022-11-29 19:07:48
261122907438,"<p>We are looking for a savvy Data Engineer to join our growing team of analytics experts. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams.</p><p></p><p><br /></p><p></p><p>The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company's data architecture to support our next generation of products and data initiatives.<br /><br />Mandatory Skills : Pyspark Python, Spark scala, spark with SQL<br /><br />Experience Range : 5 - 10 years<br /><br />Experience Preferred : 8+ years<br /><br />Job Roles and Responsibilities :<br /><br />- Experience working with varied forms of data infrastructure inclusive of relational databases such as SQL, Hadoop, Spark.<br /><br />- Proficiency in scripting languages in python, pyspark, and expertise in SQL.<br /><br />- Implement small and large scale implementations of Data Engineering projects with E2E delivery.<br /><br />- Drive technical discussions and design solutions around it.<br /><br />- Technically coach the junior team members and provide real time solutions to complex problems.<br /><br />- Identify, design, and implement internal process improvements : automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.<br /><br />- Perform Coding and unit testing independently and assistmentor team members whenever required.<br /><br />- Adherence to coding standards.<br /><br />- Works closely with the business's Data, Analytics and Machine Learning teams and gathering technical requirements.<br /><br />- Should be proactive in identifying the issues and help plan to resolve them in a timely and efficient manner.<br /><br />- Maintenance of the existing Data systems, and giving a critical view on the Data pipelines, and suggest improvements with respect to data quality, pipeline optimizations.<br /><br />- Experience in building and maintaining reliable and scalable ETL pipeline on Big Data Cloud platform through the collection, storage, processing, and transformation of large data-sets.<br /><br />- Experience in Database design data modelling.<br /><br />- Must have strong experience in data warehouse concepts.<br /><br />- Experience in Restful API development and ability to serve analytical results to the end customers in form of reports, visualizations etc.<br /><br />- Experience in AWS cloud.<br /><br />- Experience in Data bricks (not mandatory, but preferrable).</p>",Senior Data Engineer - Data Wrangling/Pipeline Building,1100000,1900000,https://www.naukri.com/job-listings-senior-data-engineer-data-wrangling-pipeline-building-fission-labs-hyderabad-secunderabad-5-to-10-years-261122907438,"['Data Quality', 'Data Pipeline', 'PySpark', 'Scala', 'Hadoop', 'Spark', 'Data Modeling', 'Analytics', 'Python', 'SQL']",['Hyderabad'],Data Engineer,2022-11-26 21:56:20
061222910566,"<p>Proven experience in a Data Engineering role with at least 1-5 years of experience</p><ul><li> High sense of ownership and strong decision-making skills backed by first principles</li></ul><p>thinking</p><ul><li> Strong analytical skills to break down complex problems and build the right solutions</li><li> Good knowledge of data engineering concepts such as ETL pipelines, building a data</li></ul><p>lake, building data warehouses/ data marts, Stream and event processing, distributed</p><p>processing of large scale data.</p><ul><li> Experience building and optimising big data' data pipelines, architectures and data sets.</li><li> In-depth knowledge in data warehousing concepts and familiarity with at least one of the</li></ul><p>warehousing tools such as Amazon Redshift, Snowflake and more.</p><ul><li> Strong SQL knowledge and understanding of performance tuning techniques</li><li> Good knowledge of big data tools such as Hadoop, Apache Spark, Apache Druid, S3,</li></ul><p>Glue, Athena, Flink, Airflow, etc.</p><ul><li> Good exposure to streaming technologies like Kafka, SQS, Kinesis</li><li> Good understanding of SQL and NoSQL databases</li><li> Experience with Amazon web services or Google Cloud Platform.</li><li> Proficient in Java, Scala or Python.</li><li> Experience in machine learning workloads is a plus</li><li> Experience in supporting and working with product managers, decision scientists</li></ul><p>and data analysts in a dynamic environment</p><p>We've Got</p><br />",Data Engineer,1000000,2000000,https://www.naukri.com/job-listings-data-engineer-unicornhead-online-ventures-india-pvt-ltd-bangalore-bengaluru-3-to-7-years-061222910566,"['python', 'spark', 'scala', 'airflow', 'kafka', 'Big Data', 'hadoop', 'Data Warehousing', 'sql']",['Bengaluru'],Data Engineer,2022-12-06 17:42:13
061222011294,"<p>Immediate opening for data engineer.</p><br /><p>W0rk mode-Remote</p><p>Experience-5+</p><p>Notice period-Upto 30 days</p><br /><br /><p>Job Description-</p><p> • Interest in new technologies and eager to bring those technologies and out of the box ideas  to the team</p><p> • 5+ years of development experience on web applications using Python, Ruby, Java, or C# • 5+ years of SQL experience. </p><p> • Intellectual curiosity and drive; self-starters will thrive in this position • Passion for Technology: Excitement for new technology, bleeding edge applications, and a  positive attitude towards solving real world challenges Additional Skills </p><p>• BS, MS or PhD in Computer Science, Engineering, or equivalent real-world experience (You've  learned something / somehow to be able to claim you are an engineer) </p><p>• Significant experience with Python, C++, or other popular language • Experience with big data and/or infrastructure.</p><p> Bonus for having experience in setting up  Petabytes of data so they can be easily accessed. Understanding of data organization, ie  partitioning, clustering, file sizes, file formats. Data cataloging with Hive/Hive metastore or  Glue or something similar.</p><p> • Experience working with classical relational databases. </p><p>• Experience with Hadoop, Hive, Spark, or other data processing tools (Lots of time will be  spent building and optimizing transformations)</p><p> • Experience building scalable data pipelines (Airflow experience a plus) • Significant experience working with AWS and/or GCP .</p><br /><br />",Permanent opening For Data Engineer.,1500000,3000000,https://www.naukri.com/job-listings-permanent-opening-for-data-engineer-experis-kolkata-hyderabad-secunderabad-pune-gurgaon-gurugram-chennai-bangalore-bengaluru-delhi-ncr-mumbai-all-areas-5-to-10-years-061222011294,"['hive', 'C++', 'java', 'spark', 'Data engineer', 'Big Data']","['Chennai', 'Pune', 'Delhi NCR', 'Bengaluru', 'Hyderabad', 'Gurgaon', 'Kolkata', 'Mumbai (All Areas)']",Data Engineer,2022-12-06 18:30:00
071222006155,"<p> • 4+ Experience working in Datawarehouse/ETL Projects<br />• Programming skills: SQL/ Python<br />• Sound knowledge in AWS Cloud Services (S3, Lambda, Step Functions, SQS etc.) & PostGres<br />• Deep understanding of data ingestion, transformation and data engineering<br />• ETL Performance Tuning<br />• Team Managing skills<br />• Good Oral and Written Communication </p><br /><br />",ETL Data Engineer,50000,200000,https://www.naukri.com/job-listings-etl-data-engineer-rekrut-india-chennai-bangalore-bengaluru-4-to-9-years-071222006155,[],"['Chennai', 'Bengaluru']",Data Engineer,2022-12-07 12:02:19
301122604334,"<p> </p><p>We are looking for<strong> Data Engineer</strong></p><p><strong>Payroll Company: - On Roll</strong></p><p><strong>Client: - MNC</strong></p><p>Experience: - 2  to 5 years</p><p>Immediate/ 15 Day / 1 month</p><p>Location: - Ahmedabad</p><br /><p><strong>Responsibilities: </strong></p><br /><ul><li> Connecting to Data sources, importing data, and transforming data for Required Processing/ Reporting/ Business Intelligence </li><li> Designing data schemas/stores and data pipelines for storing and processing of different kind of structured and unstructured data sets like transaction data, review data, video feeds, process data, email data, social feed data and many more </li><li> Develop powerful data stories and visualization in the form of Dashboards </li><li> Working closely with our backend engineering team to build a robust suite of libraries for extracting and manipulating data for our apps. </li><li> Create libraries for data quality assurance and data sanity checks </li></ul><br /><p><strong>Required Skills:</strong> </p><ul><li> Experience with SQL, other relational databases, and other types of databases like NoSQL </li><li> Fluent in Data warehousing concepts and ETL capabilities </li><li> Proficiency in ETL tools like Informatica, Talend, Alteryx and Visualization tools like Power BI, Tableau and Qlikview </li><li><strong> Proficiency in any of the cloud offerings - AWS, GCP or Azure </strong></li><li> Industry experience in Spark </li><li> Good command in Python or Scala or JavaScript preferred </li><li> Experience with implementation of RLS (Row level security) for access based on user roles. </li><li><strong> BE/B.Tech or post graduate degree in Computer Applications or IT backgrounds</strong> </li></ul><br /><p><strong>JD Matching candidates only apply.</strong></p><br /><p><strong>Required Details:-</strong></p><p><strong>1.</strong> <strong>Total Experience:-</strong></p><p><strong>2.</strong> <strong>Relevant Experience:-</strong></p><p><strong>3.</strong> <strong>Current CTC:-</strong></p><p><strong>4.</strong> <strong>If offer in hand CTC/location:-</strong></p><p><strong>5.</strong> <strong>Expected CTC:-</strong></p><p><strong>6.</strong> <strong>Notice Period (Pls</strong> <strong>mention officially on papers ; mention LWD if serving notice)</strong></p><p><strong>7.</strong> <strong>Current Location:-</strong></p><p><strong>8.</strong> <strong>Preferred Location:-</strong></p><p><strong>9.</strong> <strong>Qualification:-</strong></p><p><strong>10.</strong> <strong>Reason for leaving current organization:-</strong></p><br /><p><strong>If interested, please share your updated CV  on <u>anjana@mounttalent.com</u></strong></p><br /><p><strong>Thanks and Regards,</strong><br /><strong>Anjana Rathaur</strong></p><p><u>anjana@mounttalent.com</u></p><p>Website:www.mounttalent.com</p>",Urgent Requirement // Data Engineer //MNC Client//,450000,950000,https://www.naukri.com/job-listings-urgent-requirement-data-engineer-mnc-client-mount-talent-consulting-ahmedabad-2-to-5-years-301122604334,"['Azure', 'Power Bi', 'GCP', 'Alteryx', 'Talend', 'informatica']",['Ahmedabad'],Data Engineer,2022-11-30 10:01:07
301122004332,"<p> </p><p>We are looking for<strong> Data Engineer</strong></p><p><strong>Payroll Company: - On Roll</strong></p><p><strong>Client: - MNC</strong></p><p>Experience: - 2  to 5 years</p><p>Immediate/ 15 Day / 1 month</p><p>Location: - Ahmedabad</p><br /><p><strong>Responsibilities: </strong></p><br /><ul><li> Connecting to Data sources, importing data, and transforming data for Required Processing/ Reporting/ Business Intelligence </li><li> Designing data schemas/stores and data pipelines for storing and processing of different kind of structured and unstructured data sets like transaction data, review data, video feeds, process data, email data, social feed data and many more </li><li> Develop powerful data stories and visualization in the form of Dashboards </li><li> Working closely with our backend engineering team to build a robust suite of libraries for extracting and manipulating data for our apps. </li><li> Create libraries for data quality assurance and data sanity checks </li></ul><br /><p><strong>Required Skills:</strong> </p><ul><li> Experience with SQL, other relational databases, and other types of databases like NoSQL </li><li> Fluent in Data warehousing concepts and ETL capabilities </li><li> Proficiency in ETL tools like Informatica, Talend, Alteryx and Visualization tools like Power BI, Tableau and Qlikview </li><li><strong> Proficiency in any of the cloud offerings - AWS, GCP or Azure </strong></li><li> Industry experience in Spark </li><li> Good command in Python or Scala or JavaScript preferred </li><li> Experience with implementation of RLS (Row level security) for access based on user roles. </li><li><strong> BE/B.Tech or post graduate degree in Computer Applications or IT backgrounds</strong> </li></ul><br /><p><strong>JD Matching candidates only apply.</strong></p><br /><p><strong>Required Details:-</strong></p><p><strong>1.</strong> <strong>Total Experience:-</strong></p><p><strong>2.</strong> <strong>Relevant Experience:-</strong></p><p><strong>3.</strong> <strong>Current CTC:-</strong></p><p><strong>4.</strong> <strong>If offer in hand CTC/location:-</strong></p><p><strong>5.</strong> <strong>Expected CTC:-</strong></p><p><strong>6.</strong> <strong>Notice Period (Pls</strong> <strong>mention officially on papers ; mention LWD if serving notice)</strong></p><p><strong>7.</strong> <strong>Current Location:-</strong></p><p><strong>8.</strong> <strong>Preferred Location:-</strong></p><p><strong>9.</strong> <strong>Qualification:-</strong></p><p><strong>10.</strong> <strong>Reason for leaving current organization:-</strong></p><br /><p><strong>If interested, please share your updated CV  on <u>anjana@mounttalent.com</u></strong></p><br /><p><strong>Thanks and Regards,</strong><br /><strong>Anjana Rathaur</strong></p><p><u>anjana@mounttalent.com</u></p><p>Website:www.mounttalent.com</p>",Urgent Requirement // Data Engineer //MNC Client//,450000,950000,https://www.naukri.com/job-listings-urgent-requirement-data-engineer-mnc-client-mount-talent-consulting-ahmedabad-2-to-5-years-301122004332,"['Azure', 'Power Bi', 'GCP', 'Alteryx', 'Talend', 'informatica']",['Ahmedabad'],Data Engineer,2022-11-30 10:01:07
200721001540,"<p><strong>Unify Technologies Pvt Ltd</strong></p><p><strong>Company Nature of work: </strong>IT-Software Product Development and Product Engineering Services</p><p><strong>Founded Year: </strong>2015</p><p><strong>Company: Locations: </strong>Hyderabad, Bangalore, Pune, Chandigarh, Gurgaon - India, Seatle - USA</p><p><strong>Num of Total Employees: </strong>1,500+</p><p><strong>Company Website: </strong>http://unifytech.com/</p><p><strong>Company LinkedIn Website Address: </strong>https://www.linkedin.com/company/9206998</p><br /><p><strong>A few words about Unify Technologies:</strong> Unify is a Digital Product Development and Product Engineering services company. We have extensive experience in software product engineering and a successful track record of delivering aggressive delivery plans without compromising on the quality of Data, Cloud, Mobile, Cyber Security, E-Learning, E-commerce, and Healthcare platforms.</p><br /><p>If you are looking for a challenging opportunity to put your Computer Science skills in the right place. We're looking for Big Data engineers who have previously worked on delivering scalable backend distributed systems, big data platforms, and data pipeline and streaming systems.</p><br /><p><strong>Employment Type:</strong> Full-Time</p><p><strong>Role: </strong>Software Development Engineer</p><p><strong>Position:</strong> Developer, Senior Developer, and Lead Developer</p><p><strong>Project:</strong> Maps and Advertising Platforms product</p><p><strong>Experience:</strong> <strong>Sr SDE: 4-8 Years; Lead SDE: 8-10 Years;</strong></p><p><strong>Key Skills: Spark Core/Streaming/API with Any programming</strong> <strong>language Scala/ Java/ Python, Good Understanding of Data Transformation, Data Ingestion, Optimization mechanism, Good understanding of Big Data (Hadoop, MapReduce, Kafka, Cassandra) Technologies</strong></p><p><strong>Joining time:</strong> Immediate to 30 days</p><p><strong>Job Location:</strong> Hyderabad, Telangana - India (Hybrid Mode-3 Days WFO, 2 Days WFO)</p><p><strong>Education:</strong> Masters/bachelor's degree in Computer Science, Statistics, Engineering, or a related technical discipline will be preferred</p><br /><p><strong>Detailed Job Description:</strong></p><p><strong>Key Qualifications</strong>:</p><p>Minimum 4+ years of working experience in Big Data platforms and building large-scale distributed systems with high availability.</p><ul><li>Experience in developing Spark Applications using <strong>Spark RDD API, Spark-SQL, Spark GraphX API, Spark Streaming API, Spark-Yarn, Spark MLib API, and Data frame APIs</strong></li><li>Should have a broad knowledge of Spark Advantages, Spark Workflows, How to write Spark Jobs, Spark query tuning, and performance optimization.</li><li>Solid in <strong>Data Structures, Algorithms basics and Should have good hands-on experience with any - one programming language</strong> (Scala/ Java8 - 1st Preference OR Python - 2nd Preference OR Java - 3rd preference) and Strong investigative and problem-solving skills</li><li><strong>Data Ingestion, Optimization Techniques, Data Transformation,</strong> and aggregation pipeline design/development knowledge is required.</li><li>Experience working on cutting-edge Big Data storage systems and technologies like Hadoop, HDFS, AWS S3, AWS Lambda, Storm/Heron, Cassandra, Apache Kafka, Solr/ElasticSearch, MongoDB, DynamoDB, Postgres, and/or MySQL, etc.</li></ul><br /><p><strong>Roles & Responsibilities:</strong></p><ul><li>Create new, and maintain existing Scala/Spark jobs for data transformation and aggregation from simple to Complex Data transformations involving structured & unstructured data.</li><li>Produce unit tests for Spark transformations and helper methods</li><li>Develop data processing pipelines, data storage, and management architecture.</li><li>Define scalable calculation logic for interactive and batch use cases</li><li>Interact with infrastructure and data teams to produce complex analyses across data</li><li>You'll be working on a unique and challenging big data ecosystem with a focus on storage efficiency, data security, privacy, scalable and performant queries, expandability and flexibility, etc, with the goal to help better measure the quality of map data.</li><li>You will work with engineers to build a big data platform that processes and manages Exabytes of data and enables efficient access to those data.</li></ul><br /><p><strong>Interview Process:</strong></p><p>Interview Rounds: 3 to 4</p><p>Nature of Interview: Technical, Programming, Coding Interview</p><p>Mode of Interview: Google Meet/Webex Video call (Must enable the video in the interview)</p><br /><p>Contact Person: pavanm@unifytech.com</p><br /><p>NOTE: We are also looking for Scala Functional programming, Java back-end, Java Full-Stack, MEAN/MERN Stack Developers, SDET, and SRE/DevOps, Data Cloud Engineers.</p>",Spark+Scala Data Engineer,1500000,2500000,https://www.naukri.com/job-listings-spark-scala-data-engineer-unify-technologies-pvt-ltd-hyderabad-secunderabad-bangalore-bengaluru-4-to-8-years-200721001540,"['Algorithms', 'Data Structures', 'Hadoop', 'Cassandra', 'Mapreduce']","['Bengaluru', 'Hyderabad']",Data Engineer,2022-12-06 00:30:04
011222004883,"<p><strong>Roles and Responsibilities</strong> </p><br /><p> </p><ul><li> Strong hands on working experience of Big Data stack including PySpark and Core Java  </li><li> Good understanding on RDMS database and Linux/UNIX . </li></ul><p> Strong knowledge of multi-threading and high volume batch processing.  </p><ul><li> Able to design Big data batch and real time data services.  </li><li> Should be good in performance tuning skills in PySpark. </li><li> Should have experience on AWS platform.  </li><li> Hands experience on Agile and CI-CD process.  </li><li></li></ul><p>Preferred Skills  </p><ul><li> Experience on reporting service implemented on Big database.  </li><li> Experience on Autosys or Control-M scheduler.  </li><li> Prefer experience on Kafka , Kinesis, and Streaming real time data.  </li><li> Experience on Python data libraries.  </li></ul><br /><p><strong>Desired Candidate Profile</strong> </p><br /><br /><p><strong>Perks and Benefits</strong> </p><br /><br />",Data Engineer,1000000,2000000,https://www.naukri.com/job-listings-data-engineer-capleo-global-bangalore-bengaluru-6-to-10-years-011222004883,"['streaming', 'kafka', 'linux', 'multithreading', 'aws', 'unix']",['Bengaluru'],Data Engineer,2022-12-01 11:08:39
171122909605,"<b>The Role</b><br /><div><div><p><b><span>Key Characteristics:</span></b><span></span> </p><ul><li>Technology champion who constantly pursues knowledge enhancement and has inherent curiosity to understand work from multiple dimensions.</li><li>Interest and passion in Big Data technologies and appreciates the value that can be brought in with an effective data management solution.</li><li>Has worked on real data challenges and handledhighvolume, velocity and variety of data.</li><li>Deep data focus with expertise in one of more technologies in the data domain (Azure Databricks,Alteryx,PySpark,ADF ,Kafka, SAP Hanaetc).Proficient with data warehousing and traditional ETL technologies. </li><li>Understands Infrastructure concepts and has worked with multiple cloud tenants. </li><li>Knowledge on latest database technologies like MongoDB, Cosmos, Cassandra etc.</li><li>Knowledge on data analytics and data Visualization tools like PowerBI ,Spotfire etc.</li><li>Excellent analytical & problem-solving skills, willingness to take ownership and resolve technical challenges.</li><li>Excellent communication and stakeholder management skills.</li></ul></div><div><p><span></span></p></div><div><p><span><span>Other<span></span></span><span>Dimensions :</span></span><span></span></p></div><div><ul><li>Minimum2 -4 years of experience in the IT industry</li><li>University degree in any IT discipline</li></ul></div></div>",Data Engineer,300000,600000,https://www.naukri.com/job-listings-data-engineer-shell-india-markets-private-limited-bangalore-bengaluru-2-to-4-years-171122909605,"['ADF', 'PySpark', 'Alteryx', 'Data Engineer', 'Kafka', 'Azure Databricks', 'SAP Hanaetc', 'Big Data technologies']",['Bengaluru'],Data Engineer,2022-12-01 12:02:07
021222909037,"<p><strong>Summary</strong></p><p>Do you have a passion around processing data to enable insights for organizations? We are looking for a savvy Data Engineer to join our growing team of analytics experts. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our companys data architecture to support our next generation of products and data initiatives. Experience with “Snowflake Cloud Data warehouse” and Microsoft Azure services is preferred but AWS or open source experience is acceptable as well.</p><ul><li>Create and maintain optimal data pipeline architecture</li><li>Assemble large, complex data sets that meet functional / non-functional business requirements.</li><li>Author data services using a variety of programming languages</li><li>Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.</li><li>Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using Snowflake Cloud Data warehouse as well as SQL and Azure ‘big data’ technologies</li><li>Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.</li><li>Keep our data separated and secure across national boundaries through multiple data centers and Azure regions.</li><li>Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.</li><li>Work with data and analytics experts to strive for greater functionality in our data systems.</li><li>Work in an Agile environment with Scrum teams.</li><li>Ensure data quality and help in achieving data governance.</li></ul><p><strong>Basic Qualifications</strong></p><ul><li>2+ years of experience in a Data Engineer or Software Engineer role</li><li>Undergraduate degree required (Graduate degree preferred) in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. </li><li>Experience using the following software/tools:</li><li>Experience with “Snowflake Cloud Data warehouse”</li><li>Experience with Azure cloud services: ADLS, ADF, ADLA, AAS</li><li>Experience with data pipeline and workflow management tools</li><li>Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases</li><li>Understanding of Data warehouse (DWH) systems, and migration from DWH to data lakes/Snowflake</li><li>Understanding of ELT and ETL patterns and when to use each. Understanding of data models and transforming data into the models</li><li>Strong analytic skills related to working with unstructured datasets</li><li>Build processes supporting data transformation, data structures, metadata, dependency and workload management</li><li>Experience supporting and working with cross-functional teams in a dynamic environment</li></ul>",Snowflake Developer - Data Engineer,400000,800000,https://www.naukri.com/job-listings-snowflake-developer-data-engineer-tech-smcsquared-gcc-india-bangalore-bengaluru-2-to-6-years-021222909037,"['ADF', 'ADLA', 'data processing', 'relational databases', 'SQL', 'sql azure', 'Snowflake Cloud Datawarehouse', 'AAS', 'ADLS', 'data structures', 'software engineering', 'workflow management', 'Data Modeling', 'aws', 'ETL']",['Bengaluru'],Data Engineer,2022-12-02 17:21:56
301122006812,"<p><strong>Roles and Responsibilities</strong> </p><p> </p><ul><li>B. Tech/ Masters degree or equivalent in Computer Science/Computer Engineering/IT, Technology, or related field.</li><li>Experience must include work with SQL, SQL Server, Python and Excel</li><li>Experience must also include direct client interactions and competency to understand business requirements for data analysis</li><li>Experience related to developing internal tools for data analysis, visualizations and its implementation is good to have</li><li>Travel to end-client sites may be required.</li></ul><p><strong>Desired Candidate Profile</strong> </p><p> </p><ul><li>Excellent verbal and written communication skills </li><li>4-6 years of rock-solid programming skills in SQL, SQL Server, Python and understanding of Excel</li><li>Like working with a team and a solo cruiser when needed </li><li>Driven, and can quickly complete large volumes of work with high quality </li><li>Designing the Data Pipeline and Data Flow architecture.</li><li>Implementing Custom Delivery Framework and coordinating to build Conceptual Minimum Viable Product (MVP) solutions proposed to clients in pre-sales.</li></ul><br /><br /><br />",Data Engineer,800000,1500000,https://www.naukri.com/job-listings-data-engineer-gramener-hyderabad-secunderabad-bangalore-bengaluru-4-to-7-years-301122006812,"['excel', 'python', 'data analysis']","['Bengaluru', 'Hyderabad']",Data Engineer,2022-11-30 12:16:06
281122905434,"<p>Requirment :-<br />Minimum 3 years Pharmaceutical Manufacturing experience. Knowledge of GxP, Pharmaceutical manufacturing processes and automations systems<br /><span></span>Minimum 3 years of relevant experience with Data Analytics and Visualization design<br /><span></span>Design right interfaces for Data Visualization<br /><span></span>Extensive knowledge on PowerBI, Tableau or SAP BO, with hands on experience in developing visualizations primarily for Manufacturing domain.<br /><span></span>Strong experience with SQL, Visualization technology and good experience with Cloud Architecture<br /><span></span>Must be a good communicator, efficiently collaborate with Vendors and deliver support and new enhancements<br /><span></span>Strong experience with AWS ETL/File Movement tools ( GLUE, Athena, Lambda, Kenesis and other AWS integration stack)<br /><span></span>Strong experience with Agile Development, Cloud formation Template, AWS CodeBuilt, AWS Code Pipeline<br /><span></span>Strong experience with Two or Three AWS database technologies ( Redshift, Aurora, RDS,S3 & other AWS Data Service ) covering security, policies, access management<br /><span></span>Strong programming Experience with Python and Spark<br /><span></span>Experience with security models and development on large data sets<br /><span></span>Proficiency in designing and building architecture for data visualization tools like Power BI and/or Tableau on AWS/Azure<br /><span></span>Experience in deploying and managing Visualization as a service on cloud platforms. Understanding of DAX Query model and Tableau Data Models<br /><span></span>Strong Experience in Creating Data Models for Visualization tool and Extracting data from AWS cloud storage, AWS data warehouse, AWS database for PostgreSQL, Data Lake and Streaming Data Store and<br />other Data warehouse solutions, API’s<br /><span></span>Experience with custom visuals in Python/R for PowerBI Visualizations and Data Analytics</p>",Data Engineer ( Clinical Design),1000000,2000000,https://www.naukri.com/job-listings-data-engineer-clinical-design-edge-bangalore-bengaluru-4-to-9-years-281122905434,"['S3', 'Storm', 'Java', 'Data Modelling', 'Power BI', 'data warehousing', 'C++ Scala', 'Redshift', 'SQL', 'R', 'NoSQL', 'Glue', 'SSAS', 'Data Engineer', 'ETL', 'Spark-Streaming', 'AWS', 'Lambda', 'Python']",['Bengaluru'],Data Engineer,2022-11-28 16:55:31
261122905351,"<p></p><p>Roles and Responsibilities :<br /><br />1. Design and implement data engineering projects. <br /><br />2. Integrate multiple data sources to create data lakedata mart Perform data ingestion and ETL processes using SQL, Scoop, Spark or Hive<br /><br />3. Knowledge of new components and various emerging technologies in on-premises and Cloud (AWSAzureGoogle)<br /><br />4. Collaborate with various cross-functional teams: infrastructure, network and database<br /><br />5. Work with various teams to setup and manage users, secure and govern platforms and data and maintain business continuity through contingency plans (data archiving etc.)<br /><br />6. Monitor job performances, manage file systemdisk-space, cluster & database connectivity, log files, manage backupsecurity and troubleshoot various user issues<br /><br />7. Design, implement, test and document performance bench-marking strategy for platforms as well as for different use cases<br /><br />8. Setup, administer, monitor, tune, optimize and govern large scale implementations<br /><br />9. Implement machine learning models on real time input data stream<br /><br />10. Drive customer communication during critical events and participatelead various operational improvement initiatives<br /><br />Desired candidate :<br /><br />1. 3 - 5 years relevant experience in data engineering<br /><br />2. Exposure to any or all latest data engineering ecosystem platforms such as AWS, Azure, GCP, Cloudera and Data bricks<br /><br />3. Sound knowledge of PythonScalaJava<br /><br />4. Good knowledge of SQL NoSQL databases and data warehouse concepts<br /><br />5. Hands on experience of working on databases such as Sql Servers, PostgreSql, Cloud infrastructure, etc.<br /><br />6. Excellent knowledge of data backup, recovery, security and integrity<br /><br />7. Sound knowledge on Spark, HDFSHIVEHBASE, Shell Scripting, and Spark Streaming<br /><br />8. Excellent communication skills<br /><br />9. Must be proficient with data ingestion tools like Sqoop, flume, talend, and Kafka</p><p></p><p></p>",Data Engineer - Python,800000,1400000,https://www.naukri.com/job-listings-data-engineer-python-transorg-analytics-mumbai-delhi-ncr-bangalore-bengaluru-3-to-5-years-261122905351,"['Azure', 'NoSQL databases', 'Shell Scripting', 'PostgreSql', 'Cloudera', 'Data bricks', 'Spark Streaming', 'data engineering', 'Sql Servers', 'SQL', 'Cloud infrastructure', 'GCP', 'AWS']","['Delhi NCR', 'Mumbai', 'Bengaluru']",Data Engineer,2022-11-26 14:10:49
021222502805,"<p> </p> <p> </p> <p> </p> <ul> <li> <span> <span> <span> </span> </span> </span> <span> Experiencing with data manipulation language including optimization techniques and understanding of normalized/dimensional data modelling principles. </span> </li> <li> <span> <span> <span> </span> </span> </span> <span> Collaborating with development teams to understand product requirements and translate them in to innovative software designs </span> </li> <li> <span> <span> <span> </span> </span> </span> <span> Implementing development processes, coding best practices, and code reviews, keeping up to date with new technology </span> </li> <li> <span> <span> <span> </span> </span> </span> <span> Operating in an Agile environment to resolve technical issues and complete bug fixes </span> </li> <li> <span> <span> <span> </span> </span> </span> <span> Collaborating with teams and stakeholders to seek feedback on priorities to help shape innovative solutions for complicated software challenges </span> </li> </ul> <p> </p> <p> <strong> <span> Requirements </span> </strong> </p> <p> </p> <ul> <li> <span> <span> <span> </span> </span> </span> <span> Demonstrate good experience as an Software Engineer and knowledge of software development methodologies (e.g., Agile, Waterfall </span> </li> <li> <span> <span> <span> </span> </span> </span> <span> Have proficiency in Bigdata development languages such as Spark Scala Or Python and building data pipelines and handling data (ETL) </span> </li> <li> <span> <span> <span> </span> </span> </span> <span> Be familiar with industry best practices including code coverage </span> </li> <li> <span> <span> <span> </span> </span> </span> <span> Experience in distributed computing frameworks such as Apache Spark, Apache Flink or similar </span> </li> <li> <span> <span> <span> </span> </span> </span> <span> Have great verbal and written communication skills and enjoy collaborating with teams to solve problems </span> </li> <li> <span> <span> <span> </span> </span> </span> <span> Have the ability to write and review detailed specifications for the development of complex system components </span> </li> <li> <span> <span> <span> </span> </span> </span> <span> Have experience of test-driven development and the ability to complete complex bug fixes </span> </li> </ul>",Principal Data Engineer,700000,1200000,https://www.naukri.com/job-listings-principal-data-engineer-reed-elsevier-india-a-part-of-relx-india-pvt-ltd-bangalore-bengaluru-5-to-10-years-021222502805,"['Health insurance', 'Software development methodologies', 'Publishing', 'Coding', 'SCALA', 'Agile', 'Healthcare', 'test driven development', 'Analytics', 'Python']",['Bengaluru'],Data Engineer,2022-12-02 20:47:09
301122910246,"<ul><li>Demonstrated expertise in building enterprise-level data warehouse applications using Teradata would be mandatory.</li><li>Strong skills and background in ETL processes (BTEQ) is essential.</li><li>Experience in SQL coding and tuning (preferably in Teradata), exposure to BI, maintenance and execution plans would be essential.</li><li>A proven ability to effectively communicate, educate and influence the business stakeholders, while working in collaboration with other cross-functional engineering teams will be highly essential.</li></ul><p><i><span></span></i></p>",Data Engineer (Teradata),400000,800000,https://www.naukri.com/job-listings-data-engineer-teradata-commonwealth-bank-of-australia-bangalore-bengaluru-2-to-6-years-301122910246,"['ETL processes', 'BI', 'Data warehousing', 'maintenance', 'SQL coding']",['Bengaluru'],Data Engineer,2022-11-30 17:04:28
281122906864,"<p></p><p>Data Engineer - Google Cloud Certified<br /><br />Work Location : Hyderabad<br /><br />1 -3 yrs exp - 6 openings<br /><br />3 - 8 yrs exp - 2 openings<br /><br />Full time with Techstar<br /><br />JD : </p><p></p><p><b>Certification : Professional Google Cloud Certified Data Engineer Mandatory</b></p><p>Strong analytical skills.<br /><br />Skills : Google, Google cloud, Googlecloud, Google-cloud, Google - cloud, Google adwords, Google Analytics, Google Api, Google app engine, Google Engine, Google apps, Cloud, Public CLoud, Cloud design, data, Data engineer, Data store </p><p>- Experience building and supporting large-scale, business critical systems on Public Cloud (Google Cloud Platform) Technical / Functional Skills :<br /><br />Working knowledge of Google Cloud Platform : Good understanding of cloud design considerations and limitations in the areas of virtualization and global infrastructure, distributed systems, load balancing and networking, massive data storage and processing.<br /><br />Roles & Responsibilities :<br /><br />- Design & Build Enterprise Datawarehouse & Datamarts, deploy in cloud (GCP/AWS/Azure)<br /><br />- Liaise with client business teams for requirements gathering, scoping and finalizing tech stack & deliverables<br /><br />- Perform Descriptive Analytics & reporting<br /><br />- Perform peer code reviews, design documents & test cases<br /><br />- Support systems currently live and deployed for customers<br /><br />- Build Knowledge repository & Cloud capabilities.<br />Hyderabad - Chennai - Bangalore India</p>",Data Engineer - Google Cloud / Data Warehousing,900000,1300000,https://www.naukri.com/job-listings-data-engineer-google-cloud-data-warehousing-techstar-software-development-india-pvt-ltd-hyderabad-secunderabad-chennai-bangalore-bengaluru-1-to-3-years-281122906864,"['Azure', 'Google Analytics', 'Google Engine', 'Descriptive Analytics', 'test cases', 'code reviews', 'Google cloud', 'Google Api', 'Google apps', 'GCP', 'Cloud', 'cloud design', 'AWS', 'Google app engine', 'Google adwords']","['Chennai', 'Bengaluru', 'Hyderabad']",Data Engineer,2022-11-28 17:47:56
281122504352,<div> <ul> <li> <div> <span> Allstate Road Services (ARS) is seeking a Senior Consultant with strong experience in creating data marts using Ab-Initio  </span></div> </li> <li> The role requires strong technical skills specific to data mart/ data warehouse and should have good knowledge and experience on software development lifecycle  </li> <li> Creating and updating technical documentations which are integral to Allstate business is also part of the responsibility  </li> <li> The role would require to work closely with folks across shores and would require to perform development into getting data moved from various transactional systems into the department datamart  </li> <li> Job Responsibilities Around 5 Years of experience on complex transformation using Abinitio   </li> </ul> </div>  <ul> </ul> <div> </div>,Data Engineer,800000,1300000,https://www.naukri.com/job-listings-data-engineer-triangle-global-pune-bangalore-bengaluru-5-to-8-years-281122504352,"['Software development life cycle', 'Ab Initio', 'Data warehousing']","['Pune', 'Bengaluru']",Data Engineer,2022-11-28 18:07:01
281122000489,"<ul> <br> <li><span><span><span><span><span><span>2 to 5 years of Experience in handling large data sets and useing Python, PySpark, SQL</span></span></span></span></span></span></li> <br> <li><span><span><span><span><span><span>Experience in using AWS SDKs for creating data pipelines ingestion, processing, and orchestration.</span></span></span></span></span></span></li> <br> <li><span><span><span><span><span><span>Hands on experience in working with PySpark on AWS environment including cleaning/ transforming/cataloguing/mapping etc.</span></span></span></span></span></span></li> <br> <li><span><span><span><span><span><span>Good understanding of AWS components, storage (S3) & compute services (EC2), AWS Event Bridge, Kinesis, AppFlow</span></span></span></span></span></span></li> <br> <li><span><span><span><span><span><span>Hands on experience in AWS managed services (Redshift, Lambda, Athena) and ETL (Glue).</span></span></span></span></span></span></li> <br> <li><span><span><span><span><span><span>Experience in migrating data from on-premise sources (e.g. Oracle, API-based, data extracts) into AWS storage (S3)</span></span></span></span></span></span></li> <br> <li><span><span><span><span><span><span>Experience in setup of data warehouse using Amazon Redshift, creating Redshift clusters and perform data analysis queries</span></span></span></span></span></span></li> <br> <li><span><span><span><span><span><span>Experience in ETL and data modelling on AWS ecosystem components - AWS Glue, Redshift, DynamoDB</span></span></span></span></span></span></li> <br> <li><span><span><span><span><span><span>Experience in Git and CI/CD implementation in data engineering.</span></span></span></span></span></span></li> <br> <li><span><span><span><span><span><span>Experience in setting up AWS Glue to prepare data for analysis through automated ETL processes and lake formation</span></span></span></span></span></span></li> <br> <li><span><span><span><span><span><span>Familiarity with AWS data migration tools such as AWS DMS, Amazon EMR, and AWS Data Pipeline</span></span></span></span></span></span></li> <br> <li><span><span><span><span><span><span>Hands on experience with AWS CLI, Linux tools and shell scripts</span></span></span></span></span></span></li> <br> <li><span><span><span><span><span><span>High-level analytical and problem-solving skills. </span></span></span></span></span></span></li> <br> <li><span><span><span><span><span><span>Very Good communication skills and Client interaction</span></span></span></span></span></span></li> <br> <li><span><span><span><span><span><span>Certifications on AWS will be an added plus</span></span></span></span></span></span></li> <br></ul> <br><b> Roles and Responsibilities</b> <br><p><span><span><span><span><span>Designing and coding ETL applications on AWS platform to analyze data collections</span></span></span></span></span> Creating data processing frameworks using Spark and Python.  <span><span><span><span><span><span><span><span><span><span>Apply transformations using Scala & Python.</span></span></span></span></span></span> <span><span><span><span><span><span>Apply different HDFS formats and structure like Parquet, Avro, etc. to speed up analytics. </span></span></span></span></span></span> <span><span><span><span><span><span>Designing, building, configuring and supporting Hadoop. </span></span></span></span></span></span> <span><span><span><span><span><span>Translate complex functional and technical requirements into detailed design. </span></span></span></span></span></span> <span><span><span><span><span><span>Perform analysis of vast data stores and uncover insights. </span></span></span></span></span></span> <span><span><span><span><span><span>High-speed querying. </span></span></span></span></span></span> <span><span><span><span><span><span>Extracting data and isolating data clusters. </span></span></span></span></span></span> <span><span><span><span><span><span>Able to use Hive Spark SQL language to analyze the data. </span></span></span></span></span></span> <span><span><span><span><span><span>Testing scripts and analyzing results. </span></span></span></span></span></span> <span><span><span><span><span><span>Write optimised SQL queries to analyze the data. </span></span></span></span></span></span> <span><span><span><span><span><span>Use various Unix, Linux commands to do the manipulations & write Shell scripts. </span></span></span></span></span></span> <span><span><span><span><span><span>Troubleshooting application bugs. </span></span></span></span></span></span> <span><span><span><span><span><span>Maintaining the security of company data. </span></span></span></span></span></span> <span><span><span><span><span><span>Creating data tracking programs. </span></span></span></span></span></span> <span><span><span><span><span><span>Producing development documentation</span></span></span></span></span></span></span></span></span></span></p>",AWS Data Engineers,1000000,1600000,https://www.naukri.com/job-listings-aws-data-engineers-torry-harris-business-solutions-pvt-ltd-bangalore-bengaluru-3-to-6-years-281122000489,"['PySpark', 'Bigdata', 'Spark', 'AWS']",['Bengaluru'],Data Engineer,2022-11-28 09:05:47
011222908004,"Experience designing technological solutions to complex data problems, developing & testing.<br />Should have good knowledge -C/C++ /Python<br />Strong in computing frameworks,Like Apache Hadoop 2.0 (YARN; MR & HDFS) and associated technologies -- one or more of Hive, Sqoop, Avro, Flume, Oozie.<br />Hands-on experience with Apache Spark and its components (Streaming, SQL, MLLib) is a strong advantage.<br />Operating knowledge of cloud computing platforms (AWS, especially EMR, EC2, S3, SWF services and the AWS CLI)",Data Engineer,500000,1500000,https://www.naukri.com/job-listings-data-engineer-recbots-pune-chennai-bangalore-bengaluru-3-to-8-years-011222908004,"['Hive', 'Sqoop', 'Hadoop', 'Flume', 'Oozie', 'Avro']","['Chennai', 'Pune', 'Bengaluru']",Data Engineer,2022-12-01 15:29:22
011222011519,"<p>Hi,</p><br /><p>We have requirements for Data Architect / Data Engineer positions with one of our clients.</p><br /><p>Contract - 1 year and extendable</p><p>Work Mode : Hybrid (2 days work from office)</p><p>Location : Hyderabad / Bangalore / Pune</p><br /><p><strong>Position  : Azure Data Engineer </strong></p><p><strong>Experience : 4-15years</strong></p><p><strong><u>Job Description</u></strong></p><p>The candidate must have experience implementing or working with:</p><ul><li>Data Pipelines via Data Factory</li><li>Datasets within Synapse (or Azure SQL Database)</li><li>Loading and managing data with Azure Data Lake Storage</li><li>Performing data quality analysis</li><li>Architecting Data Platform on Azure</li><li>Designing dataset schema based on gathered requirements, with a strong understanding of facts, dimensions, and their relationships</li><li>Experience analysing, describing, elucidating, and documenting how to migrate data assets from one environment/platform to another</li><li>Identifying PII, PHI, or other sensitive data within a platform</li><li></li></ul><p>If interested, Please share your profiles to Sheetal.Kulkarni@maxotechsolutions.com Or call 7702888820</p><br /><p>Regards,</p><p>Sheetal</p>",Azure Data Engineer,700000,1600000,https://www.naukri.com/job-listings-azure-data-engineer-maxoind-tech-solutions-hyderabad-secunderabad-pune-bangalore-bengaluru-4-to-9-years-011222011519,[],"['Pune', 'Bengaluru', 'Hyderabad']",Data Engineer,2022-12-01 19:13:47
081222501706,"<div> <p> <span> <span> <span> <span> <span> <span> <span> <span> <span> <span> <span> <span> <span> Operation Support of Couchbase and/or Cassandra and/or MongoDB databases ensuring high availability through proactive monitoring, health checks, and self-healing. Resolving incidents under Incident Management, Root Cause Analysis through problem management, automate routine tasks, improve monitoring alerting and perform changes under Change Management. The individual will be required to collaborate with numerous partners including Amex application teams, infrastructure support teams and various product and support vendors. </span> </span> </span> </span> </span> </span> </span> </span> </span> </span> </span> </span> </span> </p> <p> <span> <span> <span> <span> <span> <span> <span> <span> <span> <span> <span> <span> <b> <span> Organizational Context: </span> </b> </span> </span> </span> </span> </span> </span> </span> </span> </span> </span> </span> </span> </p> <p> <span> <span> <span> <span> <span> <span> <span> <span> <span> <span> <span> <span> <span> Member of a data base management or database support team reporting to a Senior Engineer, Engineering Director, or Director of Technical Delivery. </span> </span> </span> </span> </span> </span> </span> </span> </span> </span> </span> </span> </span> </p> <p> <span> <span> <span> <span> <span> <span> <span> <span> <span> <span> <span> <span> <span> How will you make an impact in this role</span> </span> </span> </span> </span> </span> </span> </span> </span> </span> </span> </span> </span> </p> <p> <span> <span> <span> <span> <span> <span> <span> <span> <span> <span> <span> <span> <span> We are looking for a highly motivated Data Engineer (DBA) with good understanding and experience in maintaining the health and availability of our production Couchbase, Cassandra and MongoDB databases. In this position, you will be joining our global technology’s Database Operations team in American Express, Bangalore, India. </span> </span> </span> </span> </span> </span> </span> </span> </span> </span> </span> </span> </span> </p> <ul> <li> <span> <span> Proactively prevent business impacts and disruptions related to database failures. </span> </span> </li> <li> <span> <span> Participate in technology bridge calls to circumvent unplanned business disruptions.  </span> </span> </li> <li> <span> <span> Assist with upgrades and/or migrations as requested by database engineering, database development, database infrastructure and application partner teams. </span> </span> </li> <li> <span> <span> Assist in Disaster Recovery testing. </span> </span> </li> <li> <span> <span> Knowledge of ITIL processes - Incident, Problem Change Management. </span> </span> </li> <li> <span> <span> Provide technical direction and assistance to our contractors. </span> </span> </li> <li> <span> <span> Drive improvements within platforms. </span> </span> </li> <li> <span> <span> <span> <span> Look for opportunities for developing automation, Efficiency and Self Service. </span> </span> </span> </span> </li> </ul> <p> <span> <span> <span> <span> <span> <span> <span> <span> <span> <span> <span> </span> </span> </span> </span> </span> </span> </span> </span> </span> </span> </span> </p> <p> <span> <span> <span> <span> <span> <span> <span> <span> <span> <span> <span> </span> </span> </span> </span> </span> </span> </span> </span> </span> </span> </span> </p> <p> <span> <span> <span> <span> <span> <span> <span> <span> <span> <span> <span> </span> </span> </span> </span> </span> </span> </span> </span> </span> </span> </span> </p> <p> <span> <span> <span> <span> <span> <span> <span> <span> <span> <span> <span> </span> </span> </span> </span> </span> </span> </span> </span> </span> </span> </span> </p> <p> <span> <span> <span> <span> <span> <span> <span> <span> <span> <span> <span> </span> </span> </span> </span> </span> </span> </span> </span> </span> </span> </span> </p> <p> <span> <span> <span> <span> <span> <span> <span> <span> <span> <span> <span> Qualifications: </span> </span> </span> </span> </span> </span> </span> </span> </span> </span> </span> </p> <ul> <li> <span> <span> 3 year experience with Couchbase, Cassandra and/or MongoDB databases as a Database Administrator (DBA). </span> </span> </li> <li> <span> <span> Experience with troubleshooting, performance enhancements, latency, capacity, and availability issues in a production support environment. </span> </span> </li> <li> <span> <span> Experience using Ansible, GitHub, Unix Shell Scripting, Python, Splunk, ELK or Grafana is a plus. </span> </span> </li> </ul> </div>",Data Engineer I | Cassandra,1000000,1400000,https://www.naukri.com/job-listings-data-engineer-i-cassandra-american-express-company-bangalore-bengaluru-1-to-5-years-081222501706,"['Automation', 'Change management', 'Production support', 'Disaster recovery', 'Flex', 'Incident management', 'Troubleshooting', 'Unix shell scripting', 'Monitoring', 'Python']",['Bengaluru'],Data Engineer,2022-12-08 20:41:48
081222501704,"<div> <p> <span> <span> <span> <span> <span> <span> <span> <span> <span> <span> <span> <span> <span> We are looking for a highly motivated Data Engineer (DBA) with good understanding and experience in maintaining the health and availability of our production Oracle and PostgreSQL databases. In this position, you will be joining our global technology’s Database Operations team in American Express, Bangalore, India . </span> </span> </span> </span> </span> </span> </span> </span> </span> </span> </span> </span> </span> </p> <ul> <li> <span> <span> Proactively prevent business impacts and disruptions related to database failures. </span> </span> </li> <li> <span> <span> Participate in technology bridge calls to circumvent unplanned business disruptions.  </span> </span> </li> <li> <span> <span> Assist with upgrades and/or migrations as requested by database engineering, database development, database infrastructure and application partner teams. </span> </span> </li> <li> <span> <span> Assist in Disaster Recovery testing. </span> </span> </li> <li> <span> <span> Knowledge of ITIL processes - Incident, Problem Change Management. </span> </span> </li> <li> <span> <span> Provide technical direction and assistance to our contractors. </span> </span> </li> <li> <span> <span> Drive improvements within platforms. </span> </span> </li> <li> <span> <span> <span> <span> Look for opportunities for developing automation, Efficiency and Self Service. </span> </span> </span> </span> </li> </ul> <p> <span> <span> <span> <span> <span> <span> <span> <span> <span> <span> <span> <span> <b> <span> <span> Qualifications: </span> </span> </b> </span> </span> </span> </span> </span> </span> </span> </span> </span> </span> </span> </span> </p> <ul> <li> <span> <span> 4 year experience with Postgres or Oracle as a Database Administrator (DBA). </span> </span> </li> <li> <span> <span> Experience with troubleshooting, performance enhancements, latency, capacity, and availability issues in a production support environment. </span> </span> </li> <li> <span> <span> Working knowledge of Golden Gate is a plus. </span> </span> </li> <li> <span> <span> Experience using Ansible, GitHub, Unix Shell Scripting, Python or Grafana is a plus. </span> </span> </li> </ul> </div>",Data Engineer I - Oracle/Postgre,600000,1000000,https://www.naukri.com/job-listings-data-engineer-i-oracle-postgre-american-express-company-bangalore-bengaluru-2-to-6-years-081222501704,"['Automation', 'Change management', 'Production support', 'Postgresql', 'Disaster recovery', 'Flex', 'Oracle', 'Troubleshooting', 'Unix shell scripting', 'Python']",['Bengaluru'],Data Engineer,2022-12-08 20:41:48
081222011560,"<p> </p><p>Dear Candidate,</p><p> <br /> </p><p>We are looking for a strong Azure Data Engineer. Please find the detailed JD below for your reference.</p><p> </p><p>Must have experience on below.<br /> </p><ul><li> Azure Data Lake Gen 2</li><li> Azure Data Factory</li><li> Azure Synapse Analytics</li><li> Azure Analysis Service</li></ul><p> <br /> </p><p>Exp - 5-9 yrs ( Relevant of 2 yrs in Azure Data Lake)</p><p>Location - Open</p><p>Notice Period - 0-30 days (Preferably immediate) and serving notice period with only a month left</p><p> <br /> </p><p>Candidates who suffice the above requirements do apply to this posting, will get back to you at the earliest.</p><p> <br /> <br /> </p><p>Regards,</p><p>Louis</p><p>HR Team - Deloitte</p><p>lomohan@deloitte.com</p><br />",Hiring For Azure Data Engineer - Deloitte,1000000,2000000,https://www.naukri.com/job-listings-hiring-for-azure-data-engineer-deloitte-deloitte-noida-hyderabad-secunderabad-pune-gurgaon-gurugram-chennai-bangalore-bengaluru-mumbai-all-areas-5-to-9-years-081222011560,[],"['Chennai', 'Pune', 'Bengaluru', 'Hyderabad', 'Gurgaon', 'Noida', 'Mumbai (All Areas)']",Data Engineer,2022-12-08 18:34:52
080822005072,"<p><strong><u>JD for Data Engineer </u></strong><br /> <br /> <strong>On a daily basis, youll be:</strong><br /></p><ul><li>Working with product owners as we evolve requirements, iterate through features, and gather feedback</li><li>Researching, analyzing, planning, and performing hands on-development for project work or POCs</li><li>Coordinating work for your projects with our developers, vendors, IT teams, and our business</li><li>Reviewing & providing oversight with code reviews/test plans, and development prioritization</li></ul><p><strong>Responsibilities:</strong><br /></p><ul><li>Architect, Design, Develop and Implement software to support Aon's Investment consulting business</li><li>Interface with IT teams to architect, secure, and manage supporting infrastructure</li><li>Provide technical leadership, including coaching & mentoring less experienced developers</li><li>Estimate the complexity of user stories, and driving execution of the work</li><li>Provide ongoing application support and troubleshooting</li><li>Assist with requirements gathering, project management, and quality assurance</li><li>Proactively identify risk areas and opportunities for improvement</li><li>Continually learn, set, teach & apply software best practices, tools, and technologies</li></ul><p><strong>Required Experience:</strong><br /></p><ul><li>9+ years software engineering experience</li><li>2+ years in a leading role on project teams, with the ability to mentor and collaborate with other engineers</li><li>Mastery of Python, Spark, Spark SQL</li><li>We are exiting Cloudera and moving to Data Bricks & Snowflake, so interest or experience in using these a big plus.</li><li>Highly skilled in RDBMS/SQL, Experience in Airflow, AWS S3</li><li>Proficient in C#</li><li>Demonstrated ability to quickly learn new technologies & frameworks</li></ul><p><strong>Preferred Experience:</strong><br /></p><ul><li> Degree in Computer Science or related field</li><li>Finance or Investment Consulting Industry experience</li><li>Experience with Power BI, Tableau</li></ul><p> </p><p><strong>Notice Period - 15 days- 30 days </strong></p>",Data Engineer,3250000,3500000,https://www.naukri.com/job-listings-data-engineer-aon-bangalore-bengaluru-delhi-ncr-9-to-12-years-080822005072,['airflow'],"['Delhi NCR', 'Bengaluru']",Data Engineer,2022-12-06 12:39:42
071222012884,"<br /><p><strong>Greetings from HCL Technologies!!</strong></p><br /><p>We are pleased to inform you that we currently have <strong>opening for Azure data engineer / Architect @ Pan India</strong>. If your profiles match this skill set, please apply with your details.</p><br /><p>The candidates who are interested & ready to attend interview, please share your resume with below details to <strong>subbulakshmi_p@hcl.com.</strong></p><br /><p><strong>Mentioned below are the details of the openings:</strong></p><p>Experience : 9 - 16 years</p><p>Job Location : Hcl locations</p><br /><br /><p><strong>If you are interested for the weekend drive, Please apply through below link:</strong></p><p><u>https://forms.office.com/r/s67aAUBibA</u></p><br /><br /><br /><p>Regards,</p><br /><p>Subbulakshmi</p><p>subbulakshmi_p@hcl.com</p>",HCL TECH - Opening For Azure data engineer / Architect @ Pan India,900000,1900000,https://www.naukri.com/job-listings-hcl-tech-opening-for-azure-data-engineer-architect-pan-india-hcltech-kolkata-hyderabad-secunderabad-pune-chennai-bangalore-bengaluru-delhi-ncr-mumbai-all-areas-9-to-14-years-071222012884,['Azure Data Lake'],"['Chennai', 'Pune', 'Delhi NCR', 'Bengaluru', 'Hyderabad', 'Kolkata', 'Mumbai (All Areas)']",Data Engineer,2022-12-07 18:24:04
301122016177,"<p> We are hiring for Data Engineer /Senior Data Engineer with 3 to 8 years of experience and the job location would be for Bangalore Location  only(should be ready to work in Hybrid Mode) </p><br /><p>*****<strong>Please apply if only your Serving Notice Period can join within 15days Or Immediate</strong> *****</p><br /><p><u><strong>Experience:</strong></u></p><p> </p><ul><li>3+ years experience developing Data & Analytic solutions</li><li>Experience  building data lake solutions leveraging one or more of following AWS, EMR, S3, Hive & Spark </li><li>Experience with relational SQL (intermediate)</li><li>Experience with scripting languages such as Shell, Python (expert)</li><li>Experience with source control tools such as GitHub and related dev process</li><li>Experience with workflow scheduling tools such as Airflow</li><li>In-depth knowledge of scalable cloud</li><li>Has a passion for data solutions </li><li>Strong understanding of data structures and algorithms</li><li>Strong understanding of solution and technical design</li><li>Has a strong problem solving and analytical mindset</li><li>Experience working with Agile Teams.</li><li>Able to influence and communicate effectively, both verbally and written, with team members and business stakeholders</li><li>Able to quickly pick up new programming languages, technologies, and frameworks</li><li>Bachelors Degree in computer science</li></ul>",Data Engineer opportunity For Bangalore location only,800000,1300000,https://www.naukri.com/job-listings-data-engineer-opportunity-for-bangalore-location-only-teksystems-global-services-bangalore-bengaluru-3-to-8-years-301122016177,"['s3', 'Airflow', 'Pyspark', 'Data Engineering', 'Github', 'Big Data', 'emr']",['Bengaluru'],Data Engineer,2022-11-30 20:21:08
021222009858,"<p> Skills Needed :</p><p> • At least 4+ years of data engineering experience in handling large data volume. </p><p> • Solid experience in Apache Spark, Python, SQL</p><p> • Experience in data engineering solutions on the cloud. Knowledge of Azure cloud, Airflow, and databricks is preferable.</p><p> • Good to have o Exposure to setting up CI/CD pipelines o Understanding of Agile methodology, using Jira. </p>",Data Engineer,600000,1500000,https://www.naukri.com/job-listings-data-engineer-absolutdata-noida-3-to-8-years-021222009858,[],['Noida'],Data Engineer,2022-12-02 18:30:58
021222004525,"<p> </p><p>We KGiS (www.kgis.co) is hiring Data Engineer - Coimbatore/Bangalore.</p><br /><p>Interested candidates kindly share your profiles along with the below details.</p><p>Total Yrs of exp -<br />Relevant Exp -<br />Current CTC -<br />Expected CTC -<br />Notice Period -<br />Current Location -<br />Interested to relocate to Coimbatore -</p><br /><p>Skills:</p><p>Hands-on exp in Agile methodologies.</p><p>Good Experience in API Service(FastAPI), Python.</p><p>Good Experience in AWS (EC2, SC3, Sagemaker) & Git.</p><p>Experience in product-based companies would be an added advantage.</p><br /><br />",Looking For Immediate joiners - Data Engineer,500000,1300000,https://www.naukri.com/job-listings-looking-for-immediate-joiners-data-engineer-kg-invicta-services-coimbatore-bangalore-bengaluru-3-to-8-years-021222004525,['git'],"['Bengaluru', 'Coimbatore']",Data Engineer,2022-12-02 10:51:42
021222500003,"<ul> <li> <span> Build large-scale batch data processing systems with Spark and DBT in the Cloud  </span> </li> <li> <span> Collaborate with engineers and product managers to understand data needs and build exceptional data products used internally and directly by our customers  </span> </li> <li> <span> Manage data warehouse integrations with our customers data science teams  </span> </li> <li> <span> Create concise documentation for consumers of data products  </span> </li> <li> <span> Improve data quality through testing and tooling  </span> </li> <li> <span> Own your changes from development to production and beyond  </span> </li> <li> <span> Participate in engineering and architecture forums to help build a culture of excellence within the engineering organization  </span> </li> </ul> <p> <span> <strong> What You'll Bring:  </strong> </span> </p> <ul> <li> <span> Demonstrable experience with modern OLAP databases like BigQuery  </span> </li> <li> <span> Advanced SQL - Beyond just CRUD.  </span> </li> <li> <span> Experience with tools such as Spark or DBT for building pipelines  </span> </li> <li> <span> An understanding of data modeling, data access, data storage, and optimization techniques  </span> </li> <li> <span> Experience working with cloud based technologies and development processes  </span> </li> </ul> <p> <strong> Nice to Haves:  </strong> </p> <ul> <li> <span> Experience using visualization tools such as Looker or Tableau  </span> </li> </ul>",Data Engineer SDE2 - Data Products,900000,1300000,https://www.naukri.com/job-listings-data-engineer-sde2-data-products-boomerang-commerce-bangalore-bengaluru-3-to-5-years-021222500003,"['CRUD', 'tableau', 'data science', 'Data modeling', 'spark', 'Cloud', 'Data processing', 'Data quality', 'Data warehousing', 'SQL']",['Bengaluru'],Data Engineer,2022-12-02 11:52:32
